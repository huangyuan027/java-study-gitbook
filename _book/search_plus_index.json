{"./":{"url":"./","title":"前言","keywords":"","body":"前言项目查阅方式项目模块参考与鸣谢交流群前言 项目查阅方式 gitbook github源码 项目模块 Java基础 J2EE Spring Mybatis redis 数据库 sql 权限管理系统 单元测试 linux 计算机基础 工具 参考与鸣谢 本项目的内容主要来自 自己的工作总结，读书笔记，以及对网上博客文章进行整理与总结。希望以最明了易懂的方式呈现出java技术架构，以便轻松应对面试 Java学习+面试指南 阮一峰的网络日志 mrbird博客与开源项目 芋道源码 张开涛博客 纯洁的微笑 等等优秀的博客会在具体文章后列出 交流群 "},"CodeStandard/":{"url":"CodeStandard/","title":"开发规范","keywords":"","body":"开发规范开发规范 代码规范参考阿里巴巴的代码规范 阿里巴巴代码规范 本文记录一下自己开发中觉得重要或者容易忘记的 "},"CodeStandard/各层命名规范.html":{"url":"CodeStandard/各层命名规范.html","title":"各层命名规范","keywords":"","body":"各层命名规范各层命名规范 A) Service/DAO 层方法命名规约 1) 获取单个对象的方法用 get 做前缀。 2) 获取多个对象的方法用 list 做前缀，复数形式结尾如:listObjects。 3) 获取统计值的方法用 count 做前缀。 4) 插入的方法用 save/insert 做前缀。 5) 删除的方法用 remove/delete 做前缀。 6) 修改的方法用 update 做前缀。 B) 领域模型命名规约 1) 数据对象:xxxDO，xxx 即为数据表名。 2) 数据传输对象:xxxDTO，xxx 为业务领域相关的名称。 3) 展示对象:xxxVO，xxx 一般为网页名称。 4) POJO 是 DO/DTO/BO/VO 的统称，禁止命名成 xxxPOJO。 "},"base/":{"url":"base/","title":"Java基础","keywords":"","body":"Java基础Java基础 Java基础 final,static,this,super 关键字总结 Java异常处理 容器 Java容器基础 ArrayList 的扩容机制 Comparable和Comparator 多线程 线程生命周期 线程通信(等待通知wait/notify机制) 死锁 并发 synchronized关键字 ReentrantLock(补充) volatile关键字 线程池 ThreadPoolExecutor类 线程池的具体实现原理 线程池使用示例 Executors创建线程池 如何合理配置线程池的大小 ThreadLocal 乐观锁和悲观锁 Callable和Future Atomic原子类 CAS（比较并替换） AQS构建锁和同步器 IO IO总结 IO流入门 File类 文件流 FileInputStream FileOutputStream FileReader FileWriter 缓冲流 IO转换流 对象流 Jvm Java内存区域 HotSpot 虚拟机对象创建 JVM垃圾回收 JVM 内存分配与回收 对象已经死亡？ 垃圾收集算法 垃圾收集器 JVM性能调优 如何合理的规划 JVM 性能调优 类加载过程 类加载过程(精简版) 类加载器 类加载器（常见面试） tomcat类加载器 JDK监控和故障处理工具汇总 MAT MAT安装 MAT使用 "},"base/object/Java基础.html":{"url":"base/object/Java基础.html","title":"Java基础","keywords":"","body":"Java基础1. Java面向对象编程三大特性：封装 集成 多态1.1 封装1.2 继承1.3 多态1.3.1 Java 中实现多态的方式2. 构造器 Constructor 是否被override3. 重载和重写的区别4. String、 StringBuffer 和 StringBuilder 的区别是什么？String为什么是不可变的？5. 自动装箱与拆箱6. 在一个静态方法内调用一个非静态成员为什么是非法的？7. 在java中定义一个不做事且没有参数的构造方法的作用？8. 接口和抽象类的区别是什么？9. 成员变量与局部变量的区别有哪里？10. 一个类的构造方法的作用是什么，若一个类没有声明构造方法，该程序能正常执行吗？为什么？11. 构造方法有哪些特点12. 静态方法和实例方法有何不同13. 对象的相等与指向他们的引用相等，两者有什么不同14. == 与 equals（常问）15 hashCode 与 equals（重要）15.1 hashCode（）介绍15.2 为什么要有hashCode15. hashCode（）与equals（）的相关规定16. Java序列化中如果有些字段不想进行序列化，怎么办？Java基础 1. Java面向对象编程三大特性：封装 集成 多态 1.1 封装 隐藏对象的属性和实现细节，仅对外公开访问方法，控制程序中属性的读和写的访问级别。 1.2 继承 在一个现有类的基础之上，增加新的方法或重写已有方法，从而产生一个新类。 关于继承如下3点： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类的私有属性和方法子类是无法访问的，只是拥有 子类可以拥有自己属性和方法，既子类可以对父类进行扩展 子类可以用自己的方式实现父类的方法（重写） 1.3 多态 对象在不同时刻表现出来的不同状态。在编译时并不能确定，只有在运行期间才能决定 所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，既一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定 1.3.1 Java 中实现多态的方式 继承：多个子类对同一方法的重写 接口：实现接口并覆盖接口的统一方法 2. 构造器 Constructor 是否被override 再讲继承的时候我们就知道父类的私有属性和构造方法并不能被继承，所以Constructor 也就不能被 override（重写），但是可以overload（重载），所以你可以看到一个类中有多个构造函数情况 3. 重载和重写的区别 重载：发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时 重写：发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类，如果父类方法访问修饰符为private则子类就不能重写该方法 4. String、 StringBuffer 和 StringBuilder 的区别是什么？String为什么是不可变的？ 可变性 String 对象是不可变 String类中使用final 关键字修饰字符数组来保存字符串private　final　char　value[]所以String 对象是不可变的 StringBuilder 与 StringBuffer 是可变的 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，而AbstractStringBuilder 中也是使用字符数组保存字符串char[] value但是没有用final关键字修饰，所以这两种对象都是可变的 AbstractStringBuilder.java 的源码 abstract class AbstractStringBuilder implements Appendable, CharSequence { char[] value; int count; AbstractStringBuilder() { } AbstractStringBuilder(int capacity) { value = new char[capacity]; } 线程安全性 String 线程安全 String 中的对象是不可变的，可以理解为常量，线程安全 AbstractStringBuilder的子类 AbstractStringBuilder是 StringBuild 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如expandCapacity、append、insert、index等公共方法 StringBuffer 线程安全 StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的 StringBuilder 非线程安全 StringBuilder 并没有对方法进行同步锁，所以是非线程安全的 性能 String（少修改，性能好） 每次对String类型进行改变的时候，都会生成一个新的String对象，然后将指针指向新的String对象 StringBuffer（线程安全，性能比StringBuilder差一些） StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用 StringBuilder （非线程安全，比StringBuffer性能好一些） 相同情况下使用StringBuild相比StringBuffer仅能获得10%~15%左右的性能提升，但却要冒多线程不安全的风险 5. 自动装箱与拆箱 装箱：将基本类型用他们对应的引用类型包装起来 拆箱：将包装类型转换为基本数据类型 6. 在一个静态方法内调用一个非静态成员为什么是非法的？ 由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量的成员 7. 在java中定义一个不做事且没有参数的构造方法的作用？ 主要是用来解决，父类只定义了有参构造函数，子类又没有用super来调用父类特定的构造方法导致的异常 java程序在执行子类的构造方法之前，如果没有用super（）来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用super()来调用父类中特定的构造方法，则编译时将发生错误。因为java 程序在父类中找不到没有参数的构造方法可供执行，解决办法是在父类里加上一个不做事且没有参数的构造方法 8. 接口和抽象类的区别是什么？ 在方法上 接口的方法默认是public，所有方法在接口中不能有实现（java8开始接口可以有默认实现） 而抽象类可以有非抽象的方法 变量 接口中除了static、final变量，不能有其他变量 而抽象类则不一定 访问修饰符上 接口方法默认修饰符是public 抽象类可有其他修饰符 从设计层面 接口是对行为的抽象，是一种行为规范 抽象是对类的抽象，是一种模板设计 9. 成员变量与局部变量的区别有哪里？ 从语法形式上看 成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数 成员变量可以被public、privite、static 等等修饰符修饰，而局部变量不能被访问修饰符以及static所修饰 成员变量和局部变量都能被final所修饰 从变量在内存中存储方式来看 如果成员变量使用static修饰的，那么这个成员变量是属于类的，如果没有使用static 修饰，这个成员变量是属于实例的。而对象存在与堆内存（也就是成员变量存在堆），局部变量存在与栈内存 从变量在内存中生存时间上看 成员变量是对象的一部分，他随着对象创建而存在 而局部变量随着方法的调用而自动消失 成员变量如果没有被赋初始值： 这会自动以类型的默认值而复制（一种情况例外，被final修饰的成员变量也必须显式赋值） 而局部变量不会自动赋值 10. 一个类的构造方法的作用是什么，若一个类没有声明构造方法，该程序能正常执行吗？为什么？ 主要作用是完成对类对象的初始化工作。可以执行，因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法 11. 构造方法有哪些特点 名字与类名相同 没有返回值 生成类的对象时自动执行，无需调用 12. 静态方法和实例方法有何不同 在外部调用静态方法时，可以使用“类名.方法名”的方式，也可以使用“对象名.方法名”的形式，而实例方法只有后面的这种方式，也就是说，调用静态方法可以无需创建对象 静态方法在访问本来的成员时，只允许访问静态成员（既静态成员变量和静态方法），而不允许访问实例成员变量和实例方法，实例方法则无此限制 13. 对象的相等与指向他们的引用相等，两者有什么不同 对象相等：比的是内存中存放的内容是否相等 引用相等，比较的是他们指向的内存地址是否相等 14. == 与 equals（常问） == ： 他的作用是判断两个对象的地址是不是相等。既，判断两个对象是不是同一个对象（基本数据类型==比较的是值，引用数据类型==比较的是内存地址） equals(): 他的作用也是判断两个对象是否相等。但他一般有两种使用情况 情况1：类没有覆盖equals（）方法 则通过equals（）比较该类的两个对象时，等价于通过“==”比较这两个对象 情况2：类覆盖了equals（）方法 一般如果我们覆盖equals（）方法来比较两个对象的内容是否相等。若他们的内容相等，则返回true 15 hashCode 与 equals（重要） 面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写equals时必须重写hashCode方法？” 15.1 hashCode（）介绍 hashCode()的作用是获取哈希码，也称为散列码；它实际上是返回一个int整型。这个哈希码的作用是确定该对象在哈希表中的索引位置。 hashCode()定义在JDK的Object.java 中，这就意味着java中的任何类都包含有hashCode（）函数 散列表存储的键值对（key-value），他的特点是：能根据“键”快速的检索出对应的“值”，这其中就利用到了散列码（可以快速找到需要的对象） 15.2 为什么要有hashCode 以hashMap，HashSet为代表的hash是怎么检查重复的呢? 当你把对象加入 HashSet 时，hashSet 会先计算对象的hashCode来判断对象加入的位置 同时会与其他已经加入的对象的hashCode值做比较 如果没有相符的hashCode，就假设对象没有重复出现 如果发现相同的hashCode 值的对象，就调用equals（）方法来检查是否真的相同。 如果相同就不会让他插入成功，如果不同的话就会让他重新散列到其他位置 结论 hashCode() 的作用就是获取哈希码，也称散列码，它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode 在散列表中才有用，在其他情况下没用。在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置 15. hashCode（）与equals（）的相关规定 如果两个对象相等，则hashCode一定也是相同的 两个对象相等，对两个对象分别调用equals 方法都返回true 两个对象有相同的hashCode值，他们也不一定是相等的 因此，equals方法被覆盖过，则hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值，如果没有重写hashCode（）则该class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 16. Java序列化中如果有些字段不想进行序列化，怎么办？ 对于不想序列化的变量，使用transient 关键字修饰 transient关键字的作用：阻止实例中那些用此关键字修饰的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transitent只能修饰变量，不能修饰类和方法 "},"base/object/关键字总结.html":{"url":"base/object/关键字总结.html","title":" final,static,this,super 关键字总结","keywords":"","body":"final,static,this,super 关键字总结1. final 关键字2. static关键字3. this关键字3.1 this 的使用方法4. Super 关键字参考文章final,static,this,super 关键字总结 1. final 关键字 final 关键字主要用在三个地方：变量、方法、类 final 变量 如果是基本数据类型的变量，其数值一旦初始化之后便不能改变 如果是引用类型的变量：则在对其初始化之后便*不能再让其指向另一个对象 final 类 表明这个类不能被继承，final 类中的所有成员方法都会被隐式得指定为final方法 final方法 final修饰的成员方法不能被子类重写 2. static关键字 static 关键字主要有以下四种使用场景 修饰成员变量和成员方法： 被static 修饰的成员属于类，不属于这个类的某个实例，被类中所有实例共享。可以并且建议通过类名调用。 被static 声明的成员变量属于静态成员变量，静态变量存在java 内存区域的方法区。 调用格式：类名.静态变量名 类名.静态方法名 静态代码块： 静态代码块定义在类中方法外，静态代码块在非静态代码块之前执行（静态代码块—》非静态代码—》构造方法）。该类不管创建多少对象，静态代码块只执行一次 静态内部类（static修饰类的话只能修饰内部类）： 静态内部类与非静态内部类之间存在一个最大区别： 非静态内部类在编译完成之后会隐含得保存着一个引用，该引用是指向他的外围类 静态内部类，没有保存外部类引用 没有这个引用意味着 他的创建不需要依赖外围类的创建 他不能使用任何外围类的非static成员变量和方法 静态导包（用来导入类中的静态资源，1.5 之后的新特性） 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法。 3. this关键字 定义：java 中的this 关键字用于在方法中引用当前实例 3.1 this 的使用方法 明确表示使用的成员变量（instance）而不是静态变量（static）或者局部变量（local）。 private String javaFAQ; void methodName(String javaFAQ) { this.javaFAQ = javaFAQ; } this在这里代表成员变量（译者注：this.javaFAQ表示成员变量，javaFAQ表示局部变量）。在这个方法里，局部变量的优先级更高。因此，如果没有用this.表示的话则指定的是局部变量。在这个方法里面，如果局部变量的名字和成员变量的名字并不一样的话，那么用不用这个this其实就没有关系了。 this用来表示构造函数 public JavaQuestions(String javapapers) { this(javapapers, true); } 这里使用this调用同一个类中的另一个包含两个参数的构造方法 用于将当前java实例作为参数传递 obj.itIsMe(this); 和上一个类似，this还可以用于返回当前java实例 CurrentClassName startMethod() { return this; } this也可以表示当前类的句柄 Class className = this.getClass(); // this methodology is preferable in java 也可以通过 Class className = ABC.class;实现，这里的ABC指的是java类的类名。 通常，java中的this都与他的实例相关联，不能在静态方法中使用。 4. Super 关键字 super关键字用于从子类访问父类的变量和方法 public class Super { protected int number; protected showNumber() { System.out.println(\"number = \" + number); } } public class Sub extends Super { void bar() { super.number = 10; super.showNumber(); } } 在上面的例子中，Sub 类访问父类成员变量 number 并调用其其父类 Super 的 showNumber（） 方法。 使用 this 和 super 要注意的问题： 在构造器中使用 super（） 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。 this、super不能用在static方法中。 简单解释一下： 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享。而 this 代表对本类对象的引用，指向本类对象；而 super 代表对父类对象的引用，指向父类对象；所以， this和super是属于对象范畴的东西，而静态方法是属于类范畴的东西。 参考文章 final,static,this,super 关键字总结 "},"base/error/Java异常处理.html":{"url":"base/error/Java异常处理.html","title":"Java异常处理","keywords":"","body":"Java异常处理1. Java 异常类层次结构图1.1 Error（错误）1.2 Exception(异常)2. 异常处理总结2.1 在什么情况下finally块不会被执行2.2 try语句和finally语句都有return的情况Java异常处理 1. Java 异常类层次结构图 在Java中，所有的异常都有一个共同的祖先java.lang包中的Throwable类。Throwable: 有两个重要的子类：Exception（异常）和 Error（错误），二者都是Java异常处理的重要子类，各自都包含了大量子类 1.1 Error（错误） error 是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM(Java虚拟机)出现的问题。 例如，Java虚拟机运行时错误（Virtual MachineError），当JVM 不再有继续执行操作所需要的内存资源时，将出现OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止 这些错误表示故障发生于虚拟机自身，或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 1.2 Exception(异常) Exception(异常) 是程序本身可以处理的异常。 Exception类有一个重要的子类 RuntimeException。RuntimeException异常由java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 注意：异常和错误的区别：异常能被程序本身处理，错误是无法处理的 2. 异常处理总结 try块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块 catch块：用于处理try捕获的异常 finally块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句将在方法返回之前被执行 2.1 在什么情况下finally块不会被执行 在finally 语句块第一行发生了异常。因为在其他行，finally块还是会得到执行 在前面的代码中调用了System.exit(int) 已退出程序，若该语句在异常语句之后，finally会执行 程序所在的线程死亡 关闭CPU 2.2 try语句和finally语句都有return的情况 当try语句和finally 语句中都有return 语句时，在方法返回之前，finally语句的内容将被执行，并且finally 语句的返回值将会覆盖原始的返回值 public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } } } 如果调用f(2),返回值将是0，因为finally语句的返回值覆盖了try语句块的返回值 "},"base/interview/Java基础面试提问.html":{"url":"base/interview/Java基础面试提问.html","title":"Java基础面试提问","keywords":"","body":"Java基础面试提问1. String篇2. 线程安全篇3. 多线程篇4. 线程池篇5. 异常处理篇6. 容器篇Java基础面试提问 1. String篇 说说String、StringBuffer和StringBuilder的区别 为什么String 不可变？ final修饰的变量特点 StringBuffer为什么是线程安全的？（引出多线程话题） 2. 线程安全篇 什么是线程安全？ 为什么会有线程安全问题？(说说java内存模型？) 说说java 是如何保证线程安全的？ 谈谈Synchronized和ReentrantLock的区别？ Synchronized和ReentrantLock都是悲观锁，那什么是乐观锁，如何实现的呢 你有使用过volatile 关键字吗？他能保证线程安全吗？ 3. 多线程篇 为什么要使用多线程？ 你在使用多线程中有没有遇到什么问题？ 多线程导致的内存泄漏你是怎么解决的？ 什么是上下文切换？为什么需要上下文切换？ 什么是死锁？如何解除死锁？ 线程之间是如何进行线程通信的？ 我们为什么不能直接调用run()方法？ 4. 线程池篇 你是如何管理这些线程的？为什么要使用线程池呢？ 线程池的execute() 和submit() 的区别是什么？ 你是怎么创建这些线程池的？ 为什么不建议使用Executors？ 线程池从启动到工作的流程？ 如何合理的配置一个线程池的大小？ 5. 异常处理篇 说说java异常处理类的整体架构 try语句和finally语句都有return的情况如何返回？ finally一定都会执行吗？有没有特例？ 6. 容器篇 说说ArrayList 的底层数据结构？ ArrayList的扩容机制 HashMap的底层实现 HashMap的哈希表长度为什么是2的幂次方 HashSet是如何检查重复的？ "},"base/collection/Java容器基础.html":{"url":"base/collection/Java容器基础.html","title":"Java容器基础","keywords":"","body":"Java容器基础1. 说说List、Set、Map三者的区别2. ArrayList 和 LinkedList 区别？2.1 Linked为什么不支持随机访问(RandomAccess接口)2.2 list 遍历方式选择2.3 补充：双向链表和双向循环列表3.ArrayList与Vector区别？为什么要用ArrayList取代Vector呢？4. ArrayList 的扩容机制5. HashMap 和 Hashtable 的区别6. HashMap 如何保证总是使用2的幂次方作为哈希表的大小7. HashMap 和 HashSet区别8. HashSet如何检查重复8.1 hashCode（）与equals（）的相关规定：8.2 ==与equals的区别9. HashMap 的底层实现9.1 JDK1.8 之前9.3 拉链法10. HashMap 的长度为什么是2的幂次方11. HashMap 多线程操作导致死循环问题12. ConcurrentHashMap 和 HashTable 的区别12.1 两者对比图13. ConcurrentHashMap 线程安全的具体实现方式/ 底层具体实现13.1 在JDK1.7 上的ConcurrentHashMap13.2 JDK1.8 ConcurrentHashMap14. 集合框架底层数据结构总结15. 如何选用集合？Java容器基础 1. 说说List、Set、Map三者的区别 List：List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象 Set（注重独一无二的性质）：不允许重复的集合，不会有多个元素引用相同的对象 Map（用key来搜索的专家）：使用键值对存储。Map会维护与key有关联的值。两个key可以引用相同的对象，但key不能重复。 2. ArrayList 和 LinkedList 区别？ 是否保证线程安全：ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全 底层数据结构：ArrayList 底层使用的是Object 数组，LinkedList 底层使用的是双向链表（JDK1.6之前为循环链表，JDK1.7取消了循环） 插入和删除是否受元素位置的影响： | z | ArrayList（数组） | LinkedList(链表) | | ---------- | -------------------------------------- | ------------------------ | | 是否影响 | 影响 | 不影响 | | 算法复杂度 | 追加到末尾O(1)，插入和删除i位置 O(N-1) | 插入删除都不影响都是O(1) | 是否支持随机访问：LinkedList 不支持高效的随机元素访问，而ArrayList 支持。 快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用：ArrayList 的空间浪费主要体现在list列表的结尾会预留一定的容量空间。而LinkedList 的空间花费则体现在他的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据） 2.1 Linked为什么不支持随机访问(RandomAccess接口) RandomAccess接口中什么都没有定义，只是一个标识，标识实现这个接口的类具有随机访问的功能 ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？ 我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O（1），所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O（n），所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！ 2.2 list 遍历方式选择 实现了RandomAccess 接口的list，优先选择普通for循环，其次foreach 未实现 RandomAccess 接口的list，优先选择iterator遍历（foreach遍历底层也是通过iterator实现），大size的数据，千万不要使用普通for循环 2.3 补充：双向链表和双向循环列表 双向链表：包含两个指针，一个prev指向前一个节点，一个next指向后一个节点 双向循环链表：最后一个节点的next指向head，而head 的prev指向最后一个节点，构成一个环 3.ArrayList与Vector区别？为什么要用ArrayList取代Vector呢？ Vector 类的所有方法都是同步方法的。可以由两个线程安全得访问一个Vector对象。但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间 ArrayList 不是同步的，所以在不需要保证线程安全时建议使用ArrayList 4. ArrayList 的扩容机制 ArrayList 的扩容机制 5. HashMap 和 Hashtable 的区别 线程是否安全：HashMap是非线程安全的，HashTable是线程安全的；HashTable内部的方法基本都经过synchronized修饰。（如果你要保证线程安全的话使用ConcureentHashMap） 效率：因为线程安全的问题，HashMap 要比HashTable 效率要高一点，另外，HashTable 基本被淘汰，不要再代码中使用它 对Null key 和Null value 的支持：HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为null。。但是在hashTable中put进的键值只要有一个null，直接抛出NullPointerException。 初始容量大小和每次扩充容量大小的不同： 创建时如果不指定容量初始值。 Hashtable默认的初始大小为11，之后每次扩充，容量变为原来的2n+1 HashMap 默认的初始化大小为16.之后每次扩充，容量变为原来的2倍 创建时如果给定了容量初始值 Hashtable 会直接使用你给定的大小 HashMap 会将其扩充为2的幂次方大小（也就是说 HashMap 总是使用2的幂作为哈希表的大小,） 底层数据结构：JDK1.8 以后的HashMap 在解决哈希冲突时有了较大的变化。当链表长度大于阈值（默认是8），将链表转为红黑树，以减少搜索时间。HashTable 没有这样的机制 6. HashMap 如何保证总是使用2的幂次方作为哈希表的大小 HashMap 中带有初始容量的构造函数： public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor 下面这个方法保证了 HashMap 总是使用2的幂作为哈希表的大小。 /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n >>> 1; n |= n >>> 2; n |= n >>> 4; n |= n >>> 8; n |= n >>> 16; return (n = MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 7. HashMap 和 HashSet区别 HashSet 底层就是基于 HashMap 实现的，（HashSet 的源码非常非常少，除了clone()、writeObject()、readObject（）是hashSet自己不得不实现指纹，其他都是调用hashMap中的方法） HashMap hashSet 实现了Map接口 实现了set接口 存储键值对 仅存储对象 调用put() 向map中添加元素 调用add 方法向set中添加元素 HashMap 使用（key）计算hashcode HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同。所以equals（）方法来判断对象的相等行 8. HashSet如何检查重复 当你吧对象加入HashSet 时， HashSet会先计算对象的hashCode来判断对象加入的位置， 同时也会与其他加入的对象的hashCode值做比较。如果没有相符的hashCode，hashSet 会假设对象没有重复出现。 但是如果方向有hashCode值的对象，这时候会调用equals（）方法来检查hashCode相等的对象是否真的相同。如果两者相同，hashSet就不会让加入操作成功 8.1 hashCode（）与equals（）的相关规定： 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个equals方法返回true 两个对象有相同的hashcode值，它们也不一定是相等的 综上，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。 8.2 ==与equals的区别 ==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同 ==是指对内存地址进行比较 equals()是对字符串的内容进行比较 ==指引用是否相同 equals()指的是值是否相同 9. HashMap 的底层实现 9.1 JDK1.8 之前 JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key的hashCode 经过扰动函数处理过后得到 hash 值，然后通过（n-1）&hash判断当前元素的存放位置（这里的n指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的hash 值以及key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 9.2 HashMap 的扰动函数/ hash 方法源码 所谓扰动函数指的就是 HashMap 的hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法，换句话说**使用扰动函数之后可以减少碰撞 JDK 1.8 hash JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // >>>:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } JDK 1.7 的HashMap 的hash 方法源码 static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 9.3 拉链法 9.3.1 JDK1.7 版本 所谓”拉链法“就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到hash冲突，则将冲突的值加到链表中即可 JDK1.8 之后 当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 10. HashMap 的长度为什么是2的幂次方 目的：为了能让 HashMap 存取高效，尽量较少碰撞。也就是要尽量把数据分配均匀**。 总空间足够大，难以出现碰撞：我们上面也讲到过了，Hash值的方位值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。 内存放不下：但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。 取模运算余数当下标：用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标 这个数组下标的计算方法是“（n-1）&hash”，（n代表数组长度），这也解释了 HashMap 的长度为什么是2 的幂次方 这个算法应该如何设计呢？ 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&)操作（也就是说 hash%length==hash&(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。 总结： HashMap的hash表算法思路是：对数组的长度取余当下标 HashMap的hash表实际算法是：（n-1）&hash，（n代表数组长度） 取余（%）操作和（n-1）&hash相等的前提就是 length 是2的 n 次方 11. HashMap 多线程操作导致死循环问题 主要原因在于 并发下的 Rehash 会造成元素之间会形成一个循环链表。不过，JDK1.8 之后解决了这个问题，但是还是不建议在多线程下使用 HashMap，因为多线程下使用 HashMap 还是会存在其他问题，比如数据丢失，并发环境下推荐使用ConcurrentHashMap 12. ConcurrentHashMap 和 HashTable 的区别 ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式不同 底层数据结构上 JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表实现，JDK1.8 采用的数据结构跟HashMap的节后一样，数组+链表/红黑二叉树 HashTable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是HashTable 的主体，链表则是主要为了解决哈希冲突而存在的 实现线程安全的方式（重要） ConcurrentHashMap： 在JDK1.7 的时候，ConcurrentHashMap（分段锁）对整个桶数进行了分割分段（Segment）。每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 在JDK1.8 的时候，已经摒弃了Segment的概念，而是直接使用Node数组+链表+红黑树的数据结构来实现，并发控制使用synchronized 和 CAS 来操作（JDK1.6以后 对 synchronized锁做了很多优化）** 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本； HashTable（同一把锁） 使用syncronized 来保证线程安全，效率非常低下，当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用put添加元素，另一个线程不能使用put添加元素，也不能使用get，竞争会越来越激烈效率越低 12.1 两者对比图 HashTable，这个table 就一把锁 ConcurrentHashMap 在JDK1.7 的ConcurrentHashMap 在JDK1.8 的ConcurrentHashMap 13. ConcurrentHashMap 线程安全的具体实现方式/ 底层具体实现 13.1 在JDK1.7 上的ConcurrentHashMap 首先将数据分为一段一段的存储， 然后给每一段数据配一把锁， 当一个线程占用锁访问其中一个数据段数据时， 其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由Segment 数组结构和HashEntry数据结构组成 Segment 实现了ReentrantLock，所以Segment 是一种可重入锁，扮演锁的角色。HashEnery 用于存储键值对数据 static class Segment extends ReentrantLock implements Serializable { } 一个ConcurrentHashMap 里包含一个Segment 数组。Segment 的结构和 HashMap类似，是一种数组和链表结构，一个Segment包含一个HashEntry 数组，每个HashEntry 是一个链表结构的元素，每一个Segment守护着一个HashEntry数组里的元素，当对HashEntry 数组的数据进行修改时，必须首先获得对应的Segment的锁 13.2 JDK1.8 ConcurrentHashMap ConcurrentHashMap 取消了Segment分段锁，采用CAS和synchronized来保证并发安全，数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。Java8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为0(n)转换为红黑树（寻址时间复杂度为O(log(N))）） synchronized 只锁定当前链表或红黑树的首节点，这样只要hash不冲突，就不会产生并发，效率又提示了N呗 14. 集合框架底层数据结构总结 List ArrayList：Object数组 Vector：Object数组 LinkedList：双向链表（JDK1.6之前为循环链表，JDK1.7取消了循环） Set HashSet（无序，唯一）：基于HashMap实现的，底层采用 HashMap 来保存元素 LinkedHashSet: LinkedHashSet 继承与 HashSet,并且其内部是通过LinkedHashMap实现的 TreeSet(有序唯一)：红黑树（自平衡的排序二叉树） Map HashMap: JDK1.8 之前HashMap由数组+链表组成的，数组是HashMap的主题。链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树 LinkedHashMap: LinkedHashMap 继承自 HashMap, 所以他的底层仍然是基于拉链式散列结构，既由数据和链表或红黑树组成。另外，LinkedHashMap 在上面的结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表的相应操作，实现了访问顺序相关逻辑 HashTable: 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap：红黑树（自平衡的排序二叉树） 15. 如何选用集合？ 主要根据集合的特点来选用， 比如我们需要根据键值获取到元素值时就选用Map接口下的集合 需要排序时选择TreeMap 不需要排序时就选择HashMap 需要保证线程安全就选用ConcurrentHashMap 当我们只需要存放元素值时，就选择实现Collection 接口的集合 需要保证元素唯一时选择Set接口的集合，比如TreeSet或HashSet 不需要就实现List接口的。比如ArrayList或者LinkedList "},"base/collection/ArrayList 的扩容机制.html":{"url":"base/collection/ArrayList 的扩容机制.html","title":"ArrayList 的扩容机制","keywords":"","body":"ArrayList 的扩容机制1.如何实现扩容2. 手动扩容ArrayList 的扩容机制 1.如何实现扩容 底层主要是这三个私有方法配合实现grow(),grow(int minCapacity),newCapacity(int minCapacity)扩容。最终是调用了Arrays.copyOf方法来进行扩充数组容量的。默认情况下，新的容量是原容量的1.5倍。 // 扩容一个 private Object[] grow() { return grow(size + 1); } // 保证扩容到期望容量minCapacity及以上 private Object[] grow(int minCapacity) { return elementData = Arrays.copyOf(elementData, newCapacity(minCapacity)); } // 根据期望容量minCapacity计算实际需要扩容的容量 private int newCapacity(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; // 得到旧容量 int newCapacity = oldCapacity + (oldCapacity >> 1); // 设置新容量为旧容量的1.5倍 if (newCapacity - minCapacity 2. 手动扩容 grow方法主要用于实现自动扩容的，而用户也可以通过调用以下方法实现手动扩容： public void ensureCapacity(int minCapacity) { if (minCapacity > elementData.length && !(elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA && minCapacity 为什么需要手动扩容？试想一下，如果用户已经知道即将存入大量的元素，比如目前有20个元素，即将存入2000个。那这个时候使用自动扩容就会扩容多次。而手动扩容可以一次性扩容到2000，可以提高性能。 "},"base/collection/Comparable和Comparator.html":{"url":"base/collection/Comparable和Comparator.html","title":"Comparable和Comparator","keywords":"","body":"Comparable和Comparator1. Comparator 定制排序2. Comparable 定制排序Comparable和Comparator Comparable 接口实际上是出自java.lang包，他有一个compareTp(Object obj )方法用来排序 comparator 接口实际上是出自java.util 包，他有一个compare（object obj1,object obj2）方法用来排序 一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo() 方法或者compare() 方法， 当我们需要对某一个集合实现两种排序，比如一个song对象中的歌名和歌手名分别采用一种排序方式的话，我们可以重写compareTo（）方法和使用自制的Comparator方法或者两个Comparator来实现歌名排序和歌名排序， 第二种代表我们只能使用两个参数版的Collections.sort() 1. Comparator 定制排序 ArrayList arrayList = new ArrayList(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); System.out.println(\"原始数组:\"); System.out.println(arrayList); // void reverse(List list)：反转 Collections.reverse(arrayList); System.out.println(\"Collections.reverse(arrayList):\"); System.out.println(arrayList); // void sort(List list),按自然排序的升序排序 Collections.sort(arrayList); System.out.println(\"Collections.sort(arrayList):\"); System.out.println(arrayList); // 定制排序的用法 Collections.sort(arrayList, new Comparator() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); } }); System.out.println(\"定制排序后：\"); System.out.println(arrayList); 输出： 原始数组: [-1, 3, 3, -5, 7, 4, -9, -7] Collections.reverse(arrayList): [-7, -9, 4, 7, -5, 3, 3, -1] Collections.sort(arrayList): [-9, -7, -5, -1, 3, 3, 4, 7] 定制排序后： [7, 4, 3, 3, -1, -5, -7, -9] 2. Comparable 定制排序 重写compareTo 方法实现按年龄来排序 public class Person implements Comparable { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } /** * TODO重写compareTo方法实现按年龄来排序 */ @Override public int compareTo(Person o) { // TODO Auto-generated method stub if (this.age > o.getAge()) { return 1; } else if (this.age public static void main(String[] args) { TreeMap pdata = new TreeMap(); pdata.put(new Person(\"张三\", 30), \"zhangsan\"); pdata.put(new Person(\"李四\", 20), \"lisi\"); pdata.put(new Person(\"王五\", 10), \"wangwu\"); pdata.put(new Person(\"小红\", 5), \"xiaohong\"); // 得到key的值的同时得到key所对应的值 Set keys = pdata.keySet(); for (Person key : keys) { System.out.println(key.getAge() + \"-\" + key.getName()); } } 输出： 5-小红 10-王五 20-李四 30-张三 "},"base/thread/":{"url":"base/thread/","title":"多线程","keywords":"","body":"多线程1. 什么是线程和进程1.1 什么是进程1.2 什么是线程2. 线程与进程的关系与区别及优缺点2.1 进程与线程的关系2.2 进程与线程的区别3. 进程与线程的内存区域3.1 从内存区域角度来分析进程与线程区别3.2 程序计数器为什么是私有的？3.3 虚拟机栈和本地方法栈为什么是私有的3.4. 一句话简单了解堆和方法区4. 并发和并行5. 为什么要使用多线程6. 多线程可能带来的问题7. 什么是上下文切换7.1 为什么需要上下文切换7.2 为什么说上下文切换消耗时间8. 为什么我们不能直接调用 run() 方法？9. 什么是线程安全多线程 1. 什么是线程和进程 1.1 什么是进程 进程是程序的一次执行过程，是操作系统分配资源的最小单位。系统运行一个程序即是一个进程从创建，运行到消亡的过程 在java中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而main函数所在的线程就是这个进程中的一个线程，也称主线程 1.2 什么是线程 他是操作系统运算调度（程序执行）的最小单位，一个进程包含一个或多个线程（重点是调度） 与进程不同的是同类的多个线程共享堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈。所以系统在产生一个线程，或者在各个线程切换工作时，负担要比进程小得多 2. 线程与进程的关系与区别及优缺点 2.1 进程与线程的关系 一个进程包含多个线程 2.2 进程与线程的区别 进程是资源分配的最小单位，线程是程序执行的最小单位 线程之间的通信更方便，因为同一进程下的线程共享全局变量，静态变量等数据，而进程之间的通信需要以IPC的方式进行通信 进程有自己独立的地址空间，创建销毁开销大 线程是共享进程中的数据的，使用相同的地址空间，创建、切换、销毁开销小 各进程是独立的，而线程则不一定。因为同一进程中的线程极有可能相互影响 3. 进程与线程的内存区域 下图是java 的内存区域 从上图可以看出：一个进程中可以有多个线程，多个线程共享堆和方法区（JDK1.8之后的元空间）资源，但是每个线程有自己的程序计数器、虚拟机栈和本地方法栈 3.1 从内存区域角度来分析进程与线程区别 线程是进程划分成更小的运行单位 线程和进程最大的不同在于各进程是独立的，而线程则不一定。因为同一进程中的线程极有可能相互影响 线程执行开销小，但不利于资源的管理和保护，而进程相反 3.2 程序计数器为什么是私有的？ 程序计数器主要有下面两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 需要注意的是，如果执行的是 native 方法，那么程序计数器记录的是 undefined 地址，只有执行的是 Java 代码时程序计数器记录的才是下一条指令的地址。 所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。 3.3 虚拟机栈和本地方法栈为什么是私有的 虚拟机栈： 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 本地方法栈： 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。 3.4. 一句话简单了解堆和方法区 堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (所有对象都在这里分配内存)，方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 4. 并发和并行 并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行： 单位时间内，多个任务同时执行。 5. 为什么要使用多线程 从计算机底层来说: 线程比作轻量级的进程，是程序执行的最小单位，线程间的切换和调度成本远远小于进程 多核CPU时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销 从当代互联网发展趋势来说： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正式开发高并发系统的基础 利用好多线程机制可以大大提高系统整体的并发能力以及性能 6. 多线程可能带来的问题 并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如： 内存泄漏 上下文切换 死锁 还有受限于硬件和软件的资源闲置问题。 7. 什么是上下文切换 当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换到回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换 7.1 为什么需要上下文切换 多线程编程中一般线程的个数都大于CPU核心的个数，而一个CPU核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效的执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式，当一个线程的时间片用完的时候就会重新处于就绪状态让其他线程使用，这个过程就是上下文切换 7.2 为什么说上下文切换消耗时间 上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。 8. 为什么我们不能直接调用 run() 方法？ 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 9. 什么是线程安全 当多个线程访问同一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替运行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获取正确的结果，那这个对象是线程安全的。 《深入java虚拟机》 "},"base/thread/线程生命周期.html":{"url":"base/thread/线程生命周期.html","title":"线程生命周期","keywords":"","body":"线程生命周期1. 线程的生命周期2. sleep() 方法和wait()方法区别和共同点线程生命周期 1. 线程的生命周期 Java 线程在运行的生命周期中的指定时刻只可能处于下面6种不同状态的其中一个状态 状态名称 说明 NEW 初始状态，线程被构建，但是还没调用start方法 RUNNABLE 运行状态，Java线程将操作系统中的就绪和运行两种状态统称为”运行中“ BLOCKED 阻塞状态，表示线程阻塞与所 WAITING 等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作（通知或中断） TIME_WAITING 超时等待状态，该状态不同与WAITING,他是可以在指定时间自行返回的 TERMINATED 终止状态，表示当前线程已经执行完毕 线程在生命周期中并不是固定处于某一个状态，而是随着代码的执行在不同状态之间切换。状态变迁如下图所示 由上图可以看出： NEW（新建） 线程创建之后它将处于 NEW（新建） 状态 RUNNABLE(运行状态) 调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java 虚拟机（JVM）中的 RUNNABLE 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 WAITING (等待) 当线程执行 wait()方法之后，线程进入WAITING (等待)状态，进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态 TIME_WAITING(超时等待) TIME_WAITING(超时等待)状态相当于在等待状态的基础上加上超时限制，比如sleep(long millis)方法或wait(long millis)方法可以将java线程置于 TIMED WAITING 状态，当超时时间到达后 JAVA线程将会返回RUNNABLE 状态 BLOCKED（阻塞） 当线程调用同步方法时，在没有获取到锁的情况下，线程将进入BLOCKED（阻塞） 状态 TERMINATED（终止） 线程在执行Runnale的run() 方法之后将会进入到TERMINATED（终止）状态 2. sleep() 方法和wait()方法区别和共同点 两者最主要的区别在于：sleep方法没有释放锁，而wait 方法释放了锁 两者都可以暂停线程的执行 wait通常被用于线程间的交互/通信，sleep通常被用于暂停执行 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的notify()或者notifyAll方法 sleep（）方法执行完成后，线程会自动苏醒，或者可以使用wait(long timeout)超时后线程会自动苏醒 "},"base/thread/线程通信.html":{"url":"base/thread/线程通信.html","title":"线程通信(等待通知wait/notify机制)","keywords":"","body":"线程通信(等待通知wait/notify机制)1. 等待/通知机制介绍1.1 不使用等待/通知机制（轮询）1.2 什么是等待/通知机制1.2.1 等待/通知生活中的案例原型1.2.2 简介1.3 等待/通知机制的相关方法2. 等待/通知机制的实现2.1 实现案例2.2 synchronized关键字在线程通信中的作用3.相关知识点3.1 notify()锁不释放3.2 Thread.join()线程通信(等待通知wait/notify机制) 1. 等待/通知机制介绍 1.1 不使用等待/通知机制（轮询） 当两个线程之间存在生产者和消费者关系，也就是说第一个线程（生产者）做相应的操作然后第二个线程（消费者）感知到了变化又进行相应的操作 1.1.1 轮询方式案例 while(value=desire){ doSomething(); } 假设这个value值就是第一个线程操作的结果，doSomething()是第二个线程要做的事，当满足条件value=desire后才执行doSomething()。 1.1.2 轮询方式缺点 第二个语句不停过通过轮询机制来检测判断条件是否成立。如果轮询时间的间隔太小会浪费CPU资源，轮询时间的间隔太大，就可能取不到自己想要的数据。 所以这里就需要我们今天讲到的等待/通知（wait/notify）机制来解决这两个矛盾。 1.2 什么是等待/通知机制 1.2.1 等待/通知生活中的案例原型 等待/通知机制在我们生活中比比皆是，一个形象的例子就是厨师和服务员之间就存在等待/通知机制。 厨师做完一道菜的时间是不确定的，所以菜到服务员手中的时间是不确定的； 服务员就需要去“等待（wait）”； 厨师把菜做完之后，按一下铃，这里的按铃就是“通知（nofity）”； 服务员听到铃声之后就知道菜做好了，他可以去端菜了。 1.2.2 简介 等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()/notifyAll()方法，线程A收到通知后退出等待队列，进入可运行状态，进而执行后续操作。上诉两个线程通过对象O来完成交互，而对象上的wait()方法和notify()/notifyAll()方法的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。 1.3 等待/通知机制的相关方法 方法名称 描述 notify() 随机唤醒等待队列中等待同一共享资源的“一个线程”，并使该线程退出等待队列，进入可运行状态，也就是notify()方法仅通知“一个线程” notifyAll() 使所有正在等待队列中等待同一共享资源的 “全部线程” 退出等待队列，进入可运行状态。此时，优先级最高的那个线程最先执行，但也有可能是随机执行，这取决于JVM虚拟机的实现 wait() 使调用该方法的线程释放共享资源锁，然后从运行状态退出，进入等待队列，直到被再次唤醒 wait(long) 超时等待一段时间，这里的参数时间是毫秒，也就是等待长达n毫秒，如果没有通知就超时返回 wait(long，int) 对于超时时间更细力度的控制，可以达到纳秒 2. 等待/通知机制的实现 2.1 实现案例 MyList.java public class MyList { private static List list = new ArrayList(); public static void add() { list.add(\"anyString\"); } public static int size() { return list.size(); } } ThreadA.java public class ThreadA extends Thread { private Object lock; public ThreadA(Object lock) { super(); this.lock = lock; } @Override public void run() { try { synchronized (lock) { if (MyList.size() != 5) { System.out.println(\"wait begin \" + System.currentTimeMillis()); lock.wait(); System.out.println(\"wait end \" + System.currentTimeMillis()); } } } catch (InterruptedException e) { e.printStackTrace(); } } } ThreadB.java public class ThreadB extends Thread { private Object lock; public ThreadB(Object lock) { super(); this.lock = lock; } @Override public void run() { try { synchronized (lock) { for (int i = 0; i Run.java public class Run { public static void main(String[] args) { try { Object lock = new Object(); ThreadA a = new ThreadA(lock); a.start(); Thread.sleep(50); ThreadB b = new ThreadB(lock); b.start(); } catch (InterruptedException e) { e.printStackTrace(); } } } 输出的结果 2019-09-21 00:48:52 JRebel: wait begin 1568998132460 添加了1个元素! 添加了2个元素! 添加了3个元素! 添加了4个元素! 已发出通知！ 添加了5个元素! 添加了6个元素! 添加了7个元素! 添加了8个元素! 添加了9个元素! 添加了10个元素! wait end 1568998142540 2.2 synchronized关键字在线程通信中的作用 synchronized关键字可以将任何一个Object对象作为同步对象看待，而java为每个Object 都实现了等待/通知（wait/notify）机制的相关方法，他们必须用synchronized关键字同步的Object的临界区内。 通过调用wait()方法可以使处于临界区内的线程进入等待状态，同时释放被同步对象的锁 而notify()方法可以唤醒一个因调用wait操作而处于阻塞状态中的线程，使其进入就绪状态。 被重新唤醒的线程会视图重新获得临界区的控制权也就是锁，并继续执行wait方法之后的代码。如果发出notify操作时没有处于阻塞状态中的线程，那么该命令会被忽略。 3.相关知识点 3.1 notify()锁不释放 当方法wait()被执行后，锁自动被释放，但执行玩notify()方法后，锁不会自动释放。必须执行完notify()方法所在的synchronized代码块后才释放。 3.2 Thread.join() 3.2.1 Thread.join()使用背景 在很多情况下，主线程生成并起动了子线程，如果子线程里要进行大量的耗时的运算，主线程往往将于子线程之前结束，但是如果主线程处理完其他的事务后，需要用到子线程的处理结果，也就是主线程需要等待子线程执行完成之后再结束，这个时候就要用到join()方法了。另外，一个线程需要等待另一个线程也需要用到join()方法。 Thread类除了提供join()方法之外，还提供了join(long millis)、join(long millis, int nanos)两个具有超时特性的方法。这两个超时方法表示，如果线程thread在指定的超时时间没有终止，那么将会从该超时方法中返回。 3.2.2 子线程执行完主线程才退出 public class Test { public static void main(String[] args) throws InterruptedException { MyThread threadTest = new MyThread(); threadTest.start(); //Thread.sleep(?);//因为不知道子线程要花的时间这里不知道填多少时间 threadTest.join(); System.out.println(\"我想当threadTest对象执行完毕后我再执行\"); } static public class MyThread extends Thread { @Override public void run() { System.out.println(\"我想先执行\"); } } } 上面的代码仅仅加上了一句：threadTest.join();。在这里join方法的作用就是主线程需要等待子线程执行完成之后再结束。 "},"base/thread/死锁.html":{"url":"base/thread/死锁.html","title":"死锁","keywords":"","body":"死锁1. 什么是死锁1.1 案例1.2 案例代码2. 死锁产生的四个必备条件3. 如何避免死锁3.1 破坏互斥条件3.2 破坏请求与保持条件3.3 破坏不可剥夺条件3.4 破坏循环等待条件死锁 1. 什么是死锁 多个线程同时被阻塞，他们中的一个或者全部都在等待某个资源被释放。由于线程被无限期的阻塞，因此程序不可能正常终止 1.1 案例 如下图所示，线程A持有资源2，线程B持有资源 1,他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态 1.2 案例代码 通过例子来模拟线程死锁 public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } 输出 Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程A 通过synchronized（resource1） 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 2. 死锁产生的四个必备条件 互斥条件：该资源任意一个时刻只由一个线程占用 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放 不可剥夺条件：线程已获取的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系 3. 如何避免死锁 我们只要破坏产生死锁的四个条件中的一个就可以 3.1 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问） 3.2 破坏请求与保持条件 一次性申请所有的资源 3.3 破坏不可剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放他占有的资源 3.4 破坏循环等待条件 靠按顺序申请资源来预防，按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件 "},"base/concurrent/synchronized关键字.html":{"url":"base/concurrent/synchronized关键字.html","title":"synchronized关键字","keywords":"","body":"synchronized关键字1. 简介1.1 synchronized 如何保证多线程之间访问资源的同步性1.2 java早期synchronized效率为什么低2. synchronized关键字最主要的三种使用方式2.1 单例模式中双层校验锁3. synchronized 关键字的底层原理3.1 synchronized 同步语句块的情况3.2 ynchronized 修饰方法的的情况4. JDK1.6 之后的synchronized 关键字底层做了哪些优化5. 谈谈 synchronized和ReentrantLock 的区别synchronized关键字 1. 简介 synchronized关键字解决的是多个线程之间访问资源的同步性，synchronize关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行 1.1 synchronized 如何保证多线程之间访问资源的同步性 原子性 可见性 有序性 具体可参照volatile关键字 1.2 java早期synchronized效率为什么低 在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 互斥锁（Mutex Lock ）来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。 1.2.1 Java 6之后效率提高原因 庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 2. synchronized关键字最主要的三种使用方式 修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法: :也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！ 2.1 单例模式中双层校验锁 public class Singleton { private volatile static Singleton instance; private Singleton() { } public static Singleton getiInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (instance == null) { //类对象加锁 synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } } 另外，需要注意 instance 采用 volatile 关键字修饰也是很有必要。 instance = new Singleton(); 这段代码其实是分为三步执行： 为 instance 分配内存空间 初始化 instance 将 instance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出先问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getInstance() 后发现 instance 不为空，因此返回 instance，但此时 instance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 3. synchronized 关键字的底层原理 synchronized 关键字底层原理属于 JVM 层面。 3.1 synchronized 同步语句块的情况 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\"synchronized 代码块\"); } } } 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 monitor(monitor对象存在于每个Java对象的对象头中，synchronized 锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因) 的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行 monitorexit 指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 3.2 ynchronized 修饰方法的的情况 public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\"synchronized 方法\"); } } synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法，JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 4. JDK1.6 之后的synchronized 关键字底层做了哪些优化 JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。 锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 5. 谈谈 synchronized和ReentrantLock 的区别 ① 两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 ② synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。 ③ ReentrantLock 比 synchronized 增加了一些高级功能 相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：①等待可中断；②可实现公平锁；③可实现选择性通知（锁可以绑定多个条件） ReentrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制，ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition() 方法。Condition是JDK1.5之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是Condition接口默认提供的。而synchronized关键字就相当于整个Lock对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 如果你想使用上述功能，那么选择ReentrantLock是一个不错的选择。 ④ 性能已不是选择标准 "},"base/concurrent/ReentrantLock.html":{"url":"base/concurrent/ReentrantLock.html","title":"ReentrantLock(补充)","keywords":"","body":"ReentrantLock(补充)参考文章ReentrantLock(补充) 参考文章 ReentrantLock(重入锁)功能详解和应用演示 "},"base/concurrent/volatile关键字.html":{"url":"base/concurrent/volatile关键字.html","title":"volatile关键字","keywords":"","body":"volatile关键字1. 内存模型的相关概念1.1 案例1.2 多线程面临的问题2. 并发编程中的三个概念2.1 原子性2.2 可见性2.3 有序性3. Java内存模型3.1 原子性3.2 可见性3.3 有序性4. 深入剖析volatile关键字4.1 volatile关键字的两层语义4.2 volatile保证原子性吗？2.3 volatile能保证有序性吗2.4 volatile的原理和实现机制5. 使用volatile关键字的场景6. 说说 synchronized 关键字和 volatile 关键字的区别参考文章volatile关键字 volatile 关键字与Java内存模型有关，所以先要掌握内存模型相关的概念和知识，然后我们再分析volatitle关键字的实现原理，最后总结使用volatile的使用场景 1. 内存模型的相关概念 CPU的速度远比内存块，所以对数据的读写，放在CPU的高速缓存中完成 大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令的过程汇总，势必涉及到数据的读取和写入。由于程序运行过程中临时数据是存放在主存（物理内存）当中的，这是就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互进行。会大大降低指令执行速度。因此CPU里面就有了高速缓存 程序运行过程中，将需要的数据从主存复制一份到CPU高速缓存,CPU进行计算时从高速缓存读取数据和向其中写入数据，然后刷新到主存中 1.1 案例 i = i + 1; 当线程执行这个语句时，会先从主存当中读取i的值，然后复制一份到高速缓存当中，然后CPU执行指令对i进行加1操作，然后将数据写入高速缓存，最后将高速缓存中i最新的值刷新到主存当中。 1.2 多线程面临的问题 1.2.1 问题原因 在多核CPU中每条线程可能运行在不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。本文我们以多核CPU为例。 1.2.2 缓存一致性 ​ 比如同时有2个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但是事实会是这样吗？ 可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作之后，i的值为1，然后线程2把i的值写入内存。 　　最终结果i的值是1，而不是2。这就是著名的缓存一致性问题。通常称这种被多个线程访问的变量为共享变量。 　　也就是说，如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。 1.2.3 如何解决 通过在总线加LOCK锁的方式 通过缓存一致性协议 这两种方式都是硬件层面上提供的方式 1.2.3.1 通过在总线加LOCK锁的方式 在早期的CPU当中，是通过在总线上加LOCK#锁的形式来解决缓存不一致的问题。因为CPU和其他部件进行通信都是通过总线来进行的，如果对总线加LOCK#锁的话，也就是说阻塞了其他CPU对其他部件访问（如内存），从而使得只能有一个CPU能使用这个变量的内存。比如上面例子中 如果一个线程在执行 i = i +1，如果在执行这段代码的过程中，在总线上发出了LCOK#锁的信号，那么只有等待这段代码完全执行完毕之后，其他CPU才能从变量i所在的内存读取变量，然后进行相应的操作。这样就解决了缓存不一致的问题。 　　但是上面的方式会有一个问题，由于在锁住总线期间，其他CPU无法访问内存，导致效率低下。 1.2.3.2 缓存一致性协议 　所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。 2. 并发编程中的三个概念 在并发编程中，我们通常会遇到以下三个问题：原子性问题，可见性问题，有序性问题 2.1 原子性 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 一个很经典的例子就是银行账户转账问题： 　　比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。 　　试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。然后又从B取出了500元，取出500元之后，再执行 往账户B加上1000元 的操作。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。 　　所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。 　　同样地反映到并发编程中会出现什么结果呢？ 　　举个最简单的例子，大家想一下假如为一个32位的变量赋值过程不具备原子性的话，会发生什么后果？ i = 9; 　　假若一个线程执行到这个语句时，我暂且假设为一个32位的变量赋值包括两个过程：为低16位赋值，为高16位赋值。 　　那么就可能发生一种情况：当将低16位数值写入之后，突然被中断，而此时又有一个线程去读取i的值，那么读取到的就是错误的数据。 2.2 可见性 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 举个简单的例子，看下面这段代码： //线程1执行的代码 int i = 0; i = 10; //线程2执行的代码 j = i; 　　假若执行线程1的是CPU1，执行线程2的是CPU2。由上面的分析可知，当线程1执行 i =10这句时，会先把i的初始值加载到CPU1的高速缓存中，然后赋值为10，那么在CPU1的高速缓存当中i的值变为10了，却没有立即写入到主存当中。 　　此时线程2执行 j = i，它会先去主存读取i的值并加载到CPU2的缓存当中，注意此时内存当中i的值还是0，那么就会使得j的值为0，而不是10. 　　这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。 2.3 有序性 有序性：即程序执行的顺序按照代码的先后顺序执行。 举个简单的例子，看下面这段代码： int i = 0; boolean flag = false; i = 1; //语句1 flag = true; //语句2 上面代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。 下面解释一下什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 　　比如上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。 　　但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？再看下面一个例子： int a = 10; //语句1 int r = 2; //语句2 a = a + 3; //语句3 r = a*a; //语句4 这段代码有4个语句，那么可能的一个执行顺序是： 那么可不可能是这个执行顺序呢： 语句2 语句1 语句4 语句3 　　不可能，因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。 　　虽然重排序不会影响单个线程内程序执行的结果，但是多线程呢？下面看一个例子： //线程1: context = loadContext(); //语句1 inited = true; //语句2 //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 上面代码中，由于语句1和语句2没有数据依赖性，因此可能会被重排序。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行doSomethingwithconfig(context)方法，而此时context并没有被初始化，就会导致程序出错。 　　从上面可以看出，指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。 要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 3. Java内存模型 Java内存模型为我们提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。 在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。 Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 举个简单的例子：在java中，执行下面这个语句： i = 10; 　　执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。 　　那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？ 3.1 原子性 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。 Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 请分析以下哪些操作是原子性操作： `x = ``10``; ``//语句1``y = x; ``//语句2``x++; ``//语句3``x = x + ``1``; ``//语句4` 　　咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。 　　语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。 　　语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。 　　同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。 　　所以上面4个语句只有语句1的操作具备原子性。 　　也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。 　　不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。 从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。 3.2 可见性 对于可见性，Java提供了volatile关键字来保证可见性。 通过synchronized和Lock也能够保证可见性 ​ 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 　　而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 ​ 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 3.3 有序性 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 4. 深入剖析volatile关键字 4.1 volatile关键字的两层语义 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 　　1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 　　2）禁止进行指令重排序。 先看一段代码，假如线程1先执行，线程2后执行： `//线程1``boolean` `stop = ``false``;``while``(!stop){`` ``doSomething();``}` `//线程2``stop = ``true``;` 　　这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。 　　下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。 　　那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。 但是用volatile修饰之后就变得不一样了： 　　第一：使用volatile关键字会强制将修改的值立即写入主存； 　　第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）； 　　第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。 　　那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。 　　那么线程1读取到的就是最新的正确的值。 4.2 volatile保证原子性吗？ 下面看一个例子： public class Test { public volatile int inc = 0; public void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 　　大家想一下这段程序的输出结果是多少？也许有些朋友认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。 　　可能有的朋友就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。 　　这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。 　　在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现： 　　假如某个时刻变量inc的值为10， 　　线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了； 　　然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。 　　然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。 　　那么两个线程分别进行了一次自增操作后，inc只增加了1。 　　解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 　　根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。 　　把上面的代码改成以下任何一种都可以达到效果： 　　采用synchronized： public class Test { public int inc = 0; public synchronized void increase() { inc++; } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 采用Lock： public class Test { public int inc = 0; Lock lock = new ReentrantLock(); public void increase() { lock.lock(); try { inc++; } finally{ lock.unlock(); } } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 采用AtomicInteger： public class Test { public AtomicInteger inc = new AtomicInteger(); public void increase() { inc.getAndIncrement(); } public static void main(String[] args) { final Test test = new Test(); for(int i=0;i1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); } } 　　在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。 2.3 volatile能保证有序性吗 在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 　　1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 　　2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 　　可能上面说的比较绕，举个简单的例子： //x、y为非volatile变量 //flag为volatile变量 x = 2; //语句1 y = 0; //语句2 flag = true; //语句3 x = 4; //语句4 y = -1; //语句5 　　由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 　　并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。 　　那么我们回到前面举的一个例子： //线程1: context = loadContext(); //语句1 inited = true; //语句2 //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 　　前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么久可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。 　　这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。 2.4 volatile的原理和实现机制 下面这段话摘自《深入理解Java虚拟机》： 　　“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令” 　　lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能： 　　1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 　　2）它会强制将对缓存的修改操作立即写入主存； 　　3）如果是写操作，它会导致其他CPU中对应的缓存行无效。 5. 使用volatile关键字的场景 synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件： 　　1）对变量的写操作不依赖于当前值 　　2）该变量没有包含在具有其他变量的不变式中 　　实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。 　　事实上，我的理解就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。 　　下面列举几个Java中使用volatile的几个场景。 1.状态标记量 volatile boolean flag = false; while(!flag){ doSomething(); } public void setFlag() { flag = true; } volatile boolean inited = false; //线程1: context = loadContext(); inited = true; //线程2: while(!inited ){ sleep() } doSomethingwithconfig(context); 2.double check class Singleton{ private volatile static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if(instance==null) { synchronized (Singleton.class) { if(instance==null) instance = new Singleton(); } } return instance; } } 6. 说说 synchronized 关键字和 volatile 关键字的区别 volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。synchronized关键字在JavaSE1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后执行效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞 volatile关键字能保证数据的可见性，但不能保证数据的原子性。synchronized关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized关键字解决的是多个线程之间访问资源的同步性 参考文章 Java并发编程：volatile关键字解析 "},"base/concurrent/线程池.html":{"url":"base/concurrent/线程池.html","title":"线程池","keywords":"","body":"线程池1. 为什么要使用线程池2. 实现Runnable接口和Callable接口的区别3. 执行execute()方法和submit()方法的区别是什么呢？4. 如何创建线程池线程池 1. 为什么要使用线程池 降低资源消耗：通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度：当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理行性：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 2. 实现Runnable接口和Callable接口的区别 两者的区别在于 Runnable 接口不会返回结果但是 Callable 接口可以返回结果。 备注： 工具类Executors可以实现Runnable对象和Callable对象之间的相互转换。（Executors.callable（Runnable task）或Executors.callable（Runnable task，Object resule）） 3. 执行execute()方法和submit()方法的区别是什么呢？ 1)execute() 方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； 2)submit() 方法用于提交需要返回值的任务。线程池会返回一个Future类型的对象，通过这个Future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 4. 如何创建线程池 《阿里巴巴Java开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致OOM。 请求队列（LinkedBlockingQueue）不是指默认是 Integer.MAX_VALUE CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致OOM。 他们默认最大线程都是MAX 方式一：通过构造方法实现 方式二：通过Executor 框架的工具类Executors来实现 我们可以创建三种类型的ThreadPoolExecutor： FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 对应Executors工具类中的方法如图所示： "},"base/concurrent/ThreadPoolExecutor类.html":{"url":"base/concurrent/ThreadPoolExecutor类.html","title":"ThreadPoolExecutor类","keywords":"","body":"ThreadPoolExecutor类1. ThreadPoolExecutor类构造器2. 构造器参数含义2.1 corePoolSize：核心池的大小2.2 maximumPoolSize：线程池最大线程数2.3 keepAliveTime：线程没有任务执行时最多保持多久时间会终止2.4 unit：参数keepAliveTime的时间单位2.5 workQueue: 阻塞队列2.6 threadFactory: 线程工厂2.7 handler：拒绝处理任务时的策略3. ThreadPoolExecutor的继承关系4. 在ThreadPoolExecutor类中有几个非常重要的方法：4.1 execute()方法4.2 submit()方法4.3 shutdown()和shutdownNow()：关闭线程池参考文章ThreadPoolExecutor类 1. ThreadPoolExecutor类构造器 java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类，因此如果要透彻了解Java中线程池，就需要先了解这个类。下面我们来看一ThreadPoolExecutor类的具体实现源码 在ThreadPoolExecutor类中提供了四个构造方法： public class ThreadPoolExecutor extends AbstractExecutorService { // 第一个构造器 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } // 第二个构造器 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler); } // 第三个构造器 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, RejectedExecutionHandler handler) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler); } // 第四个构造器 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize 从上面的代码可以得知，ThreadPoolExecutor继承了AbstractExecutorService类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。 2. 构造器参数含义 2.1 corePoolSize：核心池的大小 corePoolSize：核心池的大小 在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务。当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； 除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法预创建线程。即在没有任务到来之前就创建corePoolSize个线程或者一个线程 2.2 maximumPoolSize：线程池最大线程数 maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程； 2.3 keepAliveTime：线程没有任务执行时最多保持多久时间会终止 keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。 默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize 即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize 但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； 2.4 unit：参数keepAliveTime的时间单位 unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性： TimeUnit.DAYS; //天 TimeUnit.HOURS; //小时 TimeUnit.MINUTES; //分钟 TimeUnit.SECONDS; //秒 TimeUnit.MILLISECONDS; //毫秒 TimeUnit.MICROSECONDS; //微妙 TimeUnit.NANOSECONDS; //纳秒 2.5 workQueue: 阻塞队列 workQueue：一个阻塞队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响，一般来说，这里的阻塞队列有以下几种选择： ArrayBlockingQueue; LinkedBlockingQueue; SynchronousQueue; ArrayBlockingQueue和PriorityBlockingQueue使用较少，一般使用LinkedBlockingQueue和Synchronous。线程池的排队策略与BlockingQueue有关。 2.6 threadFactory: 线程工厂 threadFactory：线程工厂，主要用来创建线程； 2.7 handler：拒绝处理任务时的策略 handler：表示当拒绝处理任务时的策略，有以下四种取值： ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 3. ThreadPoolExecutor的继承关系 从上面给出的ThreadPoolExecutor类的代码可以知道，ThreadPoolExecutor继承了AbstractExecutorService，我们来看一下AbstractExecutorService的实现： public abstract class AbstractExecutorService implements ExecutorService { protected RunnableFuture newTaskFor(Runnable runnable, T value) { }; protected RunnableFuture newTaskFor(Callable callable) { }; public Future submit(Runnable task) {}; public Future submit(Runnable task, T result) { }; public Future submit(Callable task) { }; private T doInvokeAny(Collection> tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException { }; public T invokeAny(Collection> tasks) throws InterruptedException, ExecutionException { }; public T invokeAny(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException { }; public List> invokeAll(Collection> tasks) throws InterruptedException { }; public List> invokeAll(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException { }; } AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。 　　我们接着看ExecutorService接口的实现： public interface ExecutorService extends Executor { void shutdown(); boolean isShutdown(); boolean isTerminated(); boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; Future submit(Callable task); Future submit(Runnable task, T result); Future submit(Runnable task); List> invokeAll(Collection> tasks) throws InterruptedException; List> invokeAll(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException; T invokeAny(Collection> tasks) throws InterruptedException, ExecutionException; T invokeAny(Collection> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } 而ExecutorService又是继承了Executor接口，我们看一下Executor接口的实现： public interface Executor { void execute(Runnable command); } 到这里，大家应该明白了ThreadPoolExecutor、AbstractExecutorService、ExecutorService和Executor几个之间的关系了。 Executor是一个顶层接口，在它里面只声明了一个方法execute(Runnable)，返回值为void，参数为Runnable类型，从字面意思可以理解，就是用来执行传进去的任务的； 　　然后ExecutorService接口继承了Executor接口，并声明了一些方法：submit、invokeAll、invokeAny以及shutDown等； 　　抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法； 然后ThreadPoolExecutor继承了类AbstractExecutorService。 4. 在ThreadPoolExecutor类中有几个非常重要的方法： 4.1 execute()方法 execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 4.2 submit()方法 submit()方法是在ExecutorService中声明的方法，在AbstractExecutorService就已经有了具体的实现，在ThreadPoolExecutor中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果，去看submit()方法的实现，会发现它实际上还是调用的execute()方法，只不过它利用了Future来获取任务执行结果（Future相关内容将在下一篇讲述）。 4.3 shutdown()和shutdownNow()：关闭线程池 参考文章 Java并发编程：线程池的使用 "},"base/concurrent/线程池的具体实现原理.html":{"url":"base/concurrent/线程池的具体实现原理.html","title":"线程池的具体实现原理","keywords":"","body":"线程池的具体实现原理1. 线程池状态2. 任务的执行2.1 线程池例子2.2 提交到最终执行完毕经历了哪些过程2.3 总结3. 线程池中的线程初始化4. 任务缓存队列及排队策略5. 任务拒绝策略6. 线程池的关闭7. 线程池容量的动态调整线程池的具体实现原理 在上一节我们从宏观上介绍了ThreadPoolExecutor，下面我们来深入解析一下线程池的具体实现原理，将从下面几个方面讲解： 　　1.线程池状态 　　2.任务的执行 　　3.线程池中的线程初始化 　　4.任务缓存队列及排队策略 　　5.任务拒绝策略 　　6.线程池的关闭 　　7.线程池容量的动态调整 1. 线程池状态 在ThreadPoolExecutor中定义了一个volatile变量，另外定义了几个static final变量表示线程池的各个状态： volatile int runState; static final int RUNNING = 0; static final int SHUTDOWN = 1; static final int STOP = 2; static final int TERMINATED = 3; runState表示当前线程池的状态，它是一个volatile变量用来保证线程之间的可见性； 下面的几个static final变量表示runState可能的几个取值。 当创建线程池后，初始时，线程池处于RUNNING状态； 如果调用了shutdown()方法，则线程池处于SHUTDOWN状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕； 如果调用了shutdownNow()方法，则线程池处于STOP状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务； 当线程池处于SHUTDOWN或STOP状态，并且所有工作线程已经销毁，任务缓存队列已经清空或执行结束后，线程池被设置为TERMINATED状态。 2. 任务的执行 　　在了解将任务提交给线程池到任务执行完毕整个过程之前，我们先来看一下ThreadPoolExecutor类中其他的一些比较重要成员变量： private final BlockingQueue workQueue; //任务缓存队列，用来存放等待执行的任务 private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小 //、runState等）的改变都要使用这个锁 private final HashSet workers = new HashSet(); //用来存放工作集 private volatile long keepAliveTime; //线程存货时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间 private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列） private volatile int maximumPoolSize; //线程池最大能容忍的线程数 private volatile int poolSize; //线程池中当前的线程数 private volatile RejectedExecutionHandler handler; //任务拒绝策略 private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程 private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数 private long completedTaskCount; //用来记录已经执行完毕的任务个数 每个变量的作用都已经标明出来了，这里要重点解释一下corePoolSize、maximumPoolSize、largestPoolSize三个变量。 2.1 线程池例子 corePoolSize在很多地方被翻译成核心池大小，其实我的理解这个就是线程池的大小。举个简单的例子： 　　假如有一个工厂，工厂里面有10个工人，每个工人同时只能做一件任务。 　　因此只要当10个工人中有工人是空闲的，来了任务就分配给空闲的工人做； 　　当10个工人都有任务在做时，如果还来了任务，就把任务进行排队等待； 　　如果说新任务数目增长的速度远远大于工人做任务的速度，那么此时工厂主管可能会想补救措施，比如重新招4个临时工人进来； 　　然后就将任务也分配给这4个临时工人做； 　　如果说着14个工人做任务的速度还是不够，此时工厂主管可能就要考虑不再接收新的任务或者抛弃前面的一些任务了。 　　当这14个工人当中有人空闲时，而新任务增长的速度又比较缓慢，工厂主管可能就考虑辞掉4个临时工了，只保持原来的10个工人，毕竟请额外的工人是要花钱的。 　　这个例子中的corePoolSize就是10，而maximumPoolSize就是14（10+4） 也就是说corePoolSize就是线程池大小，maximumPoolSize在我看来是线程池的一种补救措施，即任务量突然过大时的一种补救措施。 　　不过为了方便理解，在本文后面还是将corePoolSize翻译成核心池大小。 　　largestPoolSize只是一个用来起记录作用的变量，用来记录线程池中曾经有过的最大线程数目，跟线程池的容量没有任何关系。 2.2 提交到最终执行完毕经历了哪些过程 　在ThreadPoolExecutor类中，最核心的任务提交方法是execute()方法，虽然通过submit也可以提交任务，但是实际上submit方法里面最终调用的还是execute()方法，所以我们只需要研究execute()方法的实现原理即可： public void execute(Runnable command) { if (command == null) throw new NullPointerException(); if (poolSize >= corePoolSize || !addIfUnderCorePoolSize(command)) { if (runState == RUNNING && workQueue.offer(command)) { if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); } else if (!addIfUnderMaximumPoolSize(command)) reject(command); // is shutdown or saturated } } 首先，判断提交的任务command是否为null，若是null，则抛出空指针异常； if (poolSize >= corePoolSize || !addIfUnderCorePoolSize(command)) 由于是或条件运算符，所以先计算前半部分的值，如果线程池中当前线程数不小于核心池大小，那么就会直接进入下面的if语句块了。 　　如果线程池中当前线程数小于核心池大小，则接着执行后半部分，也就是执行 如果执行完addIfUnderCorePoolSize这个方法返回false，则继续执行下面的if语句块，否则整个方法就直接执行完毕了。 　　如果执行完addIfUnderCorePoolSize这个方法返回false，然后接着判断： `if` `(runState == RUNNING && workQueue.offer(command))` 　　如果当前线程池处于RUNNING状态，则将任务放入任务缓存队列；如果当前线程池不处于RUNNING状态或者任务放入缓存队列失败，则执行： `addIfUnderMaximumPoolSize(command)` 　　如果执行addIfUnderMaximumPoolSize方法失败，则执行reject()方法进行任务拒绝处理。 　　回到前面： `if` `(runState == RUNNING && workQueue.offer(command))` 　　这句的执行，如果说当前线程池处于RUNNING状态且将任务放入任务缓存队列成功，则继续进行判断： `if` `(runState != RUNNING || poolSize == ``0``)` 　　这句判断是为了防止在将此任务添加进任务缓存队列的同时其他线程突然调用shutdown或者shutdownNow方法关闭了线程池的一种应急措施。如果是这样就执行： `ensureQueuedTaskHandled(command)` 　　进行应急处理，从名字可以看出是保证 添加到任务缓存队列中的任务得到处理。 ... 2.3 总结 1）首先，要清楚corePoolSize和maximumPoolSize的含义； 　　2）其次，要知道Worker是用来起到什么作用的； 　　3）要知道任务提交给线程池之后的处理策略，这里总结一下主要有4点： 如果当前线程池中的线程数目小于corePoolSize，则每来一个任务，就会创建一个线程去执行这个任务； 如果当前线程池中的线程数目>=corePoolSize，则每来一个任务，会尝试将其添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务； 如果当前线程池中的线程数目达到maximumPoolSize，则会采取任务拒绝策略进行处理； 如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。 3. 线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。 在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 　　下面是这2个方法的实现： public boolean prestartCoreThread() { return addIfUnderCorePoolSize(null); //注意传进去的参数是null } public int prestartAllCoreThreads() { int n = 0; while (addIfUnderCorePoolSize(null))//注意传进去的参数是null ++n; return n; } 注意上面传进去的参数是null，根据第2小节的分析可知如果传进去的参数为null，则最后执行线程会阻塞在getTask方法中的 r = workQueue.take(); 　即等待任务队列中有任务。 4. 任务缓存队列及排队策略 在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。 　　workQueue的类型为BlockingQueue，通常可以取下面三种类型： ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE； synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 5. 任务拒绝策略 当线程池的任务缓存队列已满并且线程池中的线程数目达到maximumPoolSize，如果还有任务到来就会采取任务拒绝策略，通常有以下四种策略： ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 6. 线程池的关闭 ThreadPoolExecutor提供了两个方法，用于线程池的关闭，分别是shutdown()和shutdownNow()，其中： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 7. 线程池容量的动态调整 ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()， setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 　　当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。 "},"base/concurrent/线程池使用示例.html":{"url":"base/concurrent/线程池使用示例.html","title":"线程池使用示例","keywords":"","body":"线程池使用示例线程池使用示例 具体使用示例 public class Test { public static void main(String[] args) { ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue(5)); for(int i=0;i执行结果： 线程池中线程数目：1，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 正在执行task 0 线程池中线程数目：2，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 线程池中线程数目：3，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 正在执行task 1 正在执行task 2 线程池中线程数目：4，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 正在执行task 3 线程池中线程数目：5，队列中等待执行的任务数目：0，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：1，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：2，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：3，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：4，已执行玩别的任务数目：0 线程池中线程数目：5，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 4 线程池中线程数目：6，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 线程池中线程数目：7，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 11 正在执行task 10 线程池中线程数目：8，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 12 线程池中线程数目：9，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 13 线程池中线程数目：10，队列中等待执行的任务数目：5，已执行玩别的任务数目：0 正在执行task 14 task 1执行完毕 task 0执行完毕 正在执行task 5 正在执行task 6 task 12执行完毕 task 11执行完毕 正在执行task 8 task 10执行完毕 正在执行task 9 task 3执行完毕 task 2执行完毕 task 4执行完毕 task 13执行完毕 正在执行task 7 task 14执行完毕 task 5执行完毕 task 6执行完毕 task 9执行完毕 task 7执行完毕 task 8执行完毕 从执行结果可以看出，当线程池中线程的数目大于5时，便将任务放入任务缓存队列里面，当任务缓存队列满了之后，便创建新的线程。如果上面程序中，将for循环中改成执行20个任务，就会抛出任务拒绝异常了。 "},"base/concurrent/Executors创建线程池.html":{"url":"base/concurrent/Executors创建线程池.html","title":"Executors创建线程池","keywords":"","body":"Executors创建线程池Executors创建线程池 Executors类中提供的几个静态方法来创建线程池： Executors.newCachedThreadPool(); //创建一个缓冲池，缓冲池容量大小为Integer.MAX_VALUE Executors.newSingleThreadExecutor(); //创建容量为1的缓冲池 Executors.newFixedThreadPool(int); //创建固定容量大小的缓冲池 三个静态方法的具体实现 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue())); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue()); } 从它们的具体实现来看，它们实际上也是调用了ThreadPoolExecutor，只不过参数都已配置好了。 　　newFixedThreadPool创建的线程池corePoolSize和maximumPoolSize值是相等的，它使用的LinkedBlockingQueue； 　　newSingleThreadExecutor将corePoolSize和maximumPoolSize都设置为1，也使用的LinkedBlockingQueue； 　　newCachedThreadPool将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。 "},"base/concurrent/如何合理配置线程池的大小.html":{"url":"base/concurrent/如何合理配置线程池的大小.html","title":"如何合理配置线程池的大小","keywords":"","body":"如何合理配置线程池的大小如何合理配置线程池的大小 一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 　　当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 "},"base/concurrent/ThreadLocal.html":{"url":"base/concurrent/ThreadLocal.html","title":"ThreadLocal","keywords":"","body":"ThreadLocal1. 什么是ThreadLocal2. 为什么要学习ThreadLocal2.1 管理Connection2.2 避免一些参数传递3. ThreadLocal 实现的原理3.1 ThreadLocal 原理总结4. 避免内存泄漏5. 总结参考文章ThreadLocal 1. 什么是ThreadLocal 我们先来看下JDK 的文档介绍 /** * This class provides thread-local variables. These variables differ from * their normal counterparts in that each thread that accesses one (via its * {@code get} or {@code set} method) has its own, independently initialized * copy of the variable. {@code ThreadLocal} instances are typically private * static fields in classes that wish to associate state with a thread (e.g., * a user ID or Transaction ID). * * For example, the class below generates unique identifiers local to each * thread. * A thread's id is assigned the first time it invokes {@code ThreadId.get()} * and remains unchanged on subsequent calls. */ ThreadLocal提供了线程的局部变量。每个线程都可以通过set()和get()来对这个 局部变量进行操作，但不会和其他线程的局部变量进行冲突。实现了线程的数据隔离 简要言之：往ThreadLocal中填充的变量属于当前线程，该变量对其他线程而言是隔离的。 2. 为什么要学习ThreadLocal 从上面可以得出：ThreadLocal可以让我们拥有当前线程的变量，那这个作用有什么用呢？？？ 2.1 管理Connection 最典型的是管理数据库的Connection： 当时在学JDBC的时候，为了方便操作写了一个简单数据库连接池，需要数据库连接池的理由也很简单，频繁创建和关闭Connection是一件非常耗费资源的操作，因此需要创建数据库连接池～ 那么，数据库连接池的连接怎么管理呢？？我们交由ThreadLocal来进行管理。为什么交给它来管理呢？？ThreadLocal能够实现当前线程的操作都是用同一个Connection，保证了事务！ 当时候写的代码： public class DBUtil { //数据库连接池 private static BasicDataSource source; //为不同的线程管理连接 private static ThreadLocal local; static { try { //加载配置文件 Properties properties = new Properties(); //获取读取流 InputStream stream = DBUtil.class.getClassLoader().getResourceAsStream(\"连接池/config.properties\"); //从配置文件中读取数据 properties.load(stream); //关闭流 stream.close(); //初始化连接池 source = new BasicDataSource(); //设置驱动 source.setDriverClassName(properties.getProperty(\"driver\")); //设置url source.setUrl(properties.getProperty(\"url\")); //设置用户名 source.setUsername(properties.getProperty(\"user\")); //设置密码 source.setPassword(properties.getProperty(\"pwd\")); //设置初始连接数量 source.setInitialSize(Integer.parseInt(properties.getProperty(\"initsize\"))); //设置最大的连接数量 source.setMaxActive(Integer.parseInt(properties.getProperty(\"maxactive\"))); //设置最长的等待时间 source.setMaxWait(Integer.parseInt(properties.getProperty(\"maxwait\"))); //设置最小空闲数 source.setMinIdle(Integer.parseInt(properties.getProperty(\"minidle\"))); //初始化线程本地 local = new ThreadLocal<>(); } catch (IOException e) { e.printStackTrace(); } } public static Connection getConnection() throws SQLException { if(local.get()!=null){ return local.get(); }else{ //获取Connection对象 Connection connection = source.getConnection(); //把Connection放进ThreadLocal里面 local.set(connection); //返回Connection对象 return connection; } } //关闭数据库连接 public static void closeConnection() { //从线程中拿到Connection对象 Connection connection = local.get(); try { if (connection != null) { //恢复连接为自动提交 connection.setAutoCommit(true); //这里不是真的把连接关了,只是将该连接归还给连接池 connection.close(); //既然连接已经归还给连接池了,ThreadLocal保存的Connction对象也已经没用了 local.remove(); } } catch (SQLException e) { e.printStackTrace(); } } } 同样的，Hibernate对Connection的管理也是采用了相同的手法(使用ThreadLocal，当然了Hibernate的实现是更强大的)～ 2.2 避免一些参数传递 避免一些参数的传递的理解可以参考一下Cookie和Session： 3. ThreadLocal 实现的原理 public void set(T value) { // 得到当前线程对象 Thread t = Thread.currentThread(); // 这里获取ThreadLocalMap ThreadLocalMap map = getMap(t); // 如果map存在，则将当前线程对象t作为key，要存储的对象作为value存到map里面去 if (map != null) map.set(this, value); else createMap(t, value); } 上面有个ThreadLocalMap，我们去看看这是什么？ static class ThreadLocalMap { /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as \"stale entries\" in the code that follows. */ static class Entry extends WeakReference> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } //....很长 } 通过上面我们可以发现的是ThreadLocalMap是ThreadLocal的一个内部类。用Entry类来进行存储 我们的值都是存储到这个Map上的，key是当前ThreadLocal对象！ 如果该Map不存在，则初始化一个： void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 如果该Map存在，则从Thread中获取！ /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) { return t.threadLocals; } Thread维护了ThreadLocalMap变量 /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null 从上面又可以看出，ThreadLocalMap是在ThreadLocal中使用内部类来编写的，但对象的引用是在Thread中！ 于是我们可以总结出：Thread为每个线程维护了ThreadLocalMap这么一个Map，而ThreadLocalMap的key是LocalThread对象本身，value则是要存储的对象 有了上面的基础，我们看get()方法就一点都不难理解了 public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; } } return setInitialValue(); } 3.1 ThreadLocal 原理总结 每个Thread维护着一个ThreadLocalMap的引用 ThreadLocalMap 是ThreadLocal的内部类，用Entry来进行存储 调用ThreadLocal的set()方法时，实际上就是往ThreadLocalMap设置值，key是ThreadLocal对象，值是传递进来的对象 调用ThreadLocal的get()方法时，实际上就是往ThreadLocalMap获取值，key是ThreadLocal对象 ThreadLocal本身并不存储值，它只是作为一个key来让线程从ThreadLocalMap获取value。 4. 避免内存泄漏 我们来看一下ThreadLocal的对象关系引用图： ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 想要避免内存泄露就要手动remove()掉！ 5. 总结 ThreadLocal设计的目的就是为了能够在当前线程中有属于自己的变量，并不是为了解决并发或者共享变量的问题 参考文章 ThreadLocal就是这么简单 "},"base/concurrent/ThreadLocal/ThreadLocal使用不当导致内存泄漏.html":{"url":"base/concurrent/ThreadLocal/ThreadLocal使用不当导致内存泄漏.html","title":"ThreadLocal使用不当导致内存泄漏","keywords":"","body":"ThreadLocal使用不当导致内存泄漏参考文章ThreadLocal使用不当导致内存泄漏 线程池的一个线程使用完 Threadlocal 对象之后，由于线程池中的线程不会退出，线程池中的线程池存在，同时ThreadLocal变量也会存在，占用内存，造成OOM溢出。 参考文章 多图深入分析ThreadLocal原理 Java多线程编程-（9）-ThreadLocal造成OOM内存溢出案例演示与原理分析 "},"base/concurrent/乐观锁和悲观锁.html":{"url":"base/concurrent/乐观锁和悲观锁.html","title":"乐观锁和悲观锁","keywords":"","body":"乐观锁和悲观锁1. 乐观锁和悲观锁简介1.1 两种锁的使用场景2. 乐观锁的两种实现方式1.1 版本号机制1.2 CAS算法3. 乐观锁的缺点3.1 ABA 问题3.2 循环时间长开销大3.3 只能保证一个共享变量的原子操作4. CAS与synchronized的使用场景参考文章乐观锁和悲观锁 1. 乐观锁和悲观锁简介 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次再拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞知道它拿到锁（共享资源每次只给一个线程使用，其他线程阻塞，用完后再把资源转让给其他线程）传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下期间别人有没有更新这个数据，可以使用版本号机制和CAS算法实现，乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似write_condition机制，其实都是乐观锁，在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 1.1 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 2. 乐观锁的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 1.1 版本号机制 一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 1.1.1 版本号案例 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 1.2 CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 3. 乐观锁的缺点 ABA 问题是乐观锁一个常见的问题 3.1 ABA 问题 如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 \"ABA\"问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 3.2 循环时间长开销大 自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3.3 只能保证一个共享变量的原子操作 CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 4. CAS与synchronized的使用场景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 参考文章 面试必备之乐观锁与悲观锁 "},"base/concurrent/Callable和Future.html":{"url":"base/concurrent/Callable和Future.html","title":"Callable和Future","keywords":"","body":"Callable和FutureCallable和Future "},"base/concurrent/Atomic原子类.html":{"url":"base/concurrent/Atomic原子类.html","title":"Atomic原子类","keywords":"","body":"Atomic原子类1. 简介2. JUC包中的4类原子类3. AtomicInteger 的使用4. AtomicInteger 类的原理Atomic原子类 1. 简介 Atomic 翻译成中文的意思，在化学上，我们知道原子是构成一般物质的最小单位。在化学反应中是不可分割的。在我们这里Atomic是指一个操作是不可终端的，即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰 所以，所谓原子类说简单点就是具有原子/原子操作特征的类。 并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下,如下图所示。 2. JUC包中的4类原子类 基本类型 使用原子的方式更新基本类型 AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类 数组类型 使用原子的方式更新数组里的某个元素 AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray：引用类型数组原子类 引用类型 AtomicReference：引用类型原子类 AtomicStampedReference：原子更新引用类型里的字段原子类 AtomicMarkableReference ：原子更新带有标记位的引用类型 对象的属性修改类型 AtomicIntegerFieldUpdater：原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 3. AtomicInteger 的使用 AtomicInteger 类常用方法 public final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicInteger 类的使用示例 使用 AtomicInteger 之后，不用对 increment() 方法加锁也可以保证线程安全。 class AtomicIntegerTest { private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要对该方法加锁，也可以实现线程安全。 public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } 4. AtomicInteger 类的原理 AtomicInteger 类的部分源码： // setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。 CAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。 "},"base/concurrent/cas.html":{"url":"base/concurrent/cas.html","title":"CAS（比较并替换）","keywords":"","body":"CAS（比较并替换）1. 简介2. 问题3. 如何解决3.1 方案1：在add 方法加上synchrnized修饰3.2 方案2：CAS方案4. CAS缺点参考文章CAS（比较并替换） 1. 简介 CAS（compare and Swap），既比较并替换，实现并发算法时常用到的一种技术 在java同步器中大量使用了CAS技术，鬼斧神工的实现了多线程执行的安全性 CAS的思想很简单: 三个参数，一个当前内存值V、旧的预期值A、即将更新的值B、当且仅当预期值A和内存值V相同时，将内存值修改为B并返回true，否则什么都不做，并返回false 2. 问题 一个n++的问题 public class Case { public volatile int n; public void add() { n++; } } 通过javac Case.java 将java文件先编译成class 再通过javap -verbose Case看看add方法的字节码指令 public void add(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=1, args_size=1 0: aload_0 1: dup 2: getfield #2 // Field n:I 5: iconst_1 6: iadd 7: putfield #2 // Field n:I 10: return LineNumberTable: line 12: 0 line 13: 10 } SourceFile: \"Case.java\" n++ 被拆分成了几个指令 执行getfield拿到原始n； 执行iadd进行加1操作 执行putfield写把累加后的值写回n； 通过volatile修饰的变量可以保证线程之间的可见性，但并不能保证这3个指令的原子执行，在多线程并发执行下，无法做到线程安全，得到正确的结果，那么如何解决呢？ 3. 如何解决 3.1 方案1：在add 方法加上synchrnized修饰 public class Case { public volatile int n; public synchronized void add() { n++; } } 这个方案当然可行，但是性能上差了点 我们再来看一段代码 public int a = 1; public boolean compareAndSwapInt(int b) { if (a == 1) { a = b; return true; } return false; } 如果这段代码在并发下执行，会发生什么？ 假设线程1和线程2 都过了a==1的检查。都准备执行a进行赋值，结果就是两个线程同时修改了变量a。显然这种结果是无法符合预期的，无法确定a的最终值。 解决方案也同样暴力在compareAndSwapInt方法加锁同步，变成一个原子操作，同一时刻只有一个线程才能修改变量a。 3.2 方案2：CAS方案 除了地行政的加锁方案，我们还可以使用JDK自带的CAS方案，在CAS中，比较和替换是一组原子操作，不会被外部打断，且在性能上更占优势 下面是AtomicInteger的实现为例，分析一下CAS是如何实现的 public class AtomicInteger extends Number implements java.io.Serializable { // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; public final int get() {return value;} } Unsafe,是CAS的核心类，由于Java方法无法直接访问底层系统，需要通过本地（native）方法来访问，Unsafe相当于一个后门，基于该类可以直接操作特定内存数据 变量valueOffset，表示该变量值在内存中的偏移地址，因为Unsafe就是根据内存偏移地址获取数据的 变量value和volatile修饰，保证了多线程之间内存的可见性 看看AtomicInteger如何实现并发下的累加操作： public final int getAndAdd(int delta) { return unsafe.getAndAddInt(this, valueOffset, delta); } //unsafe.getAndAddInt public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 假设线程A和线程B同时执行getAndAdd操作（分别跑在不同的CPU上） AtomicInteger里面的value原始值为3，既主内存中AtomicInteger的value为3，根据java内存模型，线程A和线程B各自持有一份value的副本，值为3 线程A通过getIntVolatile(var1, var2)拿到value值3，这时线程A被挂起。 线程B也通过getIntVolatile(var1, var2)方法获取到value值3，运气好，线程B没有被挂起，并执行compareAndSwapInt方法比较内存值也为3，成功修改内存值为2。 这时线程A恢复，执行compareAndSwapInt方法比较，发现自己手里的值(3)和内存的值(2)不一致，说明该值已经被其它线程提前修改过了，那只能重新来一遍了。 重新获取value值，因为变量value被volatile修饰，所以其它线程对它的修改，线程A总是能够看到，线程A继续执行compareAndSwapInt进行比较替换，直到成功 整个过程中，利用CAS保证了对于value的修改的并发安全，继续深入看看Unsafe类中的compareAndSwapInt方法实现。 public final native boolean compareAndSwapInt(Object paramObject, lo Unsafe类中的compareAndSwapInt，是一个本地方法，该方法的实现位于unsafe.cpp中 UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper(\"Unsafe_CompareAndSwapInt\"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e; UNSAFE_END 先想办法拿到变量value在内存中的地址 通过Atomic::cmpxchg实现替换，其中参数x是即将更新的值，参数e是原内存的值 4. CAS缺点 CAS存在一个很明显的问题，既ABA问题 问题：如果变量V初次读取的时候是A，并且在准备赋值的时候检查到他的仍然是A，那能说明他的值没有被其他线程修改了吗？ 如果在这段期间层级被改成B,然后又改回A，那么CAS操作就会误认为他从来没有被修改过。针对这种情况，java并发包中提供了一个带有标记的原子引用类AtomicStampedReference,它可以通过控制变量值的版本来保证CAS的正确性 参考文章 深入浅出CAS "},"base/concurrent/AQS构建锁和同步器.html":{"url":"base/concurrent/AQS构建锁和同步器.html","title":"AQS构建锁和同步器","keywords":"","body":"AQS构建锁和同步器1. 简介2. AQS 原理2.1 AQS 原理概览2.2 AQS 对资源的共享方式参考文章AQS构建锁和同步器 1. 简介 AQS的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。 AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效得构造出应用广泛的大量同步器，比如我们提到了ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。 2. AQS 原理 2.1 AQS 原理概览 AQS 核心思想是 如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态 如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列实现的，即将暂时获取不到锁的线程加入到队列中 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列既不存在队列实例，仅存在结点之间的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个节点（Node）来实现锁的分配 看个AQS(AbstractQueuedSynchronizer)原理图： AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成资源线程的排队工作，AQS使用CAS对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过procted类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 2.2 AQS 对资源的共享方式 AQS定义两种资源共享方式 Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的 Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在上层已经帮我们实现好了。 …. 参考文章 并发编程面试必备：AQS 原理以及 AQS 同步组件总结 "},"base/io/IO总结.html":{"url":"base/io/IO总结.html","title":"IO总结","keywords":"","body":"IO总结1. 相关概念介绍1.1 同步与异步1.2 阻塞和非阻塞2. BIO(Blocking I/O)2.1 传统BIO2.2 伪异步IO2.3 BIO 总结3. NIO(New IO)3.1 NIO简介3.2 NIO的特性/NIO与IO区别3.3 NIO 读数据和写数据方式3.4 NIO 核心组件简单介绍3.5 代码示例3.6 为什么不愿意用 JDK 原生 NIO 进行开发？4. AIO (Asynchronous I/O)参考文章IO总结 1. 相关概念介绍 1.1 同步与异步 同步：同步就是发起一个调用后，被调用者未处理完请求之前，调用不返回 异步：异步就是发起一个调用后，立刻得到调用者的回应表示已接收到请求，但是被调用者并没有放回结果，此时我们可以处理其他请求，被调用者通常依靠事件，回调等机制来通知调用者返回结果 同步和异步的最大区别在于：调用者不需要等待处理结果，被调用者会通过回调等机制来通知调用者其返回结果 1.2 阻塞和非阻塞 阻塞：阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续 非阻塞：非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他时间 举个生活中简单的例子，你妈妈让你烧水，小时候你比较笨啊，在那里傻等着水开（同步阻塞）。等你稍微再长大一点，你知道每次烧水的空隙可以去干点其他事，然后只需要时不时来看看水开了没有（同步非阻塞）。后来，你们家用上了水开了会发出声音的壶，这样你就只需要听到响声后就知道水开了，在这期间你可以随便干自己的事情，你需要去倒水了（异步非阻塞）。 2. BIO(Blocking I/O) 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成 2.1 传统BIO BIO 通信（一请求一应答）模型如下 采用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，请求一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接，如上图所示。 如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程（主要原因是socket.accept()、socket.read()、socket.write() 涉及的三个主要函数都是同步阻塞的），也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，线程销毁。这就是典型的 一请求一应答通信模型 。我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制 改善，线程池还可以让线程的创建和回收成本相对较低。使用FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了N(客户端请求数量):M(处理客户端请求的线程数量)的伪异步I/O模型（N 可以远远大于 M），下面一节\"伪异步 BIO\"中会详细介绍到。 我们再设想一下当客户端并发访问量增加后这种模型会出现什么问题？ 在 Java 虚拟机中，线程是宝贵的资源，线程的创建和销毁成本很高，除此之外，线程的切换成本也是很高的。尤其在 Linux 这样的操作系统中，线程本质上就是一个进程，创建和销毁线程都是重量级的系统函数。如果并发访问量增加会导致线程数急剧膨胀可能会导致线程堆栈溢出、创建新线程失败等问题，最终导致进程宕机或者僵死，不能对外提供服务。 2.2 伪异步IO 为了解决同步阻塞I/O 面临的一个链路需要一个线程处理的问题，后来有人对他的线程模型进行了优化——后端通过一个线程池来处理多个客户端请求接入，形成客户端个数,线程池最大线程N的比例关系，其中M 可以远远大于N，通过线程池可以灵活的调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽 2.3 BIO 总结 当活动连接数小于1000： 可以让每一个连接专注自己的I/O 并且模型简单，也不用过多考虑系统过载，限流等问题， 线程池本身就是一个天然漏斗，可以缓冲一些系统处理不了的连接或请求 面对十万甚至百万连接的时候： 传统BIO模型是无能为力的 3. NIO(New IO) 3.1 NIO简介 NIO 是一种同步非阻塞的I/O模型，在Java1.4 中引入了NIO 框架，对应java.nio 包，提供了Channel，Selector，Buffer 等抽象 NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。 3.2 NIO的特性/NIO与IO区别 面试思路: 首先从NIO流是非阻塞IO,而IO流是阻塞式IO说起 然后说NIO的3个核心组件/特性为NIO 带来一些改进来分析 Non-blocking IO (非阻塞IO) IO流是阻塞的，NIO流是不阻塞的。 Java NIO使我们可以进行非阻塞IO操作。比如说，单线程中从通道读取数据到buffer，同时可以继续做别的事情，当数据读取到buffer中后，线程再继续处理数据。写数据也是一样的。另外，非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 Java IO的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了 Buffer (缓冲区) IO 面向流（Stream oriented）,而NIO 面向缓冲区（Buffer oriented） Buffer是一个对象，它包含一些要写入或者要读出的数据。在NIO类库中加入Buffer对象，体现了新库与原I/O的一个重要区别。在面向流的I/O中·可以将数据直接写入或者将数据直接读到 Stream 对象中。虽然 Stream 中也有 Buffer 开头的扩展类，但只是流的包装类，还是从流读到缓冲区，而 NIO 却是直接读到 Buffer 中进行操作。 在NIO厍中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。 最常用的缓冲区是 ByteBuffer,一个 ByteBuffer 提供了一组功能用于操作 byte 数组。除了ByteBuffer,还有其他的一些缓冲区，事实上，每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。 Channel(通道) NIO 通过Channel （通道）进行读写 通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。 Selector （选择器） NIO 有选择器，而IO 没有 选择器用于使用单个线程处理多个通道，因此，他需要较少的线程来处理这些通过，线程之间的切换对于操作系统来说是昂贵的。因此，为了提高系统效率选择器是有用的 3.3 NIO 读数据和写数据方式 通常来说NIO 中所有IO 都是从 Channel（通道）开始的 从通道进行数据读取：创建一个缓冲区，然后请求通道读取数据 从通道进行数据写入：创建一个缓冲区，填充数据，并要求通道写入数据 数据读取和写入操作图示： 3.4 NIO 核心组件简单介绍 Channel（通道） Buffer（缓冲区） Selector (选择器) 3.5 代码示例 客户端 IOClient.java 的代码不变，我们对服务端使用 NIO 进行改造。以下代码较多而且逻辑比较复杂，大家看看就好。 /** * * @author 闪电侠 * @date 2019年2月21日 * @Description: NIO 改造后的服务端 */ public class NIOServer { public static void main(String[] args) throws IOException { // 1. serverSelector负责轮询是否有新的连接，服务端监测到新的连接之后，不再创建一个新的线程， // 而是直接将新连接绑定到clientSelector上，这样就不用 IO 模型中 1w 个 while 循环在死等 Selector serverSelector = Selector.open(); // 2. clientSelector负责轮询连接是否有数据可读 Selector clientSelector = Selector.open(); new Thread(() -> { try { // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(3333)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) { // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) > 0) { Set set = serverSelector.selectedKeys(); Iterator keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { try { // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); } finally { keyIterator.remove(); } } } } } } catch (IOException ignored) { } }).start(); new Thread(() -> { try { while (true) { // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) > 0) { Set set = clientSelector.selectedKeys(); Iterator keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isReadable()) { try { SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println( Charset.defaultCharset().newDecoder().decode(byteBuffer).toString()); } finally { keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); } } } } } } catch (IOException ignored) { } }).start(); } } 3.6 为什么不愿意用 JDK 原生 NIO 进行开发？ 从上面代码可以看出，除了编程复杂、编程模型难之外 JDK 的 NIO 底层由 epoll 实现，该实现饱受诟病的空轮询 bug 会导致cpu飙升100% 项目庞大之后，自行实现的NIO 很容器出现各类bug，维护成本较高 Netty 的出现很大程度上改善了 JDK 原生 NIO 所存在的一些让人难以忍受的问题。 4. AIO (Asynchronous I/O) AIO 也就是NIO2。在java7 中引入了NIO的改进版NIO2。他是异常非阻塞的IO模型。异步IO 是基于事件和回调机制实现的，也就是应用操作之间会直接返回。不会阻塞在哪里，当后台处理完成，操作系统会通知响应的线程进行后续操作 参考文章 真正理解NIO "},"base/io/":{"url":"base/io/","title":"IO流入门","keywords":"","body":"IO流什么是IO什么是IO流流的分类具体IO流实现类IO流 什么是IO i: 输入 从本地写入程序叫输入 o: 输出 从程序写出到本地叫输出 都是相对我们程序的 什么是IO流 IO流的用来处理设备之间数据传输的，传输电的叫电流，传输水的叫水流，传输数据的就叫数据流 流的分类 按操作数据单位不同 字节流（8bit） 字符流（16bit） 按数据流的流向不同 输入流 输出流 按流的角色的不同分为 节点流 处理流 抽象基类 字节流 字符流 输入流 InputStream Reader 输出流 OutputStream Writer 具体IO流实现类 java的IO流共设计40多个类，都是从以上4个类派生出来的 分类 字节输入流 字节输出流 字符输入流 字符输出流 抽象基类 InputStream OutputStream Reader Writer 访问文件 FileInputStream FileOutputStream FileReader FileWriter 访问数组 ByteArrayInputStream ByteArrayOutputStream CharArrayReader CharArrayWriter 访问管道 PipedInputStream PipedOutputStream PipedReader PipedWriter 访问字符串 StringReader StringWriter 缓冲流 BufferedInputStream BufferedOutputStream BufferedReader BufferedWriter 转换流 InputStreamReader OutputStreamWriter 对象流 ObjectInputStream ObjectOutputStream FilterInputStream FilterOutputStream FilterReader FilterWriter 打印流 PrintStream PrintWriter 推回输入流 PushbackInputStream PushbackReader 特殊流 DataInputStream DataOutputStream "},"base/io/File类.html":{"url":"base/io/File类.html","title":"File类","keywords":"","body":"File类mkdir() 和mkdirs() 区别File类 file对应一个物理文件 Java中新建或者删除一个文件,文件夹以及createNewFile(),delete(),mkdir(),mkdirs()函数 判断文件的函数：exists(),isFile(),isAbsolute(),isDirectory(),canRead(),canWrite(),isHidden()函数 文件属性的函数：lastModified(),length(),list(),listFiles(),renameTo()，getName(),getParent(),getPath(),getAbsolutePath()，delete()函数 mkdir() 和mkdirs() 区别 java.io.File.mkdir()：只能创建一级目录，且父目录必须存在，否则无法成功创建一个目录。 java.io.File.mkdirs()：可以创建多级目录，父目录不一定存在。 "},"base/io/file/":{"url":"base/io/file/","title":"文件流","keywords":"","body":"文件流文件流 "},"base/io/file/FileInputStream.html":{"url":"base/io/file/FileInputStream.html","title":"FileInputStream","keywords":"","body":"FileInputStream文件读取实例最基本的文件读取赋值语句位置更改使用try-catch处理异常*使用read重载方法read(byte[] b)FileInputStream FileInputStream 是将本地文件读到程序中来 使用read读，默认的构造器是一个字节一个字节读，然后-1的时候就读到文件的结尾 文件读取实例 最基本的文件读取 @Test public void test() throws Exception { // 1.创建一个File类的对象 File file = new File(\"hello.txt\"); // 2.创建一个FileInputStream类对象 FileInputStream fis = new FileInputStream(file); // 3.调用FileInputStream 的方法，实现file文件的读取 int b = fis.read(); while (b!= -1){ System.out.println(b); b = fis.read(); } // 4.关闭相应的流 fis.close(); } 赋值语句位置更改 我们可以看到上述代码，fis.read()出现了两次，可以进行一个小的改进 // 3.调用FileInputStream 的方法，实现file文件的读取 // int b = fis.read(); // while (b!= -1){ // System.out.println(b); // b = fis.read(); // } int b; while ((b =fis.read())!=-1){ System.out.println((char)b); } 将int型的强转char，英文没问题，但是中文就会出现乱码 使用try-catch处理异常 出了异常也要及时关闭资源 @Test public void test2() { FileInputStream fis = null; // 1.创建一个File类的对象 try { File file = new File(\"hello.txt\"); // 2.创建一个FileInputStream类对象 fis = new FileInputStream(file); // 3.调用FileInputStream 的方法，实现file文件的读取 int b; while ((b =fis.read())!=-1){ System.out.println((char)b); } } catch (IOException e) { e.printStackTrace(); }finally { // 4.关闭相应的流 try { fis.close(); } catch (IOException e) { e.printStackTrace(); } } } *使用read重载方法read(byte[] b) 这样就可以不要一个一个读取 @Test public void test3() { FileInputStream fis = null; // 1.创建一个File类的对象 try { File file = new File(\"hello.txt\"); // 2.创建一个FileInputStream类对象 fis = new FileInputStream(file); // 3.调用FileInputStream 的方法，实现file文件的读取 byte[] b = new byte[8]; int len; while ((len =fis.read(b))!=-1){ String str = new String(b,0,len); System.out.print(str); } } catch (IOException e) { e.printStackTrace(); }finally { // 4.关闭相应的流 try { fis.close(); } catch (IOException e) { e.printStackTrace(); } } } "},"base/io/file/FileOutputStream.html":{"url":"base/io/file/FileOutputStream.html","title":"FileOutputStream","keywords":"","body":"FileOutputStream复制文件FileOutputStream @Test public void test(){ // 1.创建一个File对象，表明要写入的文件位置 File file = new File(\"hello2.txt\"); // 2.创建一个FileOutputStream的对象，将file的对象作为形参传递给FileOutputStream的构造器中 FileOutputStream fos =null; try { fos = new FileOutputStream(file); // 3.写入的操作 fos.write(new String(\"my test demo\").getBytes()); }catch (Exception e){ e.printStackTrace(); }finally { //4.关闭输出流 if (fos !=null){ try { fos.close(); }catch (IOException e){ e.printStackTrace(); } } } } 复制文件 通过FileInputStream 和FileOutputStream 实现复制 // 实现文件复制的方法 public void copyFile(String src,String dest){ // 1.提供读入，写出的文件 File file1 = new File(src); File file2 = new File(dest); // 2.提供相应的流 FileInputStream fis = null; FileOutputStream fos = null; try { fis = new FileInputStream(file1); fos = new FileOutputStream(file2); //3.实现文件的复制 byte[] b = new byte[1024]; int len; while ((len = fis.read(b))!=-1){ fos.write(b,0,len); } }catch (Exception e){ e.printStackTrace(); }finally { try { fis.close(); fos.close(); }catch (Exception e){ e.printStackTrace(); } } } "},"base/io/file/FileReader.html":{"url":"base/io/file/FileReader.html","title":"FileReader","keywords":"","body":"FileReaderFileReader 的基本使用复制文本文件FileReader 使用FileReader,FileWriter 可以实现文本的复制等 但是对于非文本（视频文件，音频文件，图片）只能使用字节流 FileReader 的基本使用 File file = new File(\"hello.txt\"); FileReader fr = new FileReader(file); char[] c = new char[24]; int len; while ((len = fr.read(c)) != -1){ String str = new String(c,0,len); System.out.print(str); } fr.close(); 复制文本文件 // 1.输入流对应的文件src 一定要存在，否则抛异常 // 输出流对应的文件dest 可以不存在，执行过程由程序创建 FileReader fr = null; FileWriter fw = null; try { File src = new File(\"hello.txt\"); File dest = new File(\"cphello.txt\"); fr = new FileReader(src); fw = new FileWriter(dest); char[] c = new char[24]; int len; while ((len = fr.read(c))!= -1){ fw.write(c,0,len); } }catch (Exception e){ e.printStackTrace(); }finally { if (fw !=null){ fw.close(); }if (fr != null){ fr.close(); } } "},"base/io/file/FileWriter.html":{"url":"base/io/file/FileWriter.html","title":"FileWriter","keywords":"","body":"FileWriterFileWriter "},"base/io/buffered/":{"url":"base/io/buffered/","title":"缓冲流","keywords":"","body":"缓冲流基本架构为什么缓冲流快一些使用缓冲流实现非文本复制缓冲流 基本架构 抽象基类 节点流(文件流) 缓冲流（处理流的一种） InputStream FileInputStream BufferedInputStream OutputStream FileOutputStream BufferedOutputStream（flush()） Reader FileReader BufferedReader Writer FileWriter BufferedWriter（flush()） 为什么缓冲流快一些 底层数组实现 read方法是非阻塞式的 FileInputStream是阻塞式的 BufferInputStream 不是阻塞式的 使用缓冲流实现非文本复制 同样复制一个200M 的文件，不带缓冲需要3000ms，带缓冲500ms 实际开发中使用缓冲流较多，因为效率高 // 1. 提供读入，写入的文件 File file = new File(\"1.png\"); File file2 = new File(\"2.png\"); // 2.创建相应的节点流，FileInputStream、FileOutputStream FileInputStream fis = new FileInputStream(file); FileOutputStream fos = new FileOutputStream(file2); // 3.将创建的节点流的对象作为形参传递给缓冲流的构造器中 BufferedInputStream bis = new BufferedInputStream(fis); BufferedOutputStream bos = new BufferedOutputStream(fos); // 4.具体的实现文件复制操作 byte[] b = new byte[1024]; int len; while ((len = bis.read(b)) != -1){ bos.write(b,0,len); bos.flush(); } bos.close(); bis.close(); 优化1：BufferOutput写完最好刷新一下 前面没问题，最后一次可能存不满，要刷新一下给他写出去 也只有缓冲流才能加flush bos.flush(); 优化2：针对BufferRead 可以一次读取一行 读取一行的时候，最后判断不能使用-1，要使用null 读取一整行，所以他不会自动换行，需要手动换行 while((str = br.readLine())!=null){ // 换行方式1： bw.write(str +\"\\n\"); //换行方式2： bw.newLine(); bw.flush(); } "},"base/io/IO转换流/":{"url":"base/io/IO转换流/","title":"IO转换流","keywords":"","body":"IO转换流操作流程复制文件实例IO转换流 转换流提供了字节流和字符流之间的转换， 当字节流中的数据都是字符时，转成字符流的操作效率更高 操作流程 复制文件实例 编码：字符串 —>字节数组 解码：字节数组—>字符串 // 解码 File file = new File(\"hello.txt\"); FileInputStream fis = new FileInputStream(file); InputStreamReader isr = new InputStreamReader(fis,\"UTF-8\"); BufferedReader br = new BufferedReader(isr); //编码 File file2 = new File(\"cphello2.txt\"); FileOutputStream fos = new FileOutputStream(file2); OutputStreamWriter osw = new OutputStreamWriter(fos,\"UTF-8\"); BufferedWriter bw = new BufferedWriter(osw); // char[] c =new char[1024]; String str; while ((str = br.readLine())!=null){ bw.write(str); bw.newLine(); bw.flush(); } "},"base/io/对象流/":{"url":"base/io/对象流/","title":"对象流","keywords":"","body":"对象流序列化与反序列化：某些字段不想序列话对象流 用于存储和读取对象的处理流 可以吧Java中的对象写入到数据源汇总 也能吧对象从数据源中还原 序列化与反序列化： 序列化（Serialize）：用objectOutputStream 类将一个Java对象写入IO流汇总 反序列化（Deserialize）:用ObjectInputStream类从IO流中恢复Java对象 不能序列化static和transient修饰的成员变量 某些字段不想序列话 使用transient 关键字修饰 transient关键字的作用：阻止实例中那些用此关键字修饰的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transient只能修饰变量，不能修饰类和方法 "},"base/jvm/Java内存区域.html":{"url":"base/jvm/Java内存区域.html","title":"Java内存区域","keywords":"","body":"Java内存区域1. 常见面试题1.1 基本问题1.2 拓展问题2 概述3. 运行时数据区域3.1 程序计数器3.2 Java 虚拟机栈3.3 本地方法栈3.4 堆3.5 方法区3.6 运行时常量池3.7 直接内存Java内存区域 1. 常见面试题 1.1 基本问题 介绍下Java内存区域（运行时数据区） Java对象的创建过程（五步，必须默写出来并且知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 1.2 拓展问题 Stringt 类和常量池 8种基本类型的包装类和常量池 2 概述 对于java程序员来说，在虚拟机自动内存管理机制下，不再需要像C/C++程序开发程序员这样为每一个new 操作去屑对应的delete/free操作，不容易出现内存泄漏和内存溢出问题。正式因为Java程序员把内存控制权交给Java虚拟机，一旦出现内存泄漏和溢出方面的问题。如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务 3. 运行时数据区域 Java虚拟机在执行 java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK 1.8和之前的版本略有不同，下面会介绍 JDK 1.8 之前 在JDK 1.8 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的 堆 方法区 直接内存（非运行时数据区的一部分） 3.1 程序计数器 依次读取指令，从而实现代码的流程控制 在多线程的情况下，程序计数器用于记录当前线程执行的位置 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 3.2 Java 虚拟机栈 与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 异常。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 3.2.1 那么方法/函数如何调用 java 栈可以类比数据结构中的栈，java 栈中保存的内容主要是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈。每一个函数调用结束后，都会有一个栈帧被弹出 3.2.2 Java 方法的两种返回 return 语句 抛出异常 不管哪种返回方式都会导致栈帧被弹出 3.3 本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。 3.4 堆 Java 虚拟机中所管理的内存中最大的一块，java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存 java堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好的回收内存，更快的分配内存 上图所示的eden 区，s0 区，s1区都是属于新生代，tentired区属于老年代，大部分情况 对象都会首先在Eden 区域分配 在一次新生代垃圾回收后，如果对象还存活，则会进入s0或者s1,并且对象的年龄还会加1（Eden区->Survivor 区后对象的初始年龄变为1） 当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代 对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 3.5 方法区 方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 3.5.1 方法区和永久代的关系 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 3.5.2 常用参数 JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 -XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 3.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 整个永久代有一个 JVM 本身设置固定大小上线，无法调整，而元空间使用直接内存，受本机可用内存的限制，并且永远不会得到 java.lang.OutOfMemoryError。你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 3.6 运行时常量池 运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 3.7 直接内存 直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 "},"base/jvm/HotSpot虚拟机对象创建.html":{"url":"base/jvm/HotSpot虚拟机对象创建.html","title":"HotSpot 虚拟机对象创建","keywords":"","body":"HotSpot 虚拟机对象创建1. 对象的创建1.1 Step1: 类加载检查1.2 Step2：分配内存1.3 Step3:初始化零值1.4 Step4:设置对象头1.5 Step5:执行 init 方法2. 对象的内存布局3. 对象的访问定位HotSpot 虚拟机对象创建 本章介绍HotSpot 虚拟机在Java堆中对象分配、布局和访问的全过程 1. 对象的创建 下图便是 Java 对象的创建过程（需默写，并掌握每一步） 1.1 Step1: 类加载检查 虚拟机遇到一条new指令时，首先将去检查这个指定的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表类是否已被加载过，解析和初始化过。如果没有，那必须先执行相应的类加载过程 1.2 Step2：分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 1.2.1 分配内存的两种方式（需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是\"标记-清除\"，还是\"标记-整理\"（也称作\"标记-压缩\"），值得注意的是，复制算法内存也是规整的 1.2.2 内存分配并发问题（需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 1.3 Step3:初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 1.4 Step4:设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 1.5 Step5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始， 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 2. 对象的内存布局 在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 对象头 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的自身运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3. 对象的访问定位 建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 2.直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 "},"base/jvm/JVM垃圾回收.html":{"url":"base/jvm/JVM垃圾回收.html","title":"JVM垃圾回收","keywords":"","body":"JVM垃圾回收1. 垃圾回收常见面试题2. 导火索JVM垃圾回收 1. 垃圾回收常见面试题 如何判断对象是否死亡（两种方法） 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处） 如何判断一个常量是废弃常量 如何判断一个类是无用类 垃圾收集有哪些算法，各自的特点 HotSpot 为什么要分为新生代和老年代 常见的垃圾回收器有哪些 介绍一下CMS,G1收集器 Minor GC 和 Full GC 有什么区别 2. 导火索 当需要排查各种内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些“自动化”的技术实施必要的监控和调节。 "},"base/jvm/JVM内存分配与回收.html":{"url":"base/jvm/JVM内存分配与回收.html","title":" JVM 内存分配与回收","keywords":"","body":"JVM 内存分配与回收1. JVM 内存分配与回收1.1 对象优先在eden 区分配1.2 大对象直接进入老年代1.3 长期存活的对象将进入老年代1.4 动态对象年龄判定JVM 内存分配与回收 1. JVM 内存分配与回收 Java 的自动内存管理主要是针对象内存的回收和对象的内存的分配。同时，java 自动内存管理最核心的功能是 堆内存中的对象分配与回收 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 堆空间的基本结构： 上图所示的 eden区，s0(\"From\") 区、s1(\"To\") 区都属于新生代，tentired 区属于老年代。大部分情况， 对象都会首先在 Eden 区域分配 在一次新生代垃圾回收后，如果对象还存活，则会进入 s1(\"To\")，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1) 当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中 对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置 经过这次GC后，Eden区和\"From\"区已经被清空。这个时候，\"From\"和\"To\"会交换他们的角色，也就是新的\"To\"就是上次GC前的“From”，新的\"From\"就是上次GC前的\"To\"。不管怎样，都会保证名为To的Survivor区域是空的 Minor GC会一直重复这样的过程，直到“To”区被填满，\"To\"区被填满之后，会将所有对象移动到年老代中。 1.1 对象优先在eden 区分配 目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。 在测试之前我们先来看看 Minor GC 和 Full GC 有什么不同呢？ 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 测试 public class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2; allocation1 = new byte[30900*1024]; //allocation2 = new byte[900*1024]; } } 添加的参数：-XX:+PrintGCDetails 运行结果（JDK 1.8） 从上图我们可以看出 eden 区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用 2000 多 k 内存）。假如我们再为 allocation2 分配内存会出现什么情况呢？ allocation2 = new byte[900*1024]; 简单解释一下为什么会出现这种情况： 因为给 allocation2 分配内存的时候 eden 区内存几乎已经被分配完了，我们刚刚讲了当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。可以执行如下代码验证： public class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2,allocation3,allocation4,allocation5; allocation1 = new byte[32000*1024]; allocation2 = new byte[1000*1024]; allocation3 = new byte[1000*1024]; allocation4 = new byte[1000*1024]; allocation5 = new byte[1000*1024]; } } 1.2 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么需要这样呢？ 为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率 1.3 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并将对象年龄设为1。 对象在Survivor中没经过一次MinorGC 年龄就增加1岁 当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中 对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 1.4 动态对象年龄判定 为了更好的适应不同程序的内存情况，虚拟机不是永远要求对象年龄必须达到了某个值才能进入老年代，如果 Survivor 空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需达到要求的年龄。 "},"base/jvm/对象已经死亡.html":{"url":"base/jvm/对象已经死亡.html","title":"对象已经死亡？","keywords":"","body":"对象已经死亡？1. 如何判断对象已经死亡1.1 引用计数法1.2 可达性分析算法2 再谈引用2.1．强引用2.2 软引用（SoftReference）2.3 弱引用（WeakReference）2.4 虚引用（PhantomReference）3. 不可达的对象并非“非死不可”4. 如何判断一个常量是废弃常量5. 如何判断一个类是无用类对象已经死亡？ 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是判断哪些对象已经死亡（即不能再被任何途径使用的对象）。 1. 如何判断对象已经死亡 1.1 引用计数法 给对象中添加一个引用计数器，每当有一个地方引用他。计数器就+1，当引用失效，计数器就减1.任何时候计数器为0 的对象就是不可能再被使用的 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。 public class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; } } 1.2 可达性分析算法 这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 2 再谈引用 无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。 JDK1.2 之前，Java 中引用的定义很传统：如果 reference 类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。 JDK1.2 以后，Java 对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱） 2.1．强引用 以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2.2 软引用（SoftReference） 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。 2.3 弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 2.4 虚引用（PhantomReference） \"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 3. 不可达的对象并非“非死不可” 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 4. 如何判断一个常量是废弃常量 运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 \"abc\"，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池。 5. 如何判断一个类是无用类 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 "},"base/jvm/垃圾收集算法.html":{"url":"base/jvm/垃圾收集算法.html","title":"垃圾收集算法","keywords":"","body":"垃圾收集器垃圾收集算法1. 标记-清除算法2. 复制算法3. 标记-整理算法4. 分代收集算法垃圾收集器垃圾收集算法 1. 标记-清除算法 该算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 2. 复制算法 为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 3. 标记-整理算法 根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 4. 分代收集算法 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 延伸面试问题： HotSpot 为什么要分为新生代和老年代？ "},"base/jvm/垃圾收集器.html":{"url":"base/jvm/垃圾收集器.html","title":"垃圾收集器","keywords":"","body":"垃圾收集器4.1 Serial 收集器4.2 ParNew 收集器4.3 Parallel Scavenge 收集器4.4.Serial Old 收集器4.5 Parallel Old 收集器4.6 CMS 收集器4.7 G1 收集器垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial 收集器 Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \"Stop The World\" ），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。 4.2 ParNew 收集器 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 4.3 Parallel Scavenge 收集器 Parallel Scavenge 收集器也是使用复制算法的多线程收集器，它看上去几乎和ParNew都一样。 那么它有什么特别之处呢？ -XX:+UseParallelGC 使用 Parallel 收集器+ 老年代串行 -XX:+UseParallelOldGC 使用 Parallel 收集器+ 老年代并行 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在困难的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 4.4.Serial Old 收集器 Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。 4.5 Parallel Old 收集器 Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。 4.6 CMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对为标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 4.7 G1 收集器 G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记--清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 GF 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 "},"base/jvm/GC中对象自救.html":{"url":"base/jvm/GC中对象自救.html","title":"GC中对象自救","keywords":"","body":"GC中对象自救为什么第二次自救失败？Finalizer 线程GC中对象自救 即使在可达性分析算法中, 被判定为不可达的对象, 也并非是'非死不可' 的, 这时候他们暂处于'缓刑' 阶段, 要真正宣告一个对象死亡, 至少要经历两次标记过程: 如果对象失去引用(在进行可达性分析后发现没有与 GC Roots 相连接的引用链), 并且该对象的 finalize()方法未调用过, 该对象将会被第一次标记 如果这个对象已经被标记了, 那么这个对象会被放入 ReferenceQueue 队列中, 由 FinalizeThread 线程去执行, 最终会调用该对象的 finalize() 方法. 这里所谓的'执行' 并不会保证 finalize() 方法调用结束,为如果 finalize() 方法执行缓慢, 极端情况下可能会导致 ReferenceQueue 队列中其他的对象永远处于等待, 甚至导致 GC 系统崩溃. finalize() 方法是对象逃脱死亡命运的最后一次机会, 调用 finalize() 方法后,GC 系统将对 ReferenceQueue 队列中的对象进行第二次标记, 如果对象要在 finalize() 方法中成功自救, 只要重新与引用链建立引用即可, 如:把当前对象( this )赋值给某个类变量或者对象的成员变量, 那么在第二次标记时它将被移除出 '即将回收' 的集合, 这样该对象就完成了一次自救; 如果该对象被第二次标记, 那就真的要被回收了. public class FinalizeEscapeGC { // 用于自救的类变量 private static FinalizeEscapeGC SAVA_HOOK; @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"当前对象: \" + this + \" 在 \" + Thread.currentThread() + \" 线程执行 finalize() 方法\"); // 把当前对象( this )赋值给某个类变量, 重新与引用链建立引用 FinalizeEscapeGC.SAVA_HOOK = this; } public static void main(String[] args) throws Throwable { // 创建一个对象 FinalizeEscapeGC@3654919e, SAVA_HOOK 引用该对象 SAVA_HOOK = new FinalizeEscapeGC(); System.out.println(\"第一次自救\"); SAVA_HOOK = null; // 失去引用 System.gc(); // 运行垃圾回收器 Thread.sleep(100); // 让 GC 相关线程先走 if (SAVA_HOOK != null) { System.out.println(SAVA_HOOK + \" 对象自救成功\"); } else { System.out.println(\"对象已被回收\"); } System.out.println(\"\\n第二次自救\"); SAVA_HOOK = null; // 失去引用 System.gc(); // 运行垃圾回收器 Thread.sleep(100); // 让 GC 相关线程先走 if (SAVA_HOOK != null) { System.out.println(SAVA_HOOK + \" 对象自救成功\"); } else { System.out.println(\"对象已被回收\"); } } } 运行结果 运行结果: 第一次自救 当前对象: FinalizeEscapeGC@3654919e 在 Thread[Finalizer,8,system] 线程执行 finalize() 方法 FinalizeEscapeGC@3654919e 对象自救成功 第二次自救 对象已被回收 为什么第二次自救失败？ 另外一个值得注意的地方是, 代码中两次试图实现自救, 运行结果却是: 一次自救成功, 一次失败 这是因为任何对象的 finalize() 方法都只会被调用一次, 如果对象面临下一次 GC, 它的 finalize() 方法不会被再次执行, 因此第二次自救失败了. Finalizer 线程 Finalizer 线程, 其优先级为 8, 用于在 GC 前, 执行对象的 finalize() 方法 关于 Finalizer 线程: JVM 在 GC 时会将失去引用的对象包装成 java.lang.ref.Finalizer 对象（java.lang.ref.Reference 抽象类的实现），并放入ReferenceQueue，由 Finalizer$FinalizeThread 线程来处理 "},"base/jvm/如何合理的规划 JVM 性能调优.html":{"url":"base/jvm/如何合理的规划 JVM 性能调优.html","title":"如何合理的规划 JVM 性能调优","keywords":"","body":"如何合理的规划 JVM 性能调优1. 性能调优的层次1.1 确保项目架构调优与代码调优是最优的1.2 明确性能优化目标，找到性能瓶颈2. jvm调优流程2.1 性能定义2.2 JVM调优原则2.3 性能调优步骤3. 确定内存占用3.1 运行阶段3.2 jvm内存分配和参数2.3 计算活跃数据大小4.延迟调优4.1 系统延迟需求4.2 优化新生代大小4.3 优化老年代大小对象提升率5. 吞吐量调优6. 最后参考文章如何合理的规划 JVM 性能调优 JVM性能调优涉及到方方面面的取舍，往往是牵一发而动全身，需要全盘考虑各方面的影响。但也有一些基础的理论和原则，理解这些理论并遵循这些原则会让你的性能调优任务将会更加轻松。为了更好的理解本篇所介绍的内容。你需要已经了解和遵循以下内容: 1、已了解jvm 垃圾收集器 2、已了解jvm 性能监控常用工具 3、能够读懂gc日志 4、确信不为了调优而调优，jvm调优不能解决一切性能问题 如果对这些不了解不建议读本篇文章。 本篇文章基于jvm性能调优，结合jvm的各项参数对应用程序调优，主要内容有以下几个方面： 1、jvm调优的一般流程 2、jvm调优所要关注的几个性能指标 3、jvm调优需要掌握的一些原则 4、调优策略&示例 1. 性能调优的层次 为了提升系统性能，我们需要对系统的各个角度和层次来进行优化，以下是需要优化的几个层次。 从上面我们可以看到，除了jvm调优以外，还有其他几个层面需要来处理，所以针对系统的调优不是只有jvm调优一项，而是需要针对系统来整体调优，才能提升系统的性能。本篇只针对jvm调优来讲解，其他几个方面，后续再介绍。 1.1 确保项目架构调优与代码调优是最优的 在进行jvm调优之前，我们假设项目的架构调优和代码调优已经进行过或者是针对当前项目是最优的。这两个是jvm调优的基础，并且架构调优是对系统影响最大的 ，我们不能指望一个系统架构有缺陷或者代码层次优化没有穷尽的应用，通过jvm调优令其达到一个质的飞跃，这是不可能的。 1.2 明确性能优化目标，找到性能瓶颈 另外，在调优之前，必须得有明确的性能优化目标， 然后找到其性能瓶颈。之后针对瓶颈的优化，还需要对应用进行压力和基准测试，通过各种监控和统计工具，确认调优后的应用是否已经达到相关目标。 2. jvm调优流程 调优的最终目的都是为了令应用程序使用最小的硬件消耗来承载更大的吞吐。jvm的调优也不例外，jvm调优主要是针对垃圾收集器的收集性能优化，令运行在虚拟机上的应用能够使用更少的内存以及延迟获取更大的吞吐量。当然这里的最少是最优的选择，而不是越少越好。 2.1 性能定义 要查找和评估器性能瓶颈，首先要知道性能定义，对于jvm调优来说，我们需要知道以下三个定义属性，依作为评估基础: 吞吐量：重要指标之一，是指不考虑垃圾收集引起的停顿时间或内存消耗，垃圾收集器能支撑应用达到的最高性能指标。 延迟：其度量标准是缩短由于垃圾啊收集引起的停顿时间或者完全消除因垃圾收集所引起的停顿，避免应用运行时发生抖动。 内存占用：垃圾收集器流畅运行所需要 的内存数量。 这三个属性中，其中一个任何一个属性性能的提高，几乎都是以另外一个或者两个属性性能的损失作代价，不可兼得，具体某一个属性或者两个属性的性能对应用来说比较重要，要基于应用的业务需求来确定。 2.2 JVM调优原则 在调优过程中，我们应该谨记以下3个原则，以便帮助我们更轻松的完成垃圾收集的调优，从而达到应用程序的性能要求。 MinorGC回收原则： 每次minor GC 都要尽可能多的收集垃圾对象。以减少应用程序发生Full GC的频率。 GC内存最大化原则：处理吞吐量和延迟问题时候，垃圾处理器能使用的内存越大，垃圾收集的效果越好，应用程序也会越来越流畅。 GC调优3选2原则: 在性能属性里面，吞吐量、延迟、内存占用，我们只能选择其中两个进行调优，不可三者兼得。 2.3 性能调优步骤 以上就是对应用程序进行jvm调优的基本流程，我们可以看到，jvm调优是根据性能测试结果不断优化配置而多次迭代的过程。在达到每一个系统需求指标之前，之前的每个步骤都有可能经历多次迭代。有时候为了达到某一方面的指标，有可能需要对之前的参数进行多次调整，进而需要把之前的所有步骤重新测试一遍。 另外调优一般是 从满足程序的内存使用需求开始的， 之后是时间延迟的要求， 最后才是吞吐量的要求， 要基于这个步骤来不断优化，每一个步骤都是进行下一步的基础，不可逆行之。以下我们针对每个步骤进行详细的示例讲解。 在JVM的运行模式方面，我们直接选择server模式，这也是jdk1.6以后官方推荐的模式。 在垃圾收集器方面，我们直接采用了jdk1.6-1.8 中默认的parallel收集器（新生代采用parallelGC,老生代采用parallelOldGC）。 3. 确定内存占用 在确定内存占用之前，我们需要知道两个知识点： 应用程序的运行阶段 jvm内存分配 3.1 运行阶段 应用程序的运行阶段，我可以划分为以下三个阶段: 1、初始化阶段 : jvm加载应用程序，初始化应用程序的主要模块和数据。 2、稳定阶段:应用在此时运行了大多数时间，经历过压力测试的之后，各项性能参数呈稳定状态。核心函数被执行，已经被jit编译预热过。 3、总结阶段:最后的总结阶段，进行一些基准测试，生成响应的策报告。这个阶段我们可以不关注。 确定内存占用以及活跃数据的大小，我们应该是在程序的稳定阶段来进行确定，而不是在项目起初阶段来进行确定，如何确定，我们先看以下jvm的内存分配。 3.2 jvm内存分配和参数 jvm堆中主要的空间，就是以上新生代、老生代、永久代组成,整个堆大小=新生代大小 + 老生代大小 + 永久代大小。 具体的对象提升方式，这里不再过多介绍了，我们看下一些jvm命令参数，对堆大小的指定。如果不采用以下参数进行指定的话，虚拟机会自动选择合适的值，同时也会基于系统的开销自动调整。 分代 参数 描述 堆大小 -Xms 初始堆大小，默认为物理内存的1/64( -Xmx 最大堆大小，默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 新生代 -XX:NewSize 新生代空间大小初始值 -XX:MaxNewSize 新生代空间大小最大值 -Xmn 新生代空间大小，此处的大小是(eden+2 survivor space) 永久代 -XX:PermSize 永久代空间的初始值&最小值 -XX：MaxPermSize 永久代空间的最大值 老年代 老年代的空间大小会根据新生代的大小隐式设定 初始值=-Xmx减去-XX:NewSize的值 最小值=-Xmx值减去-XX:MaxNewSize的值 在设置的时候，如果关注性能开销的话，应尽量把永久代的初始值与最大值设置为同一值，因为永久代的大小调整需要进行FullGC 才能实现。 2.3 计算活跃数据大小 计算活跃数据大小应该遵循以下流程: 如前所述，活跃数据应该是基于应用程序稳定阶段时，观察长期存活与对象在java堆中占用的空间大小。 计算活跃数据时应该确保以下条件发生： 1.测试时，启动参数采用jvm默认参数，不人为设置。 2.确保Full GC 发生时，应用程序正处于稳定阶段。 采用jvm默认参数启动，是为了观察应用程序在稳定阶段的所需要的内存使用。 2.3.1 如何才算稳定阶段 一定得需要产生足够的压力，找到应用程序和生产环境高峰符合状态类似的负荷，在此之后达到峰值之后，保持一个稳定的状态，才算是一个稳定阶段。所以要达到稳定阶段，压力测试是必不可少的，具体如何如何对应用压力测试，本篇不过多说明,后期会有专门介绍的篇幅。 在确定了应用出于稳定阶段的时候，要注意观察应用的GC日志，特别是Full GC 日志。 GC日志指令: -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc: GC日志是收集调优所需信息的最好途径，即便是在生产环境，也可以开启GC日志来定位问题，开启GC日志对性能的影响极小，却可以提供丰富数据。 必须得有FullGC 日志，如果没有的话，可以采用监控工具强制调用一次，或者采用以下命令，亦可以触发 jmap -histo:live pid 在稳定阶段触发了FullGC我们一般会拿到如下信息: 从以上gc日志中，我们大概可以分析到，在发生fullGC之时，整个应用的堆占用以及GC时间，当然了，为了更加精确，应该多收集几次，获取一个平均值。或者是采用耗时最长的一次FullGC来进行估算。 在上图中，fullGC之后，老年代空间占用在93168kb（约93MB），我们以此定为老年代空间的活跃数据。 其他堆空间的分配，基于以下规则来进行。 空间 命令参数 建议扩大倍数 java heap -Xms和-Xmx 3-4倍FullGC后的老年代空间占用 永久代 -XX:PermSize-XX:MaxPermSize 1.2-1.5倍FullGc后的永久带空间占用 新生代 -Xmn 1-1.5倍FullGC之后的老年代空间占用 老年代 2-3倍FullGC后的老年代空间占用 基于以上规则和上图中的FullGC信息，我们现在可以规划的该应用堆空间为： java 堆空间: 373Mb (=老年代空间93168kb*4) 新生代空间:140Mb(=老年代空间93168kb*1.5) 永久代空间:5Mb(=永久代空间3135kb*1.5) 老年代空间: 233Mb=堆空间-新生代看空间=373Mb-140Mb 对应的应用启动参数应该为: java -Xms373m -Xmx373m -Xmn140m -XX:PermSize=5m -XX:MaxPermSize=5m 4.延迟调优 在调优之前，我们需要知道系统的延迟需求是那些，以及对应的延迟可调优指标是那些。 在确定了应用程序的活跃数据大小之后，我们需要再进行延迟性调优，因为对于此时堆内存大小，延迟性需求无法达到应用的需要，需要基于应用的情况来进行调试。 在这一步进行期间，我们可能会再次优化堆大小的配置，评估GC的持续时间和频率、以及是否需要切换到不同的垃圾收集器上。 4.1 系统延迟需求 在调优之前，我们需要知道系统的延迟需求是那些，以及对应的延迟可调优指标是那些。 应用程序可接受的平均停滞时间: 此时间与测量的Minor GC持续时间进行比较。 可接受的Minor GC频率：Minor GC的频率与可容忍的值进行比较。 可接受的最大停顿时间: 最大停顿时间与最差情况下FullGC的持续时间进行比较。 可接受的最大停顿发生的频率：基本就是FullGC的频率。 以上中，平均停滞时间和最大停顿时间，对用户体验最为重要，可以多关注。 基于以上的要求，我们需要统计以下数据: MinorGC的持续时间； 统计MinorGC的次数； FullGC的最差持续时间； 最差情况下，FullGC的频率； 4.2 优化新生代大小 比如如上的gc日志中，我们可以看到Minor GC的平均持续时间=0.069秒，MinorGC 的频率为0.389秒一次。 如果，我们系统的设置的平均停滞时间为50ms，当前的69ms明显是太长了，就需要调整。 我们知道新生代空间越大，Minor GC的GC时间越长，频率越低。 如果想减少其持续时长，就需要减少其空间大小。 如果想减小其频率，就需要加大其空间大小。 为了降低改变新生代的大小对其他区域的最小影响。在改变新生代空间大小的时候，尽量保持老年代空间的大小。 比如此次减少了新生代空间10%的大小，应该保持老年代和持代的大小不变化，第一步调优后的参数如下变化: java -Xms359m -Xmx359m -Xmn126m -XX:PermSize=5m -XX:MaxPermSize=5m 4.3 优化老年代大小 同上一步一样，在优化之前，也需要采集gc日志的数据。此次我们关注的是FullGC的持续时间和频率。 上图中，我们可以看到 FullGC 平均频率 =5.8s FullGC 平均持续时间=0.14s (以上为了测试，真实项目的fullGC 没有这么快) 如果没有FullGC的日志，有办法可以评估么？ 我们可以通过对象提升率进行计算。 对象提升率 比如上述中启动参数中，我们的老年代大小=233Mb。 那么需要多久才能填满老年代中这233Mb的空闲空间取决于新生代到老年代的提升率。 每次提升老年代占用量=每次MinorGC 之后 java堆占用情况 减去 MinorGC后新生代的空间占用 对象提升率=平均值（每次提升老年代占用量) 除以 老年代空间 有了对象提升率，我们就可以算出填充满老年代空间需要多少次minorGC，大概一次fullGC的时间就可以计算出来了。 比如: 上图中: 第一次minor GC 之后，老年代空间:13740kb - 13732kb =8kb 第二次minor GC 之后，老年代空间:22394kb - 17905kb =4489kb 第三次minor GC 之后，老年代空间:34739kb - 17917kb =16822kb 第四次minor GC 之后，老年代空间:48143kb - 17913kb =30230kb 第五次minor GC 之后，老年代空间:62112kb - 17917kb =44195kb 老年代每次minorGC提升率 4481kb 第二次和第一次minorGC之间 12333kb 第3次和第2次minorGC之间 13408kb 第4次和第3次minorGC之间 13965kb 第5次和第4次minorGC之间 我们可以测算出： 每次minorGC 的平均提升为12211kb,约为12Mb 上图中，平均minorGC的频率为 213ms/次 提升率=12211kb/213ms=57kb/ms 老年代空间233Mb ,占满大概需要233*1024/57=4185ms 约为4.185s。 FullGC的预期最差频率时长可以通过以上两种方式估算出来，可以调整老年代的大小来调整FullGC的频率，当然了，如果FullGC持续时间过长，无法达到应用程序的最差延迟要求，就需要切换垃圾处理器了。具体如何切换，下篇再讲，比如切换为CMS，针对CMS的调优方式又有会细微的差别。 5. 吞吐量调优 经过上述漫长 调优过程，最终来到了调优的最后一步，这一步对上述的结果进行吞吐量测试，并进行微调。 吞吐量调优主要是基于应用程序的吞吐量要求而来的，应用程序应该有一个综合的吞吐指标，这个指标基于真个应用的需求和测试而衍生出来的。当有应用程序的吞吐量达到或者超过预期的吞吐目标，整个调优过程就可以圆满结束了。 如果出现调优后依然无法达到应用程序的吞吐目标，需要重新回顾吞吐要求，评估当前吞吐量和目标差距是否巨大，如果在20%左右，可以修改参数，加大内存，再次从头调试，如果巨大就需要从整个应用层面来考虑，设计以及目标是否一致了，重新评估吞吐目标。 对于垃圾收集器来说，提升吞吐量的性能调优的目标就是就是尽可能避免或者很少发生FullGC 或者Stop-The-World压缩式垃圾收集（CMS），因为这两种方式都会造成应用程序吞吐降低。尽量在MinorGC 阶段回收更多的对象，避免对象提升过快到老年代。 6. 最后 据Plumbr公司对特定垃圾收集器使用情况进行了一次调查研究，研究数据使用了84936个案例。在明确指定垃圾收集器的13%的案例中，并发收集器（CMS）使用次数最多；但大多数案例没有选择最佳垃圾收集器。这个比例占用在87%左右。 JVM调优是一个系统而又复杂的工作，目前jvm下的自动调整已经做的比较优秀，基本的一些初始参数都可以保证一般的应用跑的比较稳定了，对部分团队来说，程序性能可能优先级不高，默认垃圾收集器已经够用了。调优要基于自己的情况而来。 参考文章 如何合理的规划JVM 性能调优 "},"base/jvm/常见的JVM设置.html":{"url":"base/jvm/常见的JVM设置.html","title":"常见的JVM设置","keywords":"","body":"常见的JVM设置参考文章常见的JVM设置 参考文章 JDK8 JVM参数与实际环境中的优化配置实践 JVM堆内存和非堆内存 "},"base/jvm/内存溢出时打印内存信息.html":{"url":"base/jvm/内存溢出时打印内存信息.html","title":"内存溢出时打印内存信息","keywords":"","body":"内存溢出时打印内存信息内存溢出时打印内存信息 我们在项目启动的时候可以添加启动配置 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$APP_DIR/java_heapdump.hprof 这样当内存溢出的时候就会将日志打印出来 "},"base/jvm/类加载过程.html":{"url":"base/jvm/类加载过程.html","title":"类加载过程","keywords":"","body":"类加载过程1. 加载2.连接2.1 验证2.2 准备2.3 解析3. 初始化4. 总结类加载过程 Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些Class文件呢？ 系统加载Class类型的文件主要三步：加载->连接 ->初始化。连接过程又可以分为三步：验证->准备->解析。 1. 加载 类加载过程的第一步，主要完成下面3件事情 通过全类名获取定义此类的二进制字节流（这个步骤就是类加载器） 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口 虚拟机规范多上面这3点并不具体，因此是非常灵活的。比如：\"通过全类名获取定义此类的二进制字节流\" 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的JAR、EAR、WAR格式的基础）、其他文件生成（典型应用就是JSP）等等。 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。 类加载器、双亲委派模型也是非常重要的知识点，这部分内容会在后面的文章中单独介绍到。 加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。 2.连接 2.1 验证 2.2 准备 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值\"通常情况\"下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会复制）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被复制为 111。 基本数据类型的零值： 2.3 解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 符号引用： 符号引用就是一组符号来描述目标，可以是任何字面量 直接引用 直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄 在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方发表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。 3. 初始化 初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行类构造器 ()方法的过程。 对于（） 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 （） 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 对于初始化阶段，虚拟机严格规范了有且只有5中情况下，必须对类进行初始化： 当遇到 new 、 getstatic、putstatic或invokestatic 这4条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 使用 java.lang.reflect 包的方法对类进行反射调用时 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 当使用 JDK1.7 的动态动态语言时，如果一个 MethodHandle 实例的最后解析结构为 REF_getStatic、REF_putStatic、REF_invokeStatic、的方法句柄，并且这个句柄没有初始化，则需要先触发器初始化。 4. 总结 类加载主要包含三个步骤：加载->连接->初始化，连接又可以分为：验证->准备-解析。 加载： 通过全类名获取此类二进制字节流 将字节流静态存储结构转为方法区运行时数据结构 内存生成一个代表该类的class对象，作为方法区这些数据的访问入口 连接 验证 准备 为类变量（static）分配内存，设置初始化值（是0，不是111） 解析 将常量池的符号引用替换为直接引用 初始化 执行类构造器 "},"base/jvm/类加载过程精简版.html":{"url":"base/jvm/类加载过程精简版.html","title":"类加载过程(精简版)","keywords":"","body":"类加载过程(精简版)1. 前言2. 类加载2.1 加载2.2 链接2.2.3 解析2.3 初始化3. 总结参考文章类加载过程(精简版) 1. 前言 一个java文件从编码完成到最终执行，一般主要包含两个过程 编译 运行 编译，即把我们写好的java文件，通过javac命令编译成字节码，也就是我们常说的.class文件。 运行，则是把编译生成的.class文件交给Java虚拟机(JVM)执行。 而我们所说的类加载过程即是指JVM虚拟机把.class文件中类信息加载进内存，并进行解析生成对应的class对象的过程。 举个通俗点的例子来说，JVM在执行某段代码时，遇到了class A， 然而此时内存中并没有class A的相关信息，于是JVM就会到相应的class文件中去寻找class A的类信息，并加载进内存中，这就是我们所说的类加载过程。 由此可见，JVM不是一开始就把所有的类都加载进内存中，而是只有第一次遇到某个需要运行的类时才会加载，且只加载一次。 2. 类加载 类加载的过程主要分为三个部分： 加载 链接 初始化 而链接又可以细分为三个小部分： 验证 准备 解析 2.1 加载 简单来说，加载指的是把class字节码文件从各个来源通过类加载器装载入内存中。 这里有两个重点： 字节码来源。一般的加载来源包括从本地路径下编译生成的.class文件，从jar包中的.class文件，从远程网络，以及动态代理实时编译 类加载器。一般包括启动类加载器，扩展类加载器，应用类加载器，以及用户的自定义类加载器。 注：为什么会有自定义类加载器？ 一方面是由于java代码很容易被反编译，如果需要对自己的代码加密的话，可以对编译后的代码进行加密，然后再通过实现自己的自定义类加载器进行解密，最后再加载。 另一方面也有可能从非标准的来源加载代码，比如从网络来源，那就需要自己实现一个类加载器，从指定源进行加载。 2.2 链接 2.2.1 验证 主要是为了保证加载进来的字节流符合虚拟机规范，不会造成安全错误。 包括对于文件格式的验证，比如常量中是否有不被支持的常量？文件中是否有不规范的或者附加的其他信息？ 对于元数据的验证，比如该类是否继承了被final修饰的类？类中的字段，方法是否与父类冲突？是否出现了不合理的重载？ 对于字节码的验证，保证程序语义的合理性，比如要保证类型转换的合理性。 对于符号引用的验证，比如校验符号引用中通过全限定名是否能够找到对应的类？校验符号引用中的访问性（private，public等）是否可被当前类访问？ 2.2.2 准备 主要是为类变量（注意，不是实例变量）分配内存，并且赋予初值。 特别需要注意，初值，不是代码中具体写的初始化的值，而是Java虚拟机根据不同变量类型的默认初始值。 比如8种基本类型的初值，默认为0；引用类型的初值则为null；常量的初值即为代码中设置的值，final static tmp = 456， 那么该阶段tmp的初值就是456 2.2.3 解析 将常量池内的符号引用替换为直接引用的过程。 两个重点： 符号引用。即一个字符串，但是这个字符串给出了一些能够唯一性识别一个方法，一个变量，一个类的相关信息。 直接引用。可以理解为一个内存地址，或者一个偏移量。比如类方法，类变量的直接引用是指向方法区的指针；而实例方法，实例变量的直接引用则是从实例的头指针开始算起到这个实例变量位置的偏移量 举个例子来说，现在调用方法hello()，这个方法的地址是1234567，那么hello就是符号引用，1234567就是直接引用。 在解析阶段，虚拟机会把所有的类名，方法名，字段名这些符号引用替换为具体的内存地址或偏移量，也就是直接引用。 2.3 初始化 这个阶段主要是对类变量初始化，是执行类构造器的过程。 换句话说，只对static修饰的变量或语句进行初始化。 如果初始化一个类的时候，其父类尚未初始化，则优先初始化其父类。 如果同时包含多个静态变量和静态代码块，则按照自上而下的顺序依次执行。 3. 总结 类加载过程只是一个类生命周期的一部分，在其前，有编译的过程，只有对源代码编译之后，才能获得能够被虚拟机加载的字节码文件；在其后还有具体的类使用过程，当使用完成之后，还会在方法区垃圾回收的过程中进行卸载。 相关扩展知识点： Java虚拟机的基本机构？ 什么是类加载器？ 简单谈一下类加载的双亲委托机制？ 普通Java类的类加载过程和Tomcat的类加载过程是否一样？区别在哪？ 简单谈一下Java堆的垃圾回收机制？ 参考文章 面试官：请你谈谈Java的类加载过程 "},"base/jvm/类加载器.html":{"url":"base/jvm/类加载器.html","title":"类加载器","keywords":"","body":"类加载器1. 回顾类加载过程2. 类加载器总结3. 双亲委派模型3.1 介绍3.2 双亲委派模型源码分析3.3 双亲委派模型的好处4. 不使用双亲委派模型如果我们不想用双亲委派模型怎么办？5. 自定义类加载器类加载器 1. 回顾类加载过程 类加载过程：加载->连接->初始化。连接过程由可以分成三步：验证->准备->解析 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，他由Java虚拟机直接创建 所有的类都是由类加载器加载，加载的作用就是将.class文件加载到内存 2. 类加载器总结 JVM中内置了三个重要的ClassLoader，除了BootstrapClassLoader 其他类加载器均有 Java 实现且全部继承自java.lang.ClassLoader： BootstrapClassLoader(启动类加载器)：最顶层的加载类，由C++实现。负责加载%JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。 ExtClassLoader（扩展类加载器）：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。 AppClassLoader(应用程序类加载器) ：面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类 3. 双亲委派模型 3.1 介绍 每一个类都有一个对应他的类加载器。系统中的ClassLoader 在协同工作的时候会默认使用 双亲委派模型。既在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。 每个类加载都有一个父类加载器，我们通过下面的程序来验证。 public class ClassLoaderDemo { public static void main(String[] args) { System.out.println(\"ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader()); System.out.println(\"The Parent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent()); System.out.println(\"The GrandParent of ClassLodarDemo's ClassLoader is \" + ClassLoaderDemo.class.getClassLoader().getParent().getParent()); } } 输出 ClassLodarDemo's ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2 The Parent of ClassLodarDemo's ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586 The GrandParent of ClassLodarDemo's ClassLoader is null AppClassLoader的父类加载器为ExtClassLoader ExtClassLoader的父类加载器为null，null并不代表ExtClassLoader没有父类加载器，而是 BootstrapClassLoader 。 其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个 Mother ClassLoader 和一个 Father ClassLoader 。另外，类加载器之间的“父子”关系也不是通过继承来体现的，是由“优先级”来决定。官方API文档对这部分的描述如下: The Java platform uses a delegation model for loading classes. The basic idea is that every class loader has a \"parent\" class loader. When loading a class, a class loader first \"delegates\" the search for the class to its parent class loader before attempting to find the class itself. 3.2 双亲委派模型源码分析 双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 java.lang.ClassLoader 的 loadClass() 中，相关代码如下所示。 private final ClassLoader parent; protected Class loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查请求的类是否已经被加载过 Class c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) {//父加载器不为空，调用父加载器loadClass()方法处理 c = parent.loadClass(name, false); } else {//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { //抛出异常说明父类加载器无法完成加载请求 } if (c == null) { long t1 = System.nanoTime(); //自己尝试加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 3.3 双亲委派模型的好处 双亲委派模型保证了Java程序的稳定运行，可以避免类的重复加载 （JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类） 保证了 Java 的核心 API 不被篡改 如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。 4. 不使用双亲委派模型 如果我们不想用双亲委派模型怎么办？ 为了避免双亲委托机制，我们可以自己定义一个类加载器，然后重载 loadClass() 即可。 5. 自定义类加载器 除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader。 "},"base/jvm/类加载器常见面试.html":{"url":"base/jvm/类加载器常见面试.html","title":"类加载器（常见面试）","keywords":"","body":"类加载器（常见面试）面试官：请说说你理解的类加载器。面试官：说说有哪几种类加载器，他们的职责分别是什么，他们之前存在什么样的约定。面试官插嘴：ExtClassLoader为什么没有设置parent？双亲委派模型的好处面试官：那自己怎么去实现一个ClassLoader呢？请举个实际的例子。面试官插嘴：为什么不继承AppClassLoader呢？参考文章类加载器（常见面试） 面试官：请说说你理解的类加载器。 我：通过一个类的全限定名来获取描述此类的二进制字节流这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这个动作的代码模块称为“类加载器”。 面试官：说说有哪几种类加载器，他们的职责分别是什么，他们之前存在什么样的约定。 我：emmmm，我在纸上边画边讲吧。 类加载的结构如下： BootstrapClassLoader：启动类类加载器，它用来加载/jre/lib路径,-Xbootclasspath参数指定的路径以/jre/classes中的类。BootStrapClassLoader是由c++实现的。 ExtClassLoader：拓展类类加载器，它用来加载/jre/lib/ext路径以及java.ext.dirs系统变量指定的类路径下的类。 AppClassLoader：应用程序类类加载器，它主要加载应用程序ClassPath下的类（包含jar包中的类）。它是java应用程序默认的类加载器。 用户自定义类加载器：用户根据自定义需求，自由的定制加载的逻辑，继承ClassLoader，仅仅覆盖findClass（）即将继续遵守双亲委派模型。 在虚拟机启动的时候会初始化BootstrapClassLoader，然后在Launcher类中去加载ExtClassLoader、AppClassLoader，并将AppClassLoader的parent设置为ExtClassLoader，并设置线程上下文类加载器。 Launcher是JRE中用于启动程序入口main()的类，让我们看下Launcher的代码 public Launcher() { Launcher.ExtClassLoader var1; try { //加载扩展类类加载器 var1 = Launcher.ExtClassLoader.getExtClassLoader(); } catch (IOException var10) { throw new InternalError(\"Could not create extension class loader\", var10); } try { //加载应用程序类加载器，并设置parent为extClassLoader this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); } catch (IOException var9) { throw new InternalError(\"Could not create application class loader\", var9); } //设置默认的线程上下文类加载器为AppClassLoader Thread.currentThread().setContextClassLoader(this.loader); //此处删除无关代码。。。 } 上面画的几种类加载器是遵循双亲委派模型的，其实就是，当一个类加载器去加载类时先尝试让父类加载器去加载，如果父类加载器加载不了再尝试自身加载。这也是我们在自定义ClassLoader时java官方建议遵守的约定。 面试官插嘴：ExtClassLoader为什么没有设置parent？ 让我们看看下面代码的输出结果 public static void main(String[] args) throws ClassNotFoundException { ClassLoader classLoader = Test.class.getClassLoader(); System.out.println(classLoader); System.out.println(classLoader.getParent()); System.out.println(classLoader.getParent().getParent()); } 看看结果是啥 sun.misc.Launcher$AppClassLoader@18b4aac2 sun.misc.Launcher$ExtClassLoader@5a61f5df null 因为BootstrapClassLoader是由c++实现的，所以并不存在一个Java的类，因此会打印出null，所以在ClassLoader中，null就代表了BootStrapClassLoader（有些片面）。 双亲委派模型的好处 双亲委派模型能保证基础类仅加载一次，不会让jvm中存在重名的类。比如String.class，每次加载都委托给父加载器，最终都是BootstrapClassLoader，都保证java核心类都是BootstrapClassLoader加载的，保证了java的安全与稳定性。 API不会被篡改 面试官：那自己怎么去实现一个ClassLoader呢？请举个实际的例子。 我：好的（脸上笑嘻嘻，心里mmp）。 自己实现ClassLoader时只需要继承ClassLoader类，然后覆盖findClass（String name）方法即可完成一个带有双亲委派模型的类加载器。 我们看下ClassLoader#loadClass的代码 protected Class loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 查看是否已经加载过该类，加载过的类会有缓存，是使用native方法实现的 Class c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { //父类不为空则先让父类加载 if (parent != null) { c = parent.loadClass(name, false); } else { //父类是null就是BootstrapClassLoader，使用启动类类加载器加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // 父类类加载器不能加载该类 } //如果父类未加载该类 if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //让当前类加载器加载 c = findClass(name); } } return c; } } 经典的模板方法模式，子类只需要实现findClass，关心从哪里加载即可。 还有一点，parent需要自己设置哦，可以放在构造函数做这个事情。 面试官插嘴：为什么不继承AppClassLoader呢？ 我：因为它和ExtClassLoader都是Launcher的静态类，都是包访问路径权限的。 参考文章 好怕怕的类加载器 "},"base/jvm/tomcat类加载器.html":{"url":"base/jvm/tomcat类加载器.html","title":"tomcat类加载器","keywords":"","body":"tomcat类加载器1. 如何破坏双亲委派模型2. Tomcat 的类加载器是怎么设计的？2.1 tomcat是个web容器，那么他要解决什么问题2.2 Tomcat 如何实现自己独特的类加载机制？参考文章tomcat类加载器 通过前文我们已经了解了类加载器以及双亲委派模型，并且了解为什么使用双亲委派模型。但我们考虑一下我们tomcat 中的场景 一个web容器可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本。不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。 如果我们使用双亲委派模型就不能保证他们的隔离性了，那么我们如果要破坏双亲委派模型 1. 如何破坏双亲委派模型 双亲委任模型不是一个强制性的约束模型，而是一个建议型的类加载器实现方式。 历史上出现的三次破坏 第一次：双亲委派模型出现之前，既jdk 1.2 发布之前 第二次：因为自身模型缺陷 双亲委派模型解决了各个类加载器的基础类的同一问题（越基础的类由越上层的加载器进行加载），基础类之所以称为“基础”，是因为它们总是作为被用户代码调用的API， 但没有绝对，如果基础类调用会用户的代码怎么办呢？ 为了解决这个问题，Java设计团队只好引入了一个不太优雅的设计：线程上下文类加载器（Thread Context ClassLoader）。这个类加载器可以通过java.lang.Thread类的setContextClassLoader方法进行设置。如果创建线程时还未设置，它将会从父线程中继承一个，如果在应用程序的全局范围内都没有设置过多的话，那这个类加载器默认即使应用程序类加载器。 嘿嘿，有了线程上下文加载器，JNDI服务使用这个线程上下文加载器去加载所需要的SPI代码，也就是父类加载器请求子类加载器去完成类加载的动作，这种行为实际上就是打通了双亲委派模型的层次结构来逆向使用类加载器，实际上已经违背了双亲委派模型的一般性原则。但这无可奈何，Java中所有涉及SPI的加载动作基本胜都采用这种方式。例如JNDI，JDBC，JCE，JAXB，JBI等。 第三次：为了实现热插拔，热部署，模块化，意思是添加一个功能或减去一个功能不用重启，只需要把这模块连同类加载器一起换掉就实现了代码的热替换。 2. Tomcat 的类加载器是怎么设计的？ 2.1 tomcat是个web容器，那么他要解决什么问题 一个web容器可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。 部署在同一个web容器中相同的类库相同的版本可以共享。否则，如果服务器有10个应用程序，那么要有10份相同的类库加载进虚拟机，这是扯淡的。 web容器也有自己依赖的类库，不能于应用程序的类库混淆。基于安全考虑，应该让容器的类库和程序的类库隔离开来。 web容器要支持jsp的修改，我们知道，jsp 文件最终也是要编译成class文件才能在虚拟机中运行，但程序运行后修改jsp已经是司空见惯的事情，否则要你何用？ 所以，web容器需要支持 jsp 修改后不用重启。 再看看我们的问题：Tomcat 如果使用默认的类加载机制行不行？ 答案是不行的。为什么？我们看， 第一个问题，如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的累加器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。 第二个问题，默认的类加载器是能够实现的，因为他的职责就是保证唯一性。 第三个问题和第一个问题一样。 四个问题，我们想我们要怎么实现jsp文件的热修改（楼主起的名字），jsp 文件其实也就是class文件，那么如果修改了，但类名还是一样，类加载器会直接取方法区中已经存在的，修改后的jsp是不会重新加载的。那么怎么办呢？我们可以直接卸载掉这jsp文件的类加载器，所以你应该想到了，每个jsp文件对应一个唯一的类加载器，当一个jsp文件修改了，就直接卸载这个jsp类加载器。重新创建类加载器，重新加载jsp文件。 2.2 Tomcat 如何实现自己独特的类加载机制？ 所以，Tomcat 是怎么实现的呢？牛逼的Tomcat团队已经设计好了。我们看看他们的设计图： 我们看到，前面3个类加载和默认的一致，CommonClassLoader、CatalinaClassLoader、SharedClassLoader和WebappClassLoader则是Tomcat自己定义的类加载器，它们分别加载/common/*、/server/*、/shared/*（在tomcat 6之后已经合并到根目录下的lib目录下）和/WebApp/WEB-INF/*中的Java类库。其中WebApp类加载器和Jsp类加载器通常会存在多个实例，每一个Web应用程序对应一个WebApp类加载器，每一个JSP文件对应一个Jsp类加载器。 commonLoader：Tomcat最基本的类加载器，加载路径中的class可以被Tomcat容器本身以及各个Webapp访问； catalinaLoader：Tomcat容器私有的类加载器，加载路径中的class对于Webapp不可见； sharedLoader：各个Webapp共享的类加载器，加载路径中的class对于所有Webapp可见，但是对于Tomcat容器不可见； WebappClassLoader：各个Webapp私有的类加载器，加载路径中的class只对当前Webapp可见； 从图中的委派关系中可以看出： CommonClassLoader能加载的类都可以被Catalina ClassLoader和SharedClassLoader使用，从而实现了公有类库的共用，而CatalinaClassLoader和Shared ClassLoader自己能加载的类则与对方相互隔离。 WebAppClassLoader可以使用SharedClassLoader加载到的类，但各个WebAppClassLoader实例之间相互隔离。 而JasperLoader的加载范围仅仅是这个JSP文件所编译出来的那一个.Class文件，它出现的目的就是为了被丢弃：当Web容器检测到JSP文件被修改时，会替换掉目前的JasperLoader的实例，并通过再建立一个新的Jsp类加载器来实现JSP文件的HotSwap功能。 很显然，tomcat 不是这样实现，tomcat 为了实现隔离性，没有遵守这个约定，每个webappClassLoader加载自己的目录下的class文件，不会传递给父类加载器。 我们扩展出一个问题：如果tomcat 的 Common ClassLoader 想加载 WebApp ClassLoader 中的类，该怎么办？ 看了前面的关于破坏双亲委派模型的内容，我们心里有数了，我们可以使用线程上下文类加载器实现，使用线程上下文加载器，可以让父类加载器请求子类加载器去完成类加载的动作。牛逼吧。 参考文章 深入理解 Tomcat（四）Tomcat 类加载器之为何违背双亲委派模型 "},"base/jvm/JDK监控和故障处理工具汇总.html":{"url":"base/jvm/JDK监控和故障处理工具汇总.html","title":"JDK监控和故障处理工具汇总","keywords":"","body":"JDK监控和故障处理工具汇总1. JDK命令行工具1.1 jps:查看所有 Java 进程1.2 jstat: 监视虚拟机各种运行状态信息1.3 jinfo: 实时地查看和调整虚拟机各项参数1.4 jmap:生成堆转储快照1.5 jhat: 分析 heapdump 文件1.6 jstack :生成虚拟机当前时刻的线程快照2. JDK 可视化分析工具isual VM:多合一故障处理工具JDK监控和故障处理工具汇总 1. JDK命令行工具 这些命令在JDK 安装目录下的bin目录下 jps (JVM Process Status）: 类似 UNIX 的 ps 命令。用户查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息； jstat（ JVM Statistics Monitoring Tool）: 用于收集 HotSpot 虚拟机各方面的运行数据; jinfo (Configuration Info for Java) : Configuration Info forJava,显示虚拟机配置信息; jmap (Memory Map for Java) :生成堆转储快照; jhat (JVM Heap Dump Browser ) : 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果; jstack (Stack Trace for Java):生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。 1.1 jps:查看所有 Java 进程 jps(JVM Process Status) 命令类似 UNIX 的 ps 命令。 jps：显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier,LVMID）。jps -q ：只输出进程的本地虚拟机唯一 ID。 C:\\Users\\SnailClimb>jps 7360 NettyClient2 17396 7972 Launcher 16504 Jps 17340 NettyServer jps -l:输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 路径。 C:\\Users\\SnailClimb>jps -l 7360 firstNettyDemo.NettyClient2 17396 7972 org.jetbrains.jps.cmdline.Launcher 16492 sun.tools.jps.Jps 17340 firstNettyDemo.NettyServer 5541 mywebsocket.jar jps -v：输出虚拟机进程启动时 JVM 参数。 jps -m：输出传递给 Java 进程 main() 函数的参数。 1.2 jstat: 监视虚拟机各种运行状态信息 jstat（JVM Statistics Monitoring Tool） 使用于监视虚拟机各种运行状态信息的命令行工具。 它可以显示本地或者远程（需要远程主机提供 RMI 支持）虚拟机进程中的类信息、内存、垃圾收集、JIT 编译等运行数据，在没有 GUI，只提供了纯文本控制台环境的服务器上，它将是运行期间定位虚拟机性能问题的首选工具。 jstat 命令使用格式： jstat - [-t] [-h] [ []] 比如 jstat -gc -h3 31736 1000 10表示分析进程 id 为 31736 的 gc 情况，每隔 1000ms 打印一次记录，打印 10 次停止，每 3 行后打印指标头部 常见的 option 如下： jstat -class vmid ：显示 ClassLoader 的相关信息； jstat -compiler vmid ：显示 JIT 编译的相关信息； jstat -gc vmid ：显示与 GC 相关的堆信息； jstat -gccapacity vmid ：显示各个代的容量及使用情况； jstat -gcnew vmid ：显示新生代信息； jstat -gcnewcapcacity vmid ：显示新生代大小与使用情况； jstat -gcold vmid ：显示老年代和永久代的信息； jstat -gcoldcapacity vmid ：显示老年代的大小； jstat -gcpermcapacity vmid ：显示永久代大小； jstat -gcutil vmid ：显示垃圾收集信息； 另外，加上 -t参数可以在输出信息上加一个 Timestamp 列，显示程序的运行时间。 1.3 jinfo: 实时地查看和调整虚拟机各项参数 jinfo vmid :输出当前 jvm 进程的全部参数和系统属性 (第一部分是系统的属性，第二部分是 JVM 的参数)。 jinfo -flag name vmid :输出对应名称的参数的具体值。比如输出 MaxHeapSize、查看当前 jvm 进程是否开启打印 GC 日志 ( -XX:PrintGCDetails :详细 GC 日志模式，这两个都是默认关闭的)。 C:\\Users\\SnailClimb>jinfo -flag MaxHeapSize 17340 -XX:MaxHeapSize=2124414976 C:\\Users\\SnailClimb>jinfo -flag PrintGC 17340 -XX:-PrintGC 使用 jinfo 可以在不重启虚拟机的情况下，可以动态的修改 jvm 的参数。尤其在线上的环境特别有用,请看下面的例子： jinfo -flag [+|-]name vmid 开启或者关闭对应名称的参数。 C:\\Users\\SnailClimb>jinfo -flag PrintGC 17340 -XX:-PrintGC C:\\Users\\SnailClimb>jinfo -flag +PrintGC 17340 C:\\Users\\SnailClimb>jinfo -flag PrintGC 17340 -XX:+PrintGC 1.4 jmap:生成堆转储快照 jmap（Memory Map for Java）命令用于生成堆转储快照。 如果不使用 jmap 命令，要想获取 Java 堆转储，可以使用 “-XX:+HeapDumpOnOutOfMemoryError” 参数，可以让虚拟机在 OOM 异常出现之后自动生成 dump 文件，Linux 命令下可以通过 kill -3 发送进程退出信号也能拿到 dump 文件。 jmap 的作用并不仅仅是为了获取 dump 文件，它还可以查询 finalizer 执行队列、Java 堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在 Windows 平台下也是受限制的。 示例：将指定应用程序的堆快照输出到桌面。后面，可以通过 jhat、Visual VM 等工具分析该堆文件。 jmap -dump:format=b,file=./heap.hprof 19012 Dumping heap to /home/ftpuser/services/mywebsocket/heap.hprof ... Heap dump file created 1.5 jhat: 分析 heapdump 文件 jhat 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果。 C:\\Users\\SnailClimb>jhat C:\\Users\\SnailClimb\\Desktop\\heap.hprof Reading from C:\\Users\\SnailClimb\\Desktop\\heap.hprof... Dump file created Sat May 04 12:30:31 CST 2019 Snapshot read, resolving... Resolving 131419 objects... Chasing references, expect 26 dots.......................... Eliminating duplicate references.......................... Snapshot resolved. Started HTTP server on port 7000 Server is ready. 访问 http://localhost:7000/ 1.6 jstack :生成虚拟机当前时刻的线程快照 jstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合. 生成线程快照的目的主要是定位线程长时间出现停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的原因。线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做些什么事情，或者在等待些什么资源。 下面是一个线程死锁的代码。我们下面会通过 jstack 命令进行死锁检查，输出死锁信息，找到发生死锁的线程 public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -> { synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource2\"); synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); } } }, \"线程 1\").start(); new Thread(() -> { synchronized (resource2) { System.out.println(Thread.currentThread() + \"get resource2\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \"waiting get resource1\"); synchronized (resource1) { System.out.println(Thread.currentThread() + \"get resource1\"); } } }, \"线程 2\").start(); } } Output Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。 通过 jstack 命令分析： C:\\Users\\SnailClimb>jps 13792 KotlinCompileDaemon 7360 NettyClient2 17396 7972 Launcher 8932 Launcher 9256 DeadLockDemo 10764 Jps 17340 NettyServer C:\\Users\\SnailClimb>jstack 9256 输出的部分内容如下： Found one Java-level deadlock: ============================= \"线程 2\": waiting to lock monitor 0x000000000333e668 (object 0x00000000d5efe1c0, a java.lang.Object), which is held by \"线程 1\" \"线程 1\": waiting to lock monitor 0x000000000333be88 (object 0x00000000d5efe1d0, a java.lang.Object), which is held by \"线程 2\" Java stack information for the threads listed above: =================================================== \"线程 2\": at DeadLockDemo.lambda$main$1(DeadLockDemo.java:31) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) at DeadLockDemo$$Lambda$2/1078694789.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) \"线程 1\": at DeadLockDemo.lambda$main$0(DeadLockDemo.java:16) - waiting to lock (a java.lang.Object) - locked (a java.lang.Object) at DeadLockDemo$$Lambda$1/1324119927.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock. 可以看到 jstack 命令已经帮我们找到发生死锁的线程的具体信息。 2. JDK 可视化分析工具 isual VM:多合一故障处理工具 VisualVM 提供在 Java 虚拟机 (Java Virutal Machine, JVM) 上运行的 Java 应用程序的详细信息。在 VisualVM 的图形用户界面中，您可以方便、快捷地查看多个 Java 应用程序的相关信息。Visual VM 官网：https://visualvm.github.io/ 。Visual VM 中文文档:https://visualvm.github.io/documentation.html。 下面这段话摘自《深入理解 Java 虚拟机》。 VisualVM（All-in-One Java Troubleshooting Tool）是到目前为止随 JDK 发布的功能最强大的运行监视和故障处理程序，官方在 VisualVM 的软件说明中写上了“All-in-One”的描述字样，预示着他除了运行监视、故障处理外，还提供了很多其他方面的功能，如性能分析（Profiling）。VisualVM 的性能分析功能甚至比起 JProfiler、YourKit 等专业且收费的 Profiling 工具都不会逊色多少，而且 VisualVM 还有一个很大的优点：不需要被监视的程序基于特殊 Agent 运行，因此他对应用程序的实际性能的影响很小，使得他可以直接应用在生产环境中。这个优点是 JProfiler、YourKit 等工具无法与之媲美的。 VisualVM 基于 NetBeans 平台开发，因此他一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM 可以做到： 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的 CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。 dump 以及分析堆转储快照（jmap、jhat）。 方法级的程序运行性能分析，找到被调用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程 dump、内存 dump 等信息建立一个快照，可以将快照发送开发者处进行 Bug 反馈。 其他 plugins 的无限的可能性...... 这里就不具体介绍 VisualVM 的使用，如果想了解的话可以看: https://visualvm.github.io/documentation.html https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/index.html "},"base/jvm/mat安装.html":{"url":"base/jvm/mat安装.html","title":"MAT安装","keywords":"","body":"MAT安装1. 下载2. 安装报错2.1 报错提示2.2 报错原因2.3 MemoryAnalyzer.ini位置MAT安装 1. 下载 下载地址选择适合自己的系统版本 2. 安装报错 2.1 报错提示 2.2 报错原因 但是官方文档指出MAT需要1.7 Java才可以运行，所以此时需要为MAT指定一个1.7的Java VM。只需要在MemoryAnalyzer.ini文件中最开始添加 -vm #例如我的 -vm /Library/Java/JavaVirtualMachines/jdk1.8.0_71.jdk/Contents/Home/bin 2.3 MemoryAnalyzer.ini位置 mat.app 打开就可以看见 /Applications/mat.app/Contents/Eclipse/MemoryAnalyzer.ini "},"base/jvm/mat使用.html":{"url":"base/jvm/mat使用.html","title":"MAT使用","keywords":"","body":"MAT使用1 MAT主界面介绍1.1 导入hprof文件1.2 OverView 界面2. MAT 中一些概念2.1 Shallow heap2.2 Retained Heap2.3 GC Root3. MAT 中一些有用的视图3.1 Thread OvewView3.2 Group3.3 Path to GC Root参考文章MAT使用 1 MAT主界面介绍 1.1 导入hprof文件 导入文件之后，显示OverView界面 如果选择了第一个，则会生成一个报告 1.2 OverView 界面 需要关注的是下面的Actions区域 Histogram: 列出内存中的对象，对象的个人以及大小 Dominator Tree: 列出最大的对象以及其依赖存活的Object（大小以Retained Heap为标准的排序） Top Consumers: 通过图形列出最大的Object 一般Histogram和 Dominator Tree是最常用的。 2. MAT 中一些概念 要看懂MAT的列表信息，Shallow heap、Retained Heap、GC Root这几个概念一定要弄懂。 2.1 Shallow heap Shallow size 就是对象本身占用内存的大小，不包含其引用的对象 常规对象（非数组）的Shallow size有其成员变量的数量和类型决定 数组的shallow size 有数组元素的类型（对象类型，基本类型）和数组长度决定 因为不像c++的对象本身可以存放大量内存，java的对象成员都是些引用。真正的内存都在堆上，看起来是一堆原生的byte[], char[], int[]，所以我们如果只看对象本身的内存，那么数量都很小。所以我们看到Histogram图是以Shallow size进行排序的，排在第一位第二位的是byte，char 。 2.2 Retained Heap Retained Heap的概念，它表示如果一个对象被释放掉，那会因为该对象的释放而减少引用进而被释放的所有的对象（包括被递归释放的）所占用的heap大小。于是，如果一个对象的某个成员new了一大块int数组，那这个int数组也可以计算到这个对象中。相对于shallow heap，Retained heap可以更精确的反映一个对象实际占用的大小（因为如果该对象释放，retained heap都可以被释放）。 这里要说一下的是，Retained Heap并不总是那么有效。例如我在A里new了一块内存，赋值给A的一个成员变量。此时我让B也指向这块内存。此时，因为A和B都引用到这块内存，所以A释放时，该内存不会被释放。所以这块内存不会被计算到A或者B的Retained Heap中。为了纠正这点，MAT中的Leading Object（例如A或者B）不一定只是一个对象，也可以是多个对象。此时，(A, B)这个组合的Retained Set就包含那块大内存了。对应到MAT的UI中，在Histogram中，可以选择Group By class, superclass or package来选择这个组。 为了计算Retained Memory，MAT引入了Dominator Tree。加入对象A引用B和C，B和C又都引用到D（一个菱形）。此时要计算Retained Memory，A的包括A本身和B，C，D。B和C因为共同引用D，所以他俩的Retained Memory都只是他们本身。D当然也只是自己。我觉得是为了加快计算的速度，MAT改变了对象引用图，而转换成一个对象引用树。在这里例子中，树根是A，而B，C，D是他的三个儿子。B，C，D不再有相互关系。把引用图变成引用树，计算Retained Heap就会非常方便，显示也非常方便。对应到MAT UI上，在dominator tree这个view中，显示了每个对象的shallow heap和retained heap。然后可以以该节点位树根，一步步的细化看看retained heap到底是用在什么地方了。要说一下的是，这种从图到树的转换确实方便了内存分析，但有时候会让人有些疑惑。本来对象B是对象A的一个成员，但因为B还被C引用，所以B在树中并不在A下面，而很可能是平级。 为了纠正这点，MAT中点击右键，可以List objects中选择with outgoing references和with incoming references。这是个真正的引用图的概念， outgoing references ：表示该对象的出节点（被该对象引用的对象）。 incoming references ：表示该对象的入节点（引用到该对象的对象）。 为了更好地理解Retained Heap，下面引用一个例子来说明： 把内存中的对象看成下图中的节点，并且对象和对象之间互相引用。这里有一个特殊的节点GC Roots，这就是reference chain(引用链)的起点: 2.3 GC Root GC发现通过任何reference chain(引用链)无法访问某个对象的时候，该对象即被回收。名词GC Roots正是分析这一过程的起点，例如JVM自己确保了对象的可到达性(那么JVM就是GC Roots)，所以GC Roots就是这样在内存中保持对象可到达性的，一旦不可到达，即被回收。通常GC Roots是一个在current thread(当前线程)的call stack(调用栈)上的对象（例如方法参数和局部变量），或者是线程自身或者是system class loader(系统类加载器)加载的类以及native code(本地代码)保留的活动对象。所以GC Roots是分析对象为何还存活于内存中的利器。 3. MAT 中一些有用的视图 3.1 Thread OvewView Thread OvewView可以查看这个应用的Thread信息： 3.2 Group 在Histogram和Domiantor Tree界面，可以选择将结果用另一种Group的方式显示（默认是Group by Object），切换到Group by package，可以更好地查看具体是哪个包里的类占用内存大，也很容易定位到自己的应用程序。 3.3 Path to GC Root 在Histogram或者Domiantor Tree的某一个条目上，右键可以查看其GC Root Path： 这里也要说明一下Java的引用规则： 从最强到最弱，不同的引用（可到达性）级别反映了对象的生命周期。 Strong Ref(强引用)：通常我们编写的代码都是Strong Ref，于此对应的是强可达性，只有去掉强可达，对象才被回收。 Soft Ref(软引用)：对应软可达性，只要有足够的内存，就一直保持对象，直到发现内存吃紧且没有Strong Ref时才回收对象。一般可用来实现缓存，通过java.lang.ref.SoftReference类实现。 Weak Ref(弱引用)：比Soft Ref更弱，当发现不存在Strong Ref时，立刻回收对象而不必等到内存吃紧的时候。通过java.lang.ref.WeakReference和java.util.WeakHashMap类实现。 Phantom Ref(虚引用)：根本不会在内存中保持任何对象，你只能使用Phantom Ref本身。一般用于在进入finalize()方法后进行特殊的清理过程，通过 java.lang.ref.PhantomReference实现。 点击Path To GC Roots –> with all references 参考文章 Android内存优化之一：MAT使用入门 "},"base/jvm/mat/Shallow和Retained.html":{"url":"base/jvm/mat/Shallow和Retained.html","title":"Shallow heap和Retained heap","keywords":"","body":"Shallow heap和Retained heap1 概念1.1 Shallow Size1.2 Retained Size2. 看图理解Retained Size如果GC Roots不引用D对象？参考文章Shallow heap和Retained heap 所有包含Heap Proflin 功能的工具（MAT，TPTP等）都会使用到两个名词，一个是Shallow Size，另一个是Retained Size 1 概念 1.1 Shallow Size 对象自身占用的内存大小，不包括他引用的对象 针对非数组类型的对象，它的大小就是对象与他所有的成员变量大小的总和。当然这里面还会包括一些java语言特性的数据存储单元。 针对数组类型的对象，它的大小是数组元素对象的大小总和。 1.2 Retained Size Retained Size= 当前对象大小+当前对象可直接或间接引用到的对象的大小总和(间接引用的含义：A->B->C, C就是间接引用) 换句话说，Retained Size 就是当前对象被GC后，从Heap 上总共能释放掉的内存，从Heap 上总共能释放掉的内存 不过，释放的时候还要排除被GC Roots直接或间接引用的对象。他们暂时不会被被当做Garbage。 2. 看图理解Retained Size 上图中，GC Roots直接引用了A和B 两个对象 A对象的Retained Size = A对象Shallow Size B对象的Reatined Size = B 对象的Shallow Size + C对象的Shallow Size ps:这里不包含D对象，因为D对象被GCroot直接引用了 如果GC Roots不引用D对象？ 此时，B对象的Retained Size=B对象的Shallow Size + C对象的Shallow Size + D对象的Shallow Size 参考文章 Shallow heap & Retained heap "},"base/jvm/mat/记一次MAT分析线上项目过程.html":{"url":"base/jvm/mat/记一次MAT分析线上项目过程.html","title":"记一次MAT分析线上项目过程","keywords":"","body":"记一次MAT分析线上项目过程1. 背景2. jmap:生成堆转储快照3. MAT分析3.1 查看内存泄漏疑点报告3.2 查看Histogram3.3 查看Dominator Tree3.4 Top consumers参考文章记一次MAT分析线上项目过程 1. 背景 前段时间接手了一个项目，正常运行都没有问题。但是运行个几天就会OOM异常，导致服务不可用。我们首先第一个想到的就是该项目内存泄漏导致，但是项目本身已经比较庞大，要找到一个内存泄漏的点，还是比较难得。 所以我们使用MAT来分析线上项目的运行情况 2. jmap:生成堆转储快照 jmap -dump:format=b,file=./heap.hprof 19012 Dumping heap to /home/ftpuser/services/mywebsocket/heap.hprof ... Heap dump file created 19012 是进程号 我们将heap.hprof 导出到我们本地，使用MAT来分析 3. MAT分析 3.1 查看内存泄漏疑点报告 这是最简单有效的方式，我们根据报告的提示来进行分析 我们点开报告得到 我们根据报告得知，有个对象已经占用了44.4 MB的内存，他来源于ConcurrentHashMap$Node[] 3.2 查看Histogram 通过查看Histogram，列出内存中的对象情况（个数，大小） Class Name ： 类名称，java类名 Objects ： 类的对象的数量，这个对象被创建了多少个 Shallow Heap ：一个对象内存的消耗大小，不包含对其他对象的引用 Retained Heap ：是shallow Heap的总和，也就是该对象被GC之后所能回收到内存的总和 3.2.1 通过正则查找自己的类 这儿借助工具提供的regex正则搜索一下我们自己的类，排序后看看哪些相对是占用比较大的。 我们可以看出内存泄漏是有EsscWebSocket开始的 3.3 查看Dominator Tree 3.4 Top consumers 这张图展示的是占用内存比较多的**对象的分布，下面是具体的一些类和占用。** 参考文章 Java程序内存分析：使用mat工具分析内存占用 "},"base/jvm/JVM面试提问.html":{"url":"base/jvm/JVM面试提问.html","title":"JVM面试提问","keywords":"","body":"JVM面试提问1. 内存区域篇2. 垃圾回收篇3. 类加载篇4. JVM故障排查与调优JVM面试提问 1. 内存区域篇 介绍下Java内存区域（运行时数据区） 哪些是线程私有的？哪些是线程共享的？ 程序计数器都有哪些作用？ java虚拟机栈有哪些作用？ 本地方法栈和虚拟机栈的区别？ 详细介绍一下java 堆？ 新生代如何晋升到老年代的？ 2. 垃圾回收篇 JVM 是如何进行垃圾回收的？ 我们如何来判断对象已经无效？ 如何对一个对象不可达，那么他一定非死不可吗？ 介绍一下java 的四种引用？ 垃圾收集都有哪些算法？各自的特点是什么？ 为什么要分新生代和老年代? 介绍一下CMS,G1收集器 3. 类加载篇 说说Java类加载过程的？ 类加载中的加载步骤是做什么的？ 有哪几种类加载器，他们的职责是什么？ 介绍一下JAVA 的双亲委派模型？ 双亲委派模型有什么好处？ 如果我们不想使用双亲委派模型怎么办？ 为什么Tomcat 容器多个应用之间，为什么能依赖同一个第三方类库的不同版本？ 4. JVM故障排查与调优 你有对JVM 做过性能调优吗？怎么做的？ 你有过线上JVM故障排查经验吗？你都是怎么做的？ "},"J2EE/":{"url":"J2EE/","title":"J2EE","keywords":"","body":"J2EEJ2EE Servlet 转发和重定向 会话跟踪 会话机制 Cookie Session Cookie和Session总结 Tomcat Tomcat系统架构 "},"J2EE/Servlet/":{"url":"J2EE/Servlet/","title":"Servlet","keywords":"","body":"Servlet1.简介2.Servlet 接口3. Servlet 生命周期特点4. Servlet 与线程安全4.1 解决办法Servlet 1.简介 在Java Web程序中，Servlet 主要负责接收用户请求HttpServletRequest,在doGet()，doPost() 中做相应的处理，并将回应HttpServletResponse反馈给用户 一个Servlet类只会有一个实例，在他初始化时调用init方法，销毁时调用destory()方法 Servlet需要在web.xml中配置。一个servlet可以设置多个url访问 servlet 不是线程安全，因此谨慎使用类变量 2.Servlet 接口 Servlet 接口定义了5个方法，其中init,service,destory 与Servlet 的周明周期有关 public interface Servlet { void init(ServletConfig config) throws ServletException; void service(ServletRequest req, ServletResponse resp) throws ServletException, IOException; void destroy(); String getServletInfo(); ServletConfig getServletConfig(); } 3. Servlet 生命周期 Web容器加载Servlet 并将其实例化后，Servlet 生命周期开始 init()：容器运行期init()方法进行Servlet的初始化 初始化资源 service()： 请求到达时调用Servlet 的service() 方法,service() 方法会根据需要调用与请求对象的doGet 或doPost等方法 destory()： 当服务器关闭或项目被卸载时服务器将Servlet 实例销毁，此时会调用Servlet 的destory() 销毁资源 特点 其中init() 和destory（）方法只会执行一次， service 方法客户端每次请求Servlet 都会执行 4. Servlet 与线程安全 Servlet 不是线程安全的，多线程并发的读写会导致数据不同步的问题 4.1 解决办法 尽量不要定义name属性（成员变量），而是要把name变量分别定义在doGet()和doPost（）方法内。 synchronized(name){} 语句块可以解决问题，但会造成线程等待，不是很科学 注：写数据才会导致线程安全，数据不同步的问题。因此Servlet 里的只读属性最好定义成final类型 "},"J2EE/转发和重定向.html":{"url":"J2EE/转发和重定向.html","title":"转发和重定向","keywords":"","body":"转发和重定向1. 转发和重定向实现方式1.1 转发(Forward)1.2 重定向（Redirect）2. 转发(Forward) 和重定向(Redirect) 的区别转发和重定向 1. 转发和重定向实现方式 1.1 转发(Forward) 通过RequestDispatcher 对象的forward（HttpServletRequest request,HttpServletResponse response）方法实现的。 RequestDispatcher可以通过HttpServletRequest 的getRequestDispatcher()方法获得 案例：转发到login_success.jsp request.getRequestDispatcher(\"login_success.jsp\").forward(request, response); 1.2 重定向（Redirect） 利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。 服务器通过HttpServletResponse 的setStatus(int status)方法设置状态码，如果服务器返回301或者302,则浏览器会到新的网址重新请求该资源 2. 转发(Forward) 和重定向(Redirect) 的区别 转发是服务端行为，重定向是客户端行为 从地址栏显示来说 forward 是服务器请求资源， ​ 服务器直接访问目标地址的URL,吧那个url的响应内容读取过来，然后把这些内容再发给浏览器。 ​ 浏览器根本不知道服务器发送的内容哪里来的，所以他的地址栏还是原来的地址。 redirect 是服务端根据逻辑，发送状态码 redirect是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL. 从数据共享上说 forward： 转发页面和转发到页面可以共享request里面的数据 redirect: 不能共享数据 从运用地方来说 forward： 一般用于登录的时候。根据角色转发到响应的模块 redirect: 一般用于用户注销登陆时返回主页面和跳转到其他网站等 从效率上说 forward：高 redirect；低 "},"J2EE/会话/":{"url":"J2EE/会话/","title":"会话跟踪","keywords":"","body":"会话跟踪1. 什么是会话2. 什么是会话追踪3. 为什么需要会话跟踪4. 四种会话跟踪技术5.Session和Cookie区别6.应用场景参考会话跟踪 1. 什么是会话 会话是指一个终端用户（服务器）与交互系统（客户端）进行通讯的过程 2. 什么是会话追踪 对同一个用户对服务器的连续的请求和接受响应的监视。（将用户与同一用户发出的不同请求之间关联，为了数据共享） 3. 为什么需要会话跟踪 浏览器与服务器之间的通信是通过HTTP协议进行通信的，而http协议是”无状态“的协议，他不能保存客户的信息，既一次响应完成之后连接就断开了。下一次的请求需要重新连接，这就需要判断是否是同一个用户，所以才会有会话跟踪技术来实现这种要求 4. 四种会话跟踪技术 URL重写 URL(统一资源定位符)是web 上特定页面的地址，url地址重写的原理是将改用户Session的id信息重写到URL地址中，以便在服务器端进行识别不同的用户。 优势 URL重写能够在客户端停用cookies或者不支持cookies 的时候仍然发挥作用 隐藏表单域 将会话ID添加到html表单元素中提交到服务器，此表单元素并不在客户端显示，浏览时看不到，源代码中有 Cookie Cookie是Web服务器发送给客户端的一小段信息，客户端请求时可以读取该信息发送到服务端，进而进行用户的识别。对于客户端的每次请求，服务端都会将Cookie发送到客户端，在客户端可以进行保存以便下次使用。 缺点：服务器创建保存与浏览器端，不可跨域名行，大小及数量有限 保存方法 保存在客户端内存中，称为临时Cookie，浏览器关闭后，这个Cookie对象将消息 保存在客户机的磁盘上，称为永久Cookie，以后可会断只要访问该网站，就会将Cookie再次发送到服务器上（前期这个Cookie在有效期内） session 每一个用户都有一个不同的session，各个用户之间是不能共享的，每个用户所独享的，在session中可以存放信息 特点：保存在服务端，需要解决多态服务器共享问题。如果session内容过于复杂，当大量客户访问服务器时可能会导致内存溢出，因此，session里的信息应该尽量精简 当服务端会创建一个session对象，产生一个sessionID来标识这个session对象，然后将这个sessionID放入到Cookie中发送到服务端，下一次访问时，sessionID会发送到服务器，在服务器进行识别不同用户 Session是依赖Cookie的，如果cookie被禁用，那么session也将失效 5.Session和Cookie区别 cookie数据存放在在客户的浏览器上，session数据放在服务器上 cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session session会在一定时间内保存在服务器上，当访问增多，会比较占服务器性能，考虑到减轻服务器性能方面，应当使用cookie 单个cookie保存的数据不能不能超过4k，很多浏览器都限制一个站点最多保存20个cookie 6.应用场景 cookie： 记住账号密码，购物车 session 私人信息及登录验证信息在session 参考 会话跟踪技术的四种实现方法及特点整理Cookie "},"J2EE/会话/会话机制.html":{"url":"J2EE/会话/会话机制.html","title":"会话机制","keywords":"","body":"会话机制Cookie和Session之间的区别和联系(例子)会话机制 Web程序中常用的技术，用来跟踪用户的整个会话。常用的会话跟踪技术是Cookie与Session。Cookie通过在客户端记录信息确定用户身份，Session通过在服务器端记录信息确定用户身份。 Cookie和Session之间的区别和联系(例子) 假如一个咖啡店有喝5杯咖啡免费赠一杯咖啡的优惠，然而一次性消费5杯咖啡的机会微乎其微，这时就需要某种方式来纪录某位顾客的消费数量。想象一下其实也无外乎下面的几种方案： 该店的店员很厉害，能记住每位顾客的消费数量，只要顾客一走进咖啡店，店员就知道该怎么对待了。这种做法就是协议本身支持状态。但是http协议本身是无状态的 发给顾客一张卡片，上面记录着消费的数量，一般还有个有效期限。每次消费时，如果顾客出示这张卡片，则此次消费就会与以前或以后的消费相联系起来。这种做法就是在客户端保持状态。也就是cookie。 顾客就相当于浏览器，cookie如何工作，下面会详细讲解 发给顾客一张会员卡，除了卡号之外什么信息也不纪录，每次消费时，如果顾客出示该卡片，则店员在店里的纪录本上找到这个卡号对应的纪录添加一些消费信息。这种做法就是在服务器端保持状态。 由于HTTP协议是无状态的，而出于种种考虑也不希望使之成为有状态的，因此，后面两种方案就成为现实的选择。具体来说cookie机制采用的是在客户端保持状态的方案，而session机制采用的是在服务器端保持状态的方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的，但实际上它还有其他选择 "},"J2EE/会话/Cookie.html":{"url":"J2EE/会话/Cookie.html","title":"Cookie","keywords":"","body":"Cookie1. 奶茶买五送一的例子中，Cookie如何识别用户？2.如何创建Cookie3. 工作原理4.Cookie有效期5. Cookie使用范围5、总结Cookie：6.cookie案例Cookie 1. 奶茶买五送一的例子中，Cookie如何识别用户？ Cookie 相当于给用户发一张积分卡篇，每次买奶茶都盖一个章。 这种做法就是客户端保存状态 2.如何创建Cookie 延续我们的奶茶例子就是，如何发积分卡，填写积分卡内容，并发送给用户 //由服务器进行创建，也就相当于咖啡店来创建会员卡，在创建会员卡的同时，就会将会员卡中的内容也给设置了 Cookie cookie = new Cookie(key,value);　　//以键值对的方式存放内容， response.addCookie(cookie);　　//发送回浏览器端 //注意：一旦cookie创建好了，就不能在往其中增加别的键值对，但是可以修改其中的内容， cookie.setValue();　　//将key对应的value值修改 3. 工作原理 客户如何使用积分卡，cookie在客户端是如何工作的，工作原理是什么？ 这个过程就相当于，奶茶店创建好了会员卡，并且已经设置了其中的内容，交到了客户手中，下次客户过来时，就带着积分卡过来，就知道你是会员了，然后奶茶店就拿到你的会员卡对其进行操作。 4.Cookie有效期 积分卡的有效日期？也就是cookie也是拥有有效日期的。 这个可以自由设置，默认是关闭浏览器，cookie就没用了。 cookie.setMaxAge(expiry);　　//设置cookie被浏览器保存的时间。 expiry：单位秒，默认为-1， expiry=-1：代表浏览器关闭后，也就是会话结束后，cookie就失效了，也就没有了。 expiry>0：代表浏览器关闭后，cookie不会失效，仍然存在。并且会将cookie保存到硬盘中，直到设置时间过期才会被浏览器自动删除， expiry=0：删除cookie。不管是之前的expiry=-1还是expiry>0，当设置expiry=0时，cookie都会被浏览器给删除。 5. Cookie使用范围 设置服务器端获取cookie的访问路径，而并非在服务器端的web项目中所有的servlet都能访问该cookie。 cookie默认路径：当前访问的servlet父路径。 例如：http://localhost:8080/test01/a/b/c/SendCookieServlet 默认路径：/test01/a/b/c　　也就是说，在该默认路径下的所有Servlet都能够获取到cookie，/test01/a/b/c/MyServlet　这个MyServlet就能获取到cookie。 修改cookie的访问路径 setPath(\"/\")；　　//在该服务器下，任何项目，任何位置都能获取到cookie， 通途：保证在tomcat下所有的web项目可以共享相同的cookie 例如：tieba , wenku , beike 多个项目共享数据。例如用户名。 setPath(\"/test01/\");　　//在test01项目下任何位置都能获取到cookie。 5、总结Cookie： 工作流程： servlet创建cookie，保存少量数据，发送浏览器。 浏览器获得服务器发送的cookie数据，将自动的保存到浏览器端。 下次访问时，浏览器将自动携带cookie数据发送给服务器。 cookie操作 创建cookie：new Cookie(name,value) 发送cookie到浏览器：HttpServletResponse.addCookie(Cookie) servlet接收cookie：HttpServletRequest.getCookies() 浏览器发送的所有cookie cookie特点 每一个cookie文件大小：4kb ， 如果超过4kb浏览器不识别 一个web站点（web项目）：发送20个 一个浏览器保存总大小：300个 cookie 不安全，可能泄露用户信息。浏览器支持禁用cookie操作。 默认情况生命周期：与浏览器会话一样，当浏览器关闭时cookie销毁的。---临时cookie 6.cookie案例 记住用户名 　登录时，在服务器端获取到用户名，然后创建一个cookie，将用户名存入cookie中，发送回浏览器端，然后浏览器下次在访问登录页面时，先拿到cookie，将cookie中的信息拿出来，看是否保存了该用户名，如果保存了，那么直接用他，如果没有，则自己手写用户名。 历史记录 比如购物网站，都会有我们的浏览记录的，实现原理其实也是用cookie技术，每浏览一个商品，就将其存入cookie中，到需要显示浏览记录时，只需要将cookie拿出来遍历即可。　　 "},"J2EE/会话/Session.html":{"url":"J2EE/会话/Session.html","title":"Session","keywords":"","body":"Session1. 奶茶买五送一的例子中，Session如何识别用户？2. session原理分析：3. 获取session：4. session属性操作：5. session生命周期5.1 错误思想：只要关闭浏览器，session就消失了5.2 控制session的有效时间5.3 session的生命周期6. session id的URL重写6.1 解决方案：URL重写Session 1. 奶茶买五送一的例子中，Session如何识别用户？ Session 相当于办了一张会员卡来确定你是哪个用户。 店铺给顾客办理一张会员卡，除了卡号什么信息都不记录 每次消费，如果顾客出示会员卡，店员记录找到这个卡号对应记录添加一些消费信息 这种做法就是在服务器端保持状态。 这就是session的用法，在服务器端来保持状态，保存一些用户信息。 功能作用：服务器用于共享数据技术， 2. session原理分析： 浏览器请求服务器访问web站点时， 首先会检查是否创建过session 程序需要为客户端的请求创建一个session的时候，服务器首先会检查这个客户端请求是否已经包含了一个session标识、称为SESSIONID， 如果已经包含sessionId，则检索出原来的seesion 如果客户端请求不包含session id，创建一个session并且生成一个与此session相关联的session id sessionid将在本次响应中返回到客户端保存 保存方式: 名字为JSESSIONID的cookie中 3. 获取session： request.getSession();　　 如果没有将创建一个新的，等效getSession(true); PS:为什么是从request中获取呢？因为在获取session时，需要检查是否有session表示 request.getSession(boolean);　　 true：没有将创建，false：没有将返回null 4. session属性操作： xxxAttribute(...) 　　用来存放一些信息，然后才能共享信息　 setAttrubute(key,value); getAttribute(key); 5. session生命周期 5.1 错误思想：只要关闭浏览器，session就消失了 常常听到这样一种误解“只要关闭浏览器，session就消失了”。其实可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。 对session来说也是一样的，除非程序通知服务器删除一个session，否则服务器会一直保留，程序一般都是在用户做log off的时候发个指令去删除session。然而浏览器从来不会主动在关闭之前通知服务器它将要关闭，因此服务器根本不会有机会知道浏览器已经关闭， 之所以会有这种错觉，是大部分session机制都使用会话cookie来保存session id，而关闭浏览器后这个session id就消失了，再次连接服务器时也就无法找到原来的session。如果服务器设置的cookie被保存到硬盘上，或者使用某种手段改写浏览器发出的HTTP请求头，把原来的session id发送给服务器，则再次打开浏览器仍然能够找到原来的session　 恰恰是由于关闭浏览器不会导致session被删除，迫使服务器为seesion设置了一个失效时间，一般是30分钟，当距离客户端上一次使用session的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把session删除以节省存储空间 5.2 控制session的有效时间 session.invalidate() 将session对象销毁 setMaxInactiveInterval(int interval) 设置有效时间，单位秒 在web.xml中配置session的有效时间 　 30 单位：分钟 5.3 session的生命周期 创建：第一次调用getSession() 销毁： 超时，默认30分钟 执行api：session.invalidate()将session对象销毁、setMaxInactiveInterval(int interval) 设置有效时间，单位秒 服务器非正常关闭 自杀，直接将JVM马上关闭 如果正常关闭，session就会被持久化(写入到文件中，因为session默认的超时时间为30分钟，正常关闭后，就会将session持久化，等30分钟后，就会被删除) 位置：　D:\\java\\tomcat\\apache-tomcat-7.0.53\\work\\Catalina\\localhost\\test01\\SESSIONS.ser 6. session id的URL重写 当浏览器将cookie禁用，基于cookie的session将不能正常工作，每次使用request.getSession() 都将创建一个新的session。达不到session共享数据的目的， 但是我们知道原理，只需要将session id 传递给服务器session就可以正常工作的。 6.1 解决方案：URL重写 通过URL将session id 传递给服务器：URL重写 手动方式： url;jsessionid=.... api方式： 如果浏览器禁用cooke，api将自动追加session id ，如果没有禁用，api将不进行任何修改。 response.encodeURL(java.lang.String url) 进行所有URL重写 　　response.encodeRedirectURL(java.lang.String url) 进行重定向 URL重写　 ​ 这两个用法基本一致,只不过考虑特殊情况,要访问的链接可能会被Redirect到其他servlet去进行处理,这样你用上述方法带来的session的id信息不能被同时传送到其他servlet.这时候用encodeRedirectURL()方法就可以了　 　　　　　　　　　　 注意：如果浏览器禁用cookie，web项目的所有url都需进行重写。否则session将不能正常工作 当禁止了cookie时， 　　　　　　　　　　　 "},"J2EE/会话/Cookie和Session总结.html":{"url":"J2EE/会话/Cookie和Session总结.html","title":"Cookie和Session总结","keywords":"","body":"Cookie和Session总结1. 什么是cookie和什么是session？2. cookie的工作原理？session的工作原理？Cookie和Session总结 1. 什么是cookie和什么是session？ cookie是一种在客户端记录用户信息的技术 因为http协议是无状态的，为了解决这个问题而产生了cookie。记录用户名等一些应用 session是一种在服务端记录用户信息的技术 一般session用来在服务器端共享数据， 2. cookie的工作原理？session的工作原理？ cookie工作原理 cookie是由服务器端创建发送回浏览器端的 并且每次请求服务器都会将cookie带过去，以便服务器知道该用户是哪一个。 其cookie中是使用键值对来存储信息的，并且一个cookie只能存储一个键值对。 所以在获取cookie时，是会获取到所有的cookie，然后从其中遍历。 session的工作原理 就是依靠cookie来做支撑， 第一次使用request.getSession()时session被创建， 并且会为该session创建一个独一无二的sessionid存放到cookie中， 然后发送会浏览器端，浏览器端每次请求时，都会带着这个sessionid， 服务器就会认识该sessionid，知道了sessionid就找得到哪个session。 以此来达到共享数据的目的。 这里需要注意的是，session不会随着浏览器的关闭而死亡，而是等待超时时间。 "},"J2EE/tomcat/":{"url":"J2EE/tomcat/","title":"Tomcat","keywords":"","body":"TomcatTomcat "},"J2EE/tomcat/Tomcat系统架构.html":{"url":"J2EE/tomcat/Tomcat系统架构.html","title":"Tomcat系统架构","keywords":"","body":"Tomcat系统架构1. 学习目的2. Tomcat 顶层架构2.1 tomcat结构3. server.xml配置文件4. Tomcat顶层架构小结5.Connector和Container的关系5.1 Tomcat网络请求流程5.2 Connector5.3 Container架构分析5.4 Container 如何处理请求Tomcat系统架构 1. 学习目的 Server、Service、Connector、Container四大组件之间的关系和联系，以及他们的主要功能点 Tomcat执行的整体架构、请求是如何被一步步处理的 Engine、Host、Context、Wrapper相关的概念关系 Container是如何处理请求的 Tomcat用到的相关设计模式 2. Tomcat 顶层架构 Tomcat 顶层架构图，如下 Service Tomcat 中最顶层的容器是Server，代表着整个服务器 一个Server可以包含至少一个Service，用于具体提供服务。 Service 主要组成（Tomcat的心脏的两个组件） Connector 用于处理连接相关的事情，并提供Socket与Request和Response相关的转化 Container 用于封装和管理Servlet，以及具体处理Request请求 2.1 tomcat结构 一个Tomcat中只有一个Server， 一个Server可以包含多个Service， 一个Service只有一个Container， 但是可以有多个Connectors，这是因为一个服务可以有多个连接。 如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接，示意图如下： 多个Connector 和一个Container 就形成一个Service，有了Service就可以对外提供服务了，但是还要一个生存环境，必须要有人能够给他生命，掌握其生死大权，那就非Server莫属了！所以整个Tomcat有Server控制 3. server.xml配置文件 上述关系都可以从server.xml看出 配置文件对应的结构图 结构图解析 Server标签设置的端口为8005，shutdown=“SHUTDOWN\", 表示在8005端口监听”SHUTDOWN“命令，如果收到了就会关闭Tomcat Container Service左边的内容都是属于Container（Engine等） Service Service下边是Connector 4. Tomcat顶层架构小结 Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container； Server掌管着整个Tomcat的生死大权； Service 是对外提供服务的； Connector用于接受请求并将请求封装成Request和Response来具体处理； Container用于封装和管理Servlet，以及具体处理request请求； 5.Connector和Container的关系 5.1 Tomcat网络请求流程 一个请求发送到Tomcat之后 首先经过Service然后交给Connector Connector 用于接受请求并将接收到的请求封装为Request和Response来具体处理，request和response封装完之后再交由Container进行处理 Container处理完请求之后再返回给Connector 最后再由Connector通过Socket将处理的结果返回给客户端 5.2 Connector Connector 最底层使用的事Socket来进行封装的，Request和Response 是按照Http协议来封装的，所以Connector同时需要实现TCP/IP协议和Http协议 5.3 Container架构分析 Container 用于封装和管理Servlet，以及具体处理Request请求，在Connector内部包含了4个自容器 5.3.1 4个子容器的作用 Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine Host: 代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点 Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面web.xml文件 Wrapper：每一个Wrapper封装这一个Servlet tomcat的文件目录对照，如下图 Context和Host的区别是Context表示一个应用，我们的tomcat中默认的配置下webapps下的每一个文件夹目录是一个Context，其中ROOT目录存放着主应用，其他目录存放着子应用，而珍格格webapps就是一个Host站点 我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：www.isture.com，如果是Host（webapps）下的其他应用，则可以使用www.isture.com/docs进行访问，当然默认指定的根应用ROOT可以进行设置， 5.4 Container 如何处理请求 Container处理请求是使用Pipeline-Valve管道来处理的！（Valve是阀门之意） Pipeline-Valve是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的请求返回，再让下一个处理着继续处理。 "},"spring/":{"url":"spring/","title":"Spring","keywords":"","body":"SpringSpring Spring常见知识点 SpringAOP SpringAOP概述 Spring AOP实现原理 Spring Boot常见知识点 "},"spring/interview/":{"url":"spring/interview/","title":"Spring常见知识点","keywords":"","body":"Spring常见知识点1. 什么是Spring框架2. 列举一些重要的Spring模块？3. @RestController 和@Controller3.1 Controller 返回一个页面3.2 @RestController 返回JSON 或XML 形式数据3.3 @Controller+@Responsebody 返回Json或xml形式数据4. Spring IOC4.1 谈谈自己对Spring IoC 的理解4.2 Spring IoC的初始化过程4.3 IoC容器有几种类型4.4 BeanFactory和ApplicationContext有什么区别？5. AOP5.1 谈谈自己对Spring AOP 的理解5.2 Spring AOP 和 AspectJ AOP 有什么区别6. Spring Bean6.1 Spring 中 bean 的作用域有哪些？6.2 Spring 中的单例 bean 的线程安全问题6.3 @Component 和 @Bean 的区别是什么6.4 将一个类声明为Spring 的bean的注解有哪些6.5 Spring 中的bean 生命周期？7. Spring MVC7.1 说说自己对于Spring MVC 了解7.2 SpringMVC 工作原理了解吗8. Spring 框架中用到哪些设计模式9. Spring 事务9.1 Spring 管理事务的方式几种？9.2 Spring 事务中的隔离级别有哪几种8.3 Spring 事务中哪几种事务传播行为?9.4 @Transactional( rollbackFor = Exception.class) 注解10. JPA9.1 如何使用JPA 在数据库中非持久化一个字段Spring常见知识点 1. 什么是Spring框架 Spring 是一种轻量级开发框架，旨在提高开发人员的开发效率以及系统的可维护性 我们一般说的Spring框架指的都是Spring Framwork，他是很多模块的集合，使用这些模块可以很方便地协助我们进行开发： 主要涉及的模块： 核心容器 数据访问/集成 Web AOP(面向切面编程) 工具 消息 测试模块 Spring官网列出的Spring的6个特性 核心技术：依赖注入（DI）,AOP, 事件（events）,资源，i18n,验证，数据绑定，类型转换，SpEL 测试 ：模拟对象，TestContext框架，Spring MVC 测试，WebTestClient。 数据访问 ：事务，DAO支持，JDBC，ORM，编组XML。 Web支持 : Spring MVC和Spring WebFlux Web框架。 集成 ：远程处理，JMS，JCA，JMX，电子邮件，任务，调度，缓存。 语言 ：Kotlin，Groovy，动态语言。 2. 列举一些重要的Spring模块？ 下图对应的是Spring4.X 版本，目前最新的5.X版本中Web模块的Portlet组件已经被废弃掉，同时增加了用于异步响应式处理的WebFlux 组件 Spring Core: 基础，可以说 Spring 其他所有的功能都需要依赖于该类库，主要提供Ioc 依赖注入功能 Spring Aspects: 该模块为与AspectJ 的集成提供支持 Spring AOP: 提供了面向切面的编程实现 Spring JDBC: java数据库连接 Spring JMS: java消息服务 Spring ORM: 用于支持Hibernate 等ORM工具 Spring Web:为创建Web应用程序提供支持 Spring Test: 提供了对Junit 和TestNG 测试的支持 3. @RestController 和@Controller 3.1 Controller 返回一个页面 单独使用@Controller 不加@ResponseBody 的话一般使用在要返回一个视图的情况，这种情况属于比较传统的Spring MVC 的应用，对应于前后端不分离的情况 3.2 @RestController 返回JSON 或XML 形式数据 @RestController 只返回对象，对象数据直接以 JSON 或 XML 形式写入HTTP 响应（Response）中，这种情况属于RESTful Web服务，这也是目前开发所接触的最常用的情况（前后端分离） 3.3 @Controller+@Responsebody 返回Json或xml形式数据 如果你需要在Spring4之前开发 RESTful Web服务的话，你需要使用@Controller 并结合@ResponseBody注解，也就是说@Controller +@ResponseBody= @RestController（Spring 4 之后新加的注解）。 @ResponseBody 注解的作用是将 Controller 的方法返回的对象通过适当的转换器转换为指定的格式之后，写入到HTTP 响应(Response)对象的 body 中，通常用来返回 JSON 或者 XML 数据，返回 JSON 数据的情况比较多。 4. Spring IOC 4.1 谈谈自己对Spring IoC 的理解 Ioc（Inverse of Control:控制反转）是一种设计思想，就是将原本在程序中手动创建对象的控制权，交由Spring框架来管理。Ioc 在其他语言中也有应用，并非Spring特有。IoC 容器是Spring 用来实现 Ioc的载体，IoC 容器实际上就是个Map,Map中存放的是各种对象 将对象之间的相互依赖关系交给IoC容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度简化应用的开发，把应用从复杂的依赖关系中解放出来。IoC 容器就想一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象如何被创建出来。在实际项目中一个Service 类可能有几百甚至上千个类作为他的底层，加入我们需要实例化这个Service。你可能每次都要搞清这个Service 所有底层类构造函数，这可能会把人逼疯。如果我们利用IoC的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度 Spring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 SpringBoot 注解配置就慢慢开始流行起来。 4.2 Spring IoC的初始化过程 4.3 IoC容器有几种类型 BeanFactory ApplicationContext 4.4 BeanFactory和ApplicationContext有什么区别？ beanfactory是基本容器，而applicationcontext是高级容器。Applicationcontext是扩展了beanfactory的接口。Applicationcontext比beanfactory提供了更多东西，比如跟aop的集成, 消息资源处理等等。 5. AOP 5.1 谈谈自己对Spring AOP 的理解 AOP(Aspect-Oriented Programming: 面向切面编程)：能够分离系统的业务逻辑和系统服务（例如，事务处理，日志管理，权限管理），便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可扩展性和可维护性 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP 会使用 JDK Proxy, 去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 Cglib 生成一个被代理对象的子类来作为代理 当然你也可以使用AspectJ ，Spring AOP 已经集成了AspectJ,AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 5.2 Spring AOP 和 AspectJ AOP 有什么区别 Spring AOP 属于运行时增强，而AspectJ 是编译时增强。 Spring AOP基于代理（Proxying），而AspectJ 基于字节码操作（Bytecode Manipulation） Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单 如果切面比较少，两者性能差异不大。但是，当切面太多的话，最好选择AspectJ ,他比Spring AOP 快很多 6. Spring Bean 6.1 Spring 中 bean 的作用域有哪些？ singleton: 唯一 bean 实例，Spring 中 bean 默认都是单例的 prototype: 每次请求都会创建一个新的 bean 实例 request：每一次HTTP请求都会产生一个新的bean，该bean仅在当前的Http request 内有效 session：每一次Http请求都会产生一个新的bean，该bean 仅在当前 HTTP session 内有效 global-session：全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。 Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 6.2 Spring 中的单例 bean 的线程安全问题 单例bean 存在线程安全问题，主要是因为当多个线程操作同一个对象的时候，对这个对象的非静态成员变量的写操作可存在线程安全问题 常见的两种解决办法： 在Bean 对象中尽量避免定义可变的成员变量（不太现实） 在类中定义一个ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式） 6.3 @Component 和 @Bean 的区别是什么 作用对象不同：@Component注解作用于类，而@Bean注解作用于方法 @Component 通常是通过类路径扫描来自动侦测以及自动装配到Spring 容器中（我们可以使用@ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个bean。@Bean告诉Spring 这是某个类的示例，当我们需要用它的时候还给我 @Bean 注解比@Component 注解的自定义性更强，而且很多地方我们只能通过@Bean注解来注册bean，比如当我们引用第三方库中的类需要装配到Spring 容器时，则只能通过 @Bean 来实现 @Bean 注解使用实例 @Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 上面的代码相当于下面的 xml 配置 下面这个例子是通过@Component 无法实现的 @Bean public OneService getService(status) { case (status) { when 1: return new serviceImpl1(); when 2: return new serviceImpl2(); when 3: return new serviceImpl3(); } } 6.4 将一个类声明为Spring 的bean的注解有哪些 我们一般使用 @Autowired 注解自动装配 bean，要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,采用以下注解可实现： @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个Bean不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao层。 @Controller : 对应 Spring MVC 控制层，主要接收用户请求并调用 Service 层返回数据给前端页面。 6.5 Spring 中的bean 生命周期？ 生命周期大体就是 初始化构造函数 设置对应属性 判断是否实现了各个接口 若实现BeanNameAware接口，则调用setBeanName（）方法 若实现BeanFactoryAware接口，则调用setBeanFactory()方法 若实现ApplicationContextAware接口，则调用setApplicationContext()方法 若实现BeanPostProcessor接口，则调用postProcessBeforeInitialization() 初始化接口方法 若实现InitializingBean接口，则调用afterPropertiesSet()方法 init-method 声明了初始化方法，该方法也会被调用 若实现BeanpostProcessor接口，则调用postProcessorAfterInitialization()方法 bean准备就绪，一直驻留在应用上下文，直到上下文销毁 上下文销毁，额外处理 若实现DisposableBean接口，调用destory方法 若 destroy-method 申明了销毁方法，调用销毁方法 图示 对应的中文版 7. Spring MVC 7.1 说说自己对于Spring MVC 了解 MVC 是一种设计模式，Spring MVC 是一款很优秀的MVC 框架，Spring MVC 可以帮助我们进行更简洁的Web 层的开发，并且他天生与Spring 框架集成。 Spring MVC 下我们一般把后端项目分为 Controller 层（控制层，返回数据给前台页面） Service 层（处理业务）、 Dao 层(数据库操作) Entity层（实体类） Spring MVC的简单原理 7.2 SpringMVC 工作原理了解吗 Spring MVC 的入口函数也就是前端控制器 DispatcherServlet 的作用是接收请求，响应结果。 流程说明（重要） 客户端（浏览器）发送请求，直接请求到DispatcherServlet DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler 解析到对应的 Handler（也就是Controller控制器）后，开始由HandlerAdapter 适配器处理 HandlerAdapter会根据 Hander 来调用真正的处理器处理请求，并处理相应的业务逻辑 处理器处理完业务后，会返回一个ModelAndView对象，Model 是返回的数据对象，View 是个逻辑上的View ViewResolver 会根据逻辑View 查找实际的View DispaterServlet 把返回的 model 传给View （视图渲染） 把view 返回给请求者（浏览器） 8. Spring 框架中用到哪些设计模式 工厂设计模式：Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建bean 对象 代理设计模式：Spring AOP 功能的实现 单例设计模式：Spring 中的Bean 默认都是单例的 模板方法模式：Spring 中的jdbcTemplate、hibernateTemplate等以template 结尾的对数据库操作的类，他们都使用到了模板模式 包装器设计模式：我们需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库，这种模式让我们可以根据客户的需求能够动态切换不同的数据源 观察者模式：Spring 事件驱动模型就是观察者模式很经典的一个应用 适配器模式：Spring AOP 的增强或通知（Advice）使用到了适配器模式、Spring MVC 中也是用到了适配器 模式适配controller …... 9. Spring 事务 9.1 Spring 管理事务的方式几种？ 编程式事务，在代码中硬编码（不推荐使用） 声明式事务，在配置文件中配置（推荐使用） 声明式事务又分为两种 基于XML 的声明式事务 基于注解的声明式事务 9.2 Spring 事务中的隔离级别有哪几种 TransactionDefinition 接口中定义了五个表示隔离级别的常量 TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用REPEATABLE_READ 隔离级别，Oracle默认采用READ_COMMITTED 隔离级别 TransactionDefintion.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 8.3 Spring 事务中哪几种事务传播行为? 支持当前事务的情况 TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况 TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 9.4 @Transactional( rollbackFor = Exception.class) 注解 我们知道：Exception 分为运行时异常RuntimeException和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，他也可以保证数据的一致性 当@Transactional 注解作用于类上时，该类的所有public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义，如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会滚回 在@Transactional 注解中如果不配置rollbackFor 属性，那么事务只会在遇到 RuntimeException 的时候才会回滚，加上rollbackFor=Exception.class,可以让事务在遇到非运行时异常时也回滚 10. JPA 9.1 如何使用JPA 在数据库中非持久化一个字段 假如我们有下面一个类： Entity(name=\"USER\") public class User { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Column(name = \"ID\") private Long id; @Column(name=\"USER_NAME\") private String userName; @Column(name=\"PASSWORD\") private String password; private String secrect; } 如果我们想让secrect 这个字段不被持久化，也就是不被数据库存储怎么办？我们可以采用下面几种方法： static String transient1; // not persistent because of static final String transient2 = “Satish”; // not persistent because of final transient String transient3; // not persistent because of transient @Transient String transient4; // not persistent because of @Transient 一般使用后面两种方式比较多，我个人使用注解的方式比较多。 "},"spring/Framework/Spring整体架构.html":{"url":"spring/Framework/Spring整体架构.html","title":"Spring整体架构","keywords":"","body":"Spring整体架构1. 什么是 Spring Framework？2. Spring Framework 中有多少个模块，他们分别是什么？3. 使用 Spring 框架能带来哪些好处？3.1 Spring 框架带来的缺点4. Spring 框架中都用到了哪些设计模式？Spring整体架构 1. 什么是 Spring Framework？ Spring 是一个开源应用框架，旨在降低应用程序开发的复杂度 它是轻量级、松散耦合的 轻量级相对于EJB，随着Spring 的体系越来越庞大，大家被Spring 的配置搞懵了，所以后来出了Spring Boot 它具有分层体系架构，允许用户选择组件，同时还为J2EE 应用程序开发提供了一个有凝聚力的框架 他可以集成其他框架，如Spring MVC,Hibernate,MyBatis等，所以又称为框架的框架（粘合剂，脚手架） 2. Spring Framework 中有多少个模块，他们分别是什么？ Spring Framework 的模块图 Spring 核心容器（图中的Core Container） 该层基本上是Spring Framework 的核心(其实就是 Spring IOC)。它包含以下模块： Spring Core Spring Bean 核心容器提供 Spring 框架的基本功能。核心容器的主要组件是BeanFactory，它是工厂模式的实现，BeanFactory 使用控制反转（IOC）模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。 Spring Context Spring 上下文是一个配置文件，向Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如JNDO,EJB、电子邮件，国际化、事件机制、校验的调度功能 SpEL(Spring Expression Language) 数据访问（图中的Data Access） 该层提供与数据库交互的支持。它包含以下模块： JDBC(Java DataBase Connectivity) Spring 对 JDBC 的封装模块，提供了对关系数据库的访问 ORM(Object Relational Mapping) Spring ORM 模块，提供了对hibernate5 和 JPA 的集成 hibernate5 是一个 ORM 框架 JPA 是一个 Java 持久化 API OXM(Object XML Mappers) Spring 提供了一套类似 ORM 的映射机制，用来将 Java 对象和 XML 文件进行映射。 Transaction Spring 简单而强大的事务管理功能，包含申明式事务和编程式事务 Web 该层提供了创建Web 应用程序的支持。他包含以下模块 WebMVC MVC 框架 死一个全功能的构建Web 应用程序的MVC 实现。通过策略接口，MVC框架变成为高度可配置的，MVC 容纳了大量的视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI WebFlux 基于 Reactive 库的响应式的Web 开发框架 WebSocket Spring 4.0 的一个最大更新是增加了对Websocket 的支持。Websocket 提供了一个在Web 应用中实现高效、双向通讯、需考虑客户端（浏览器）和服务端之间高频和低延时消息交换的机制。一般的应用场景有：在线交易，网页聊天，游戏、协作、数据可视化等 AOP 该层支持面向切面编程，包含以下模块 AOP 通过配置管理特性、Spring AOP 模块直接将面向切面的编程功能集成到了Spring 框架中，所以，可以很容易地使Spring 框架管理的任何对象支持AOP Spring AOP 模块为基于Spring 的应用程序中的对象提供了事务管理服务。通过使用Spring AOP，不用依赖EJB组件们就可以将声明式事务集成到应用程序中 Aspects 该模块为与AspectJ 的继承提供支持 Instrumentation 该层为类检测和类加载器实现提供支持。（PS: 用得比较少） 其他 JMS(Java Message Service) 提供了一个JMS 集成框架，简化了 JMS API 的使用。 Test 该模块为使用JUnit 和TestNG 进行测试提供支持 Messaging 该模块为STOMP 提供支持。他还支持注解编程模型，该模型用于从WebSocket 客户端路由和处理 STOMP消息 3. 使用 Spring 框架能带来哪些好处？ DI: Dependency Injection(DI) 方法，使得构造器和JavaBean、properties 文件中的依赖关系一目了然 轻量级：与EJB 容器相比较，IoC容器更加倾向于轻量级。这样一来 IoC 容器在有限的内存和 CPU 资源的情况下，进行应用程序的开发和发布就变得十分有利 面向切面编程（AOP）: Spring 支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来 集成主流框架：Spring 并没有闭门造车，Spring 集成了已有的技术栈，比如ORM 框架、Logging 日期框架、J2EE、Quartz 和 JDK Timer，以及其他视图技术 模块化：Spring 框架是按照模块的形式来组织的。由包和类的命名，就可以看出其所属的模块，开发者仅仅需要选用他们需要的模块即可。 便捷的测试： Web 框架：Spring 的Web 框架亦是一个精心设计的Web MVC 框架，为开发者们在 Web 框架的选择上提供了一个除了主流框架比如 Struts 、过度设计的、不流行 Web 框架的以外的有力选项。 事务管理：Spring 提供了一个便捷的事务管理接口，适用于小型的本地事务处理（比如在单DB 的环境下）和复杂的共同事务处理（比如利用JTA 的复杂DB环境） 异常处理：Spring 提供一个方便的API,将特定技术的异常（由JDBC，Hibernate，或JDO抛出）转化为一致的、Unchecked异常。 3.1 Spring 框架带来的缺点 每个框架都有可能存在问题，调试阶段不直观，后期的bug 对应阶段、不容易判断问题所在。要花一定时间去理解他 把很多JavaEE 的东西封装了，在满足快速开发高质量程序的同时，隐藏了实现细节 离开Spring 之后，就不知道怎么工作。从 Java 工程师，变成了Spring 工程师 4. Spring 框架中都用到了哪些设计模式？ Spring 框架中使用到了大量的设计模式，下面列举了一些比较有代表性的 代理模式： 在AOP 和 remoting 中被用的比较多 单例模式：在Spring 配置文件中定义的Bean 默认为单例模式 模板方法：用来解决代码重复的问题。比如 RestTemplate、JdbcTemplate 工厂模式：BeanFactory 用来创建对象的实例 前端控制器：Spring 提供了 DispatcherServlet 来对请求进行分发 依赖注入：贯穿于BeanFactory/ApplicationContext 接口的核心理解 "},"spring/ioc/SpringIoC常见问题.html":{"url":"spring/ioc/SpringIoC常见问题.html","title":"Spring IoC常见问题","keywords":"","body":"Spring IoC常见问题1. 概述2. 什么是依赖注入？3. IoC 和 DI 有什么区别？4. 可以通过多少种方式完成依赖注入？4.1 构造函数和setter 注入的优缺点5. Spring 中有多少种IoC 容器5.1 BeanFactory 与 ApplicationContext 的两种差异6. 请介绍下常用的BeanFactory 容器？7.请介绍下常用的 ApplicationContext 容器？8. 列举一些 IoC 的一些好处？9. 简述Spring IoC 的实现机制？10. Spring 框架中有哪些不同类型的事件？10.1 自定义扩展事件Spring IoC常见问题 1. 概述 Spring 框架的核心是Spring IoC 容器。容器创建 Bean 对象，将它们装配在一起，配置它们并管理它们的完整生命周期 Spring 容器使用依赖注入来管理组成应用程序的 Bean 对象。 容器通过读取提供的配置元数据 Bean Definition来接收对象进行实例化，配置和组装的指令 该配置元数据Bean Definition 可以通过XML，Java 注解或Java Config代码提供 2. 什么是依赖注入？ 在依赖注入中，你不必主动，手动创建对象，但必须描述如何创建它们。 你不是直接在代码中将组件和服务连接在一起，而是描述配置文件中哪些组件需要哪些服务 然后，再由IoC容器将他们装配在一起 依赖注入的英文缩写是 Dependency Injection ，简称 DI 。 3. IoC 和 DI 有什么区别？ IoC 是个更宽泛的概念，DI 是更具体的。 4. 可以通过多少种方式完成依赖注入？ 通常，依赖注入可以通过以下三种方式完成 接口注入 构造函数注入 setter 注入 目前，在Spring Framework中，仅使用构造函数和setter 注入这两种方式 4.1 构造函数和setter 注入的优缺点 构造函数注入 setter注入 没有部分注入 有部分注入 不会覆盖setter 属性 会覆盖setter属性 任意修改都会创建一个新的实例 任意修改不会创建一个新实例 适用于设置很多属性 适用于设置少量属性 实际场景下，setter 注入使用的更多 5. Spring 中有多少种IoC 容器 Spring 提供了两种（不是”个“）IoC 容器，分别是BeanFactory、ApplicationContext BeanFactory BeanFactory 在Spring-beans 项目提供 BeanFactory，就像一个包含Bean 集合的工厂类。他会在客户端要求时实例化 Bean 对象。 ApplicationContext ApplicationContext 在 spring-context 项目提供 ApplicationContext接口扩展了BeanFactory接口，他在BeanFactory基础上提供了一些额外的功能。内置如下功能： MesssageSource：管理message，实现国际化等功能 ApplicationEventPublisher：事件发布。 ResourcePatternResolver：多资源加载 EnvironmentCapable：系统Environment（profile+Properties）相关 Lifecycle：管理生命周期 Closable：关闭，释放资源 initalizingBean：自定义初始化 BeanNameAware : 设置beanName的Aware接口 另外，ApplicationContext 会自动初始化非懒加载的Bean 对象们 5.1 BeanFactory 与 ApplicationContext 的两种差异 | BeanFactory | ApplicationContext | | -------------------------- | ----------------------| | 使用懒加载 | 使用即时加载 | | 它使用语法显式提供资源对象 | 它自己创建和管理资源对象 | | 不支持国际化 | 支持国际化 | | 不支持基于依赖的注解 | 支持基于依赖的注解 | 另外、BeanFactory 也被称为低级容器，而ApplicationContext 被称为高级容器 6. 请介绍下常用的BeanFactory 容器？ BeanFactory 最常用的是XmlBeanFactory，它可以根据XML文件中定义的内容，创建相应的Bean 7.请介绍下常用的 ApplicationContext 容器？ 以下是三种较常见的ApplicationContext 实现方式 ClassPathXmlApplicationContext：从ClassPath的XML 配置文件中读取上下文，并生成上下文定义。应用程序上下文从程序环境变量中取得 ApplicationContext context = new ClassPathXmlApplicationContext(“bean.xml”); FileSystemXmlApplicationContext: 由文件系统的XML配置文件读取上下文。 ApplicationContext context = new FileSytemXmlApplicationContext(\"bean.xml\") XmlWebApplicationContext: 由Web 应用的XML文件读取上下文。例如我们在Spring MVC 使用情况 ConfigServletWebServerApplicationContext（Spring Boot）： 目前我们更多使用的是Spring Boot 为主，所以使用的是第四种ApplicationContext容器。ConfigServletWebServerApplicationContext。 8. 列举一些 IoC 的一些好处？ 它将最小化应用程序中的代码 它以最小的影响和最少的侵入机制促进松耦合 它支持即时的实例化和延迟加载Bean对象 它将使您的应用程序易于测试，因为他不需要单元测试用例中的任何单例或JNDI查找机制 9. 简述Spring IoC 的实现机制？ 简单来说，Spring 中的IoC的实现原理，就是工厂模式加反射机制 interface Fruit { public abstract void eat(); } class Apple implements Fruit { public void eat(){ System.out.println(\"Apple\"); } } class Orange implements Fruit { public void eat(){ System.out.println(\"Orange\"); } } class Factory { public static Fruit getInstance(String className) { Fruit f = null; try { f = (Fruit) Class.forName(className).newInstance(); } catch (Exception e) { e.printStackTrace(); } return f; } } class Client { public static void main(String[] args) { Fruit f = Factory.getInstance(\"io.github.dunwu.spring.Apple\"); if(f != null){ f.eat(); } } } Fruit接口，有Apple 和Orange两个实现类 Factory工厂，通过反射机制，创建className 对应的Fruit 对象 Client 通过Factory 工厂，获得对应的Fruit 对象 实际情况下，Spring IoC 比这个复杂很多很多，例如单例Bean 对象，Bean 的属性注入，相互依赖的Bean 的处理 10. Spring 框架中有哪些不同类型的事件？ Spring 的ApplicationContext 提供了支持事件和代码中监听器的功能。 我们可以创建 Bean 用来监听在 ApplicationContext 中发布的事件。如果一个 Bean 实现了 ApplicationListener 接口，当一个ApplicationEvent 被发布以后，Bean 会自动被通知。示例代码如下 public class AllApplicationEventListener implements ApplicationListener { @Override public void onApplicationEvent(ApplicationEvent applicationEvent) { // process event } } Spring 提供了以下五种标准的事件 上下文更新事件（ContextRefreshedEvent）：该事件会在ApplicationContext 被初始化或者更新时发布。也可以在调用ConfigurableApplicationContext 接口中的#refresh（）方法时被触发 上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的#start（）方法开始/重新开始容器时触发该事件。 上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext 的#stop()方法停止容器时触发该事件 上下文关闭事件（ContextCloseEvent）：当ApplicationContext 被关闭时触发该事件。容器被关闭时，其管理的所有单例 Bean 都被销毁 请求处理事件（RequestHandledEvent）：在Web应用中，当一个Http 请求（request）结束触发该事件 10.1 自定义扩展事件 除了以上事件，还可以通过扩展 ApplicationEvent 类来开发自定义的事件 实例自定义的事件的类 public class CustomApplicationEvent extends ApplicationEvent{ public CustomApplicationEvent(Object source, final String msg) { super(source); } } 为了监听这个事件，还需要创建一个监听器 public class CustomEventListener implements ApplicationListener { @Override public void onApplicationEvent(CustomApplicationEvent applicationEvent) { // handle event } } 之后通过ApplicationContext 接口的#publishEvent(Object event)方法，来发布自定义事件 // 创建 CustomApplicationEvent 事件 CustomApplicationEvent customEvent = new CustomApplicationEvent(applicationContext, \"Test message\"); // 发布事件 applicationContext.publishEvent(customEvent); "},"spring/ioc/SpringBean常见问题.html":{"url":"spring/ioc/SpringBean常见问题.html","title":"Spring Bean常见问题","keywords":"","body":"Spring Bean常见问题1. 什么是Spring Bean？2. Spring 有哪些配置方式3. 解释什么叫延迟加载？Spring Bean常见问题 1. 什么是Spring Bean？ Bean 由Spring IoC 容器实例化，配置、装配和管理 Bean 是基于用户提供给IoC 容器的配置元数据Bean Definition 创建 2. Spring 有哪些配置方式 单纯从Spring Framework 提供的方式，一共有三种： XML 配置文件 注解配置 Java Config 配置 3. 解释什么叫延迟加载？ 默认情况下，容器启动之后会将所有作用域为单例的 Bean 都创建好，但是有的业务场景我们并不需要它提前都创建好。此时，我们可以在Bean 中设置 lzay-init = \"true\" 。 这样，当容器启动之后，作用域为单例的 Bean ，就不在创建。 而是在获得该 Bean 时，才真正在创建加载。 "},"spring/ioc/循环依赖问题.html":{"url":"spring/ioc/循环依赖问题.html","title":"循环依赖问题","keywords":"","body":"循环依赖问题1. 什么是循环依赖2. 循环依赖的两种场景3. 解决流程总结循环依赖问题 1. 什么是循环依赖 循环依赖，其实就是循环引用，就是两个或者两个以上的bean 互相引用对方，最终形成一个闭环，如A 依赖B,B依赖C，C依赖A。如下图所示 循环依赖，其实就是一个死循环的过程，在初始化A的时候发现引用了B，这时候就会去初始化B，然后又开发B引用了A，则又会出初始化A，依次循环用不退出，除非有终结条件 2. 循环依赖的两种场景 构造器的循环依赖 field 属性的循环依赖 对于构造器的循环依赖，Spring 是无法解决的，只能抛出 BeanCurrentlyInCreationException 异常表示循环依赖，所以下面我们分析的都是基于 field 属性的循环依赖。 Spring 只解决 scope 为 singleton 的循环依赖。对于scope 为 prototype 的 bean ，Spring 无法解决，直接抛出 BeanCurrentlyInCreationException 异常。 3. 解决流程总结 首先 A 完成初始化第一步并将自己提前曝光出来（通过 ObjectFactory 将自己提前曝光），在初始化的时候，发现自己依赖对象 B，此时就会去尝试 get(B)，这个时候发现 B 还没有被创建出来 然后 B 就走创建流程，在 B 初始化的时候，同样发现自己依赖 C，C 也没有被创建出来 这个时候 C 又开始初始化进程，但是在初始化的过程中发现自己依赖 A，于是尝试 get(A)，这个时候由于 A 已经添加至缓存中（一般都是添加至三级缓存 singletonFactories ），通过 ObjectFactory 提前曝光，所以可以通过 ObjectFactory#getObject() 方法来拿到 A 对象，C 拿到 A 对象后顺利完成初始化，然后将自己添加到一级缓存中 回到 B ，B 也可以拿到 C 对象，完成初始化，A 可以顺利拿到 B 完成初始化。到这里整个链路就已经完成了初始化过程了 "},"spring/aop/":{"url":"spring/aop/","title":"SpringAOP概述","keywords":"","body":"SpringAOP1. 什么是AOP1.1 面向切面的好处2. AOP中的基本概念3. Spring 中对AOP的支持SpringAOP 1. 什么是AOP AOP既面向切面编程，官方定义 面向切面—— Spring提供了面向切面编程的丰富支持，允许通过分离应用的业务逻辑与系统级服务（例如审计（auditing）和事务（transaction）管理）进行内聚性的开发。应用对象只实现它们应该做的——完成业务逻辑——仅此而已。它们并不负责（甚至是意识）其它的系统级关注点，例如日志或事务支持。 AOP可以分离系统的业务逻辑和系统服务(日志，安全等)，这个功能我想是不难明白（原理是使用了代理模式），但关键是为什么要将这两种进行分离呢？或者说这样做有什么好处？ 1.1 面向切面的好处 在日常的软件开发中，拿日志来说，一个系统软件的开发都是必须进行日志记录的，不然万一系统出现什么bug，你都不知道是哪里出了问题。举个小栗子，当你开发一个登陆功能，你可能需要在用户登陆前后进行权限校验并将校验信息（用户名,密码,请求登陆时间，ip地址等）记录在日志文件中，当用户登录进来之后，当他访问某个其他功能时，也需要进行合法性校验。想想看，当系统非常地庞大，系统中专门进行权限验证的代码是非常多的，而且非常地散乱，我们就想能不能将这些权限校验、日志记录等非业务逻辑功能的部分独立拆分开，并且在系统运行时需要的地方（连接点）进行动态插入运行，不需要的时候就不理，因此AOP是能够解决这种状况的思想吧！ 下图就很直观地展示这个过程： 2. AOP中的基本概念 通知（Adivce） 通知有5种类型: Before 在方法被调用之前调用 After 在方法完成后调用通知，无论方法是否执行成功 After-returning 在方法成功执行之后调用通知 After-throwing 在方法抛出异常后调用通知 Around 通知了好、包含了被通知的方法，在被通知的方法调用之前后调用之后执行自定义的行为 切点（Pointcut） 切点在Spring AOP中确实是对应系统中的方法。但是这个方法是定义在切面中的方法，一般和通知一起使用，一起组成了切面。 连接点（Join point） 比如：方法调用、方法执行、字段设置/获取、异常处理执行、类初始化、甚至是 for 循环中的某个点 理论上, 程序执行过程中的任何时点都可以作为作为织入点, 而所有这些执行时点都是 Joint point 但 Spring AOP 目前仅支持方法执行 (method execution) 也可以这样理解，连接点就是你准备在系统中执行切点和切入通知的地方（一般是一个方法，一个字段） 切面（Aspect） 切面是切点和通知的集合，一般单独作为一个类。通知和切点共同定义了关于切面的全部内容，它是什么时候，在何时和何处完成功能。 引入（Introduction） 引用允许我们向现有的类添加新的方法或者属性 织入（Weaving） 组装方面来创建一个被通知对象。这可以在编译时完成（例如使用AspectJ编译器），也可以在运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 3. Spring 中对AOP的支持 首先AOP思想的实现一般都是基于代理模式，在JAVA中一般采用JDK动态代理模式，但是我们都知道，JDK动态代理模式只能代理接口，如果要代理类那么就不行了 Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理，当你的真实对象有实现接口时，Spring AOP会默认采用JDK动态代理，否则采用cglib代理。 如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类； 如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心。 "},"spring/aop/SpringAOP实现原理.html":{"url":"spring/aop/SpringAOP实现原理.html","title":"Spring AOP实现原理","keywords":"","body":"Spring AOP实现原理1. 代理模式2. 静态代理3.动态代理3.1 JDK自带方法3.2 CGLIB 库的方法参考文章Spring AOP实现原理 1. 代理模式 代理模式UML 类图如下 类图中虚线箭头表示接口实现 菱形和箭头表示组合 具体参考uml类图 代理类实现了被代理类的接口，同时与被代理类是组合关系。下面看一下代理模式的实现 2. 静态代理 接口类： interface Person { void speak(); } 真实实体类： class Actor implements Person { private String content; public Actor(String content) { this.content = content; } @Override public void speak() { System.out.println(this.content); } } 代理类： class Agent implements Person { private Actor actor; private String before; private String after; public Agent(Actor actor, String before, String after) { this.actor = actor; this.before = before; this.after = after; } @Override public void speak() { //before speak System.out.println(\"Before actor speak, Agent say: \" + before); //real speak this.actor.speak(); //after speak System.out.println(\"After actor speak, Agent say: \" + after); } } 测试方法: public class StaticProxy { public static void main(String[] args) { Actor actor = new Actor(\"I am a famous actor!\"); Agent agent = new Agent(actor, \"Hello I am an agent.\", \"That's all!\"); agent.speak(); } } 结果： 3.动态代理 3.1 JDK自带方法 3.1.1 InvocationHandler接口 InvocationHandler接口是最核心的接口 public interface InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; } 我们对于被代理的类的操作都会由该接口中的invoke方法实现，其中的参数的含义分别是： proxy：被代理的类的实例 method：调用被代理的类的方法 args：该方法需要的参数 使用方法 使用方法首先是需要实现该接口，并且我们可以在invoke方法中调用被代理类的方法并获得返回值，自然也可以在调用该方法的前后去做一些额外的事情，从而实现动态代理 3.1.2 Proxy类的newProxyInstance方法 public static Object newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) throws IllegalArgumentException 其中的参数含义如下： loader：被代理的类的类加载器 interfaces：被代理类的接口数组 invocationHandler：就是刚刚介绍的调用处理器类的对象实例 该方法会返回一个被修改过的类的实例，从而可以自由的调用该实例的方法。下面是一个实际例子。 3.1.3 JDK自动代理实际例子 Fruit接口： public interface Fruit { public void show(); } Apple实现Fruit接口： public class Apple implements Fruit{ @Override public void show() { System.out.println(\"代理类Agent.java： public class DynamicAgent { //实现InvocationHandler接口，并且可以初始化被代理类的对象 static class MyHandler implements InvocationHandler { private Object proxy; public MyHandler(Object proxy) { this.proxy = proxy; } //自定义invoke方法 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\">>>>before invoking\"); //真正调用方法的地方 Object ret = method.invoke(this.proxy, args); System.out.println(\">>>>after invoking\"); return ret; } } //返回一个被修改过的对象 public static Object agent(Class interfaceClazz, Object proxy) { return Proxy.newProxyInstance(interfaceClazz.getClassLoader(), new Class[]{interfaceClazz}, new MyHandler(proxy)); } } 测试类： public class ReflectTest { public static void main(String[] args) throws InvocationTargetException, IllegalAccessException { //注意一定要返回接口，不能返回实现类否则会报错 Fruit fruit = (Fruit) DynamicAgent.agent(Fruit.class, new Apple()); fruit.show(); } } 结果： 可以看到对于不同的实现类来说，可以用同一个动态代理类来进行代理，实现了“一次编写到处代理”的效果。但是这种方法有个缺点，就是被代理的类一定要是实现了某个接口的，这很大程度限制了本方法的使用场景。下面还有另外一个使用了CGlib增强库的方法。 3.2 CGLIB 库的方法 CGlib是一个字节码增强库，为AOP等提供了底层支持。下面看看它是怎么实现动态代理的。 import com.zszdevelop.aopdemo.case2.Apple; import org.springframework.cglib.proxy.Enhancer; import org.springframework.cglib.proxy.MethodInterceptor; import org.springframework.cglib.proxy.MethodProxy; import java.lang.reflect.Method; /** * @author zhangshengzhong * @date 2019/10/11 */ public class CGlibAgent implements MethodInterceptor { private Object proxy; public Object getInstance(Object proxy) { this.proxy = proxy; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(this.proxy.getClass()); // 回调方法 enhancer.setCallback(this); // 创建代理对象 return enhancer.create(); } //回调方法 @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { System.out.println(\">>>>before invoking\"); //真正调用 Object ret = methodProxy.invokeSuper(o, objects); System.out.println(\">>>>after invoking\"); return ret; } public static void main(String[] args) { CGlibAgent cGlibAgent = new CGlibAgent(); Apple apple = (Apple) cGlibAgent.getInstance(new Apple()); apple.show(); } } 可以看到结果和JDK动态代理是一样的，但是可以直接对实现类进行操作而非接口，这样会有很大的便利。 参考文章 Spring AOP实现原理spring/aop/SpringAOP实现原理.md "},"spring/transaction/":{"url":"spring/transaction/","title":"Spring事务","keywords":"","body":"Spring事务1. 事务概念1.1 什么是事务1.2 事务的特性2. Spring事务管理接口2.1 PlatformTransactionManager接口介绍2.2 TransactionDefinition接口介绍2.3 TransactionStatus接口介绍参考文章Spring事务 1. 事务概念 1.1 什么是事务 事务是逻辑上的一组操作，要么都执行，要么都不执行 1.2 事务的特性 原子性：事务是最小的执行单位，不予许分割。事务的原子性确保动作要么全部完成，要么完全不起作用 一致性：执行事务前后，数据保持一致 隔离性：并发访问数据库时，一个用户的事务不被其他事务所干扰，个并发事务之间的数据库是独立的 持久性：一个事务被提交之后，他对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响 2. Spring事务管理接口 PlatformTransactionManager: （平台）事务管理器 TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则) TransactionStatus： 事务运行状态 2.1 PlatformTransactionManager接口介绍 所谓事务管理，其实就是“按照给定的事务规则来执行提交或者回滚操作”。 Spring并不直接管理事务，而是提供了多种事务管理器 ，他们将事务管理的职责委托给Hibernate或者JTA等持久化机制所提供的相关平台框架的事务来实现。 Spring事务管理器的接口是： org.springframework.transaction.PlatformTransactionManager ，通过这个接口，Spring为各个平台如JDBC、Hibernate等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 2.1.1PlatformTransactionManager接口代码如下： PlatformTransactionManager接口中定义了三个方法： Public interface PlatformTransactionManager()...{ // Return a currently active transaction or create a new one, according to the specified propagation behavior（根据指定的传播行为，返回当前活动的事务或创建一个新事务。） TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // Commit the given transaction, with regard to its status（使用事务目前的状态提交事务） Void commit(TransactionStatus status) throws TransactionException; // Perform a rollback of the given transaction（对执行的事务进行回滚） Void rollback(TransactionStatus status) throws TransactionException; } 2.2 TransactionDefinition接口介绍 事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition类 ，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面。 2.2.1 TransactionDefinition接口中的方法如下： TransactionDefinition接口中定义了5个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等的常量。 我下面只是列出了TransactionDefinition接口中的方法而没有给出接口中定义的常量，该接口中的常量信息会在后面依次介绍到。 public interface TransactionDefinition { // 返回事务的传播行为 int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getIsolationLevel(); // 返回事务必须在多少秒内完成 //返回事务的名字 String getName()； int getTimeout(); // 返回是否优化为只读事务。 boolean isReadOnly(); } 2.2.1.1 隔离级别 TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 2.2.1.2 事务传播行为（为了解决业务层方法之间互相调用的事务问题） 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED 是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 2.2.1.3 事务超时属性(一个事务允许执行的最长时间) 所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 2.2.1.4 事务只读属性（对事物资源是否执行只读操作） 事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 2.2.1.5 回滚规则（定义事务回滚规则） 这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚（这一行为与EJB的回滚行为是一致的）。 但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。 2.3 TransactionStatus接口介绍 TransactionStatus接口用来记录事务的状态 该接口定义了一组方法,用来获取或判断事务的相应状态信息. PlatformTransactionManager.getTransaction(…) 方法返回一个 TransactionStatus 对象。返回的TransactionStatus 对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。 TransactionStatus接口接口内容如下： public interface TransactionStatus{ boolean isNewTransaction(); // 是否是新的事物 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成 } 参考文章 可能是最漂亮的Spring事务管理详解 "},"spring/springmvc/SpringMVC常见知识点.html":{"url":"spring/springmvc/SpringMVC常见知识点.html","title":"SpringMVC常见知识点","keywords":"","body":"SpringMVC常见知识点1. Spring MVC 简介2. 介绍下 Spring MVC 的核心组件？3. 描述一下 DispatcherServlet的工作流程？3.1 前后端分离的Spring MVC 流程4. @Controller 注解有什么用？5. RestController 和 @Controller 有什么区别？6. @ReuqestMapping 注解有什么用？7. @RequestMapping 和@GetMapping 注解的不同之处在哪里8. 返回JSON 格式使用什么注解？9. 介绍一下WebApplicationContext？10.Spirng MVC 的异常处理？11. Spring MVC 有什么优点？12. Spring MVC怎样设定重定向和转发？13. Spring MVC 的 Controller 是不是单例？SpringMVC常见知识点 1. Spring MVC 简介 Spring MVC 提供”模型-视图-控制器“（Model - View - Controller） 架构和随时可用的组件，用于开发灵活且松散耦合的Web应用程序。 MVC 模式有助于分离应用程序的不同方面，如输入逻辑，业务逻辑和UI逻辑，同时在所有这些元素之间提供松散耦合 2. 介绍下 Spring MVC 的核心组件？ Spring MVC 一共有九大核心组件，分别是： MultipartResolver LocaleResolver ThemeResolver HandlerMapping HandlerAdapter HandlerExceptionResolver RequestToViewNameTransalator ViewResolver FlashMapManager 虽然很多，但是最关键的只有HandlerMapping+HandlerAdapter+HandlerExceptionResolver 3. 描述一下 DispatcherServlet的工作流程？ DiapatcherServlet 的工作流程可以用一副图来说明 发送请求 用户向服务器发送HTTP请求，请求被 Spring MVC 的调度器 DispatherServlet 捕获 映射处理器（HandlerMapping） DispatcherServlet 根据请求 URL,调用 HandlerMapping 获取该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以 HandlerExecutionChain 对象的形式返回。 既 HandlerExecutionChain 中，包含对应的Handler 对象和拦截器门 HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; 处理器适配（HandlerAdapter） DispatcherServlet 根据获得的 Handler，选择一个合适的HandlerAdapter（注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的#preHandler(...)方法） 提取请求 Request 中的模型数据，填充 Handler 入参，开始执行Handler（Controller）。在填充Handler的入参过程中，根据你的配置，Spring 将帮你做一些额外的操作 HttpMessageConverter：会将请求消息（如 JSON,XML 等数据）转换成一个对象 数据转换：对请求消息进行数据转换。如String 转换成Integer，Double等 数据格式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等 数据验证： 验证数据的有效性（长度、格式等），验证结果存储到 BindingResult 或 Error 中。 Handler(Controller) 执行完成后，向 DispatcherServlet 返回一个 ModelAndView 对象。 ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; 调用处理器方法 解析视图 根据返回的ModelAndView，选择一个合适的ViewResolver（必须是已经注册到Spring容器中的ViewResolver），解析出View 对象，然后返回给DispatcherServlet View resolveViewName(String viewName, Locale locale) throws Exception; 7渲染视图+响应请求 ViewResolver 结合Model 和View，来渲染视图，并写回给用户（浏览器） void render(@Nullable Map model, HttpServletRequest request, HttpServletResponse response) throws Exception; 3.1 前后端分离的Spring MVC 流程 对于前后端分离的架构，Spring MVC 只负责 Model 和 Controller 两块，而将View 移交给了前端，所以，上图中的步骤5，6 两步，就不需要了 那么会变成什么样？ 步骤3中，如果Handler（Controller）执行完后，如果判断方法有@ResponseBody 注解，则直接将结果写回给浏览器 返回的是Java POJO 对象，HTTP是不支持的，怎么办？ 需要将结果使用HttpMessageConverter 进行转换后，才能返回。例如说，大家锁熟悉的 FastJsonHttpMessage，将POJO 转换成JSON 字符串返回 4. @Controller 注解有什么用？ @Controller 注解，他将一个类标记为Spring MVC 控制器Controller 5. RestController 和 @Controller 有什么区别？ @RestController 注解，在@Controller 基础上，增加了@ResponseBody 注解，更加适合目前前后端分离的架构下，提供Restful API，返回例如JSON 数据格式。当然，返回什么样的格式，根据客户端的”Accept“请求头来决定 6. @ReuqestMapping 注解有什么用？ @RequestMapping 注解，用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注解可应用于两个级别： 类级别：映射请求的URL 方法级别：映射 URL 以及HTTP 请求方法 7. @RequestMapping 和@GetMapping 注解的不同之处在哪里 @RequestMapping 可注解在类和方法上，@GetMapping 仅可注册在方法上 @RequestMapping 可进行 GET、POST、PUT、DELETE 等请求方法; @GetMapping 是 @RequestMapping 的GET 请求与方法的特例，目的是为了提高清晰度 8. 返回JSON 格式使用什么注解？ 可以使用@Response 注解，或者使用包含@ResponseBody 注解的@RestController 注解。 当然，还是需要配合相应的支持JSON格式化的HttpMessageConverter 实现类。例如，Spring MVC 默认使用MappingJackson2HttpMessageConverter 9. 介绍一下WebApplicationContext？ WebApplicationContext 是实现ApplicationContext 接口的子类，专门为Web应用准备的 他允许从相对于Web 根目录的路径中加载配置文件，完成初始化Spring MVC 组件的工作 从WebApplicationContext中，可以获取ServletContext 引用，整个Web 应用上下文对象将作为属性放置在SerletContext中，一遍Web 应用环境可以访问Spring 上下文 10.Spirng MVC 的异常处理？ Spring MVC 提供了异常解析器 HandlerExceptionResolver 接口，将处理器（handler）执行时发生的异常，解析（转换）成对应的ModelAndView 结果，代码如下 public interface HandlerExceptionResolver { /** * 解析异常，转换成对应的 ModelAndView 结果 */ @Nullable ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex); } 11. Spring MVC 有什么优点？ 使用非常方便，无论是添加HTTP请求方法映射的方法，还是不同数据格式的响应 提供拦截器机制，可以方便的对请求进行拦截处理 提供异常机制，可以方便的对异常做统一的处理 可以任意使用各种视图技术，而不仅仅局限于JSP，例如Freemarker、Thymeleaf等等 不依赖于Servlet API(目标虽是如此，但是在实现的时候确实是依赖于 Servlet 的，当然仅仅依赖 Servlet ，而不依赖 Filter、Listener )。 12. Spring MVC怎样设定重定向和转发？ 结果转发：在返回值的前面加 \"forward:/\" 。 重定向：在返回值的前面加上 \"redirect:/\" 。 当然，目前前后端分离之后，我们作为后端开发，已经很少有机会用上这个功能了。 13. Spring MVC 的 Controller 是不是单例？ 绝大多数情况下,Controller 是单例。 那么，Controller 里一般不建议存在共享的变量 "},"spring/springmvc/SpringMVC拦截器.html":{"url":"spring/springmvc/SpringMVC拦截器.html","title":"SpringMVC拦截器","keywords":"","body":"SpringMVC拦截器1. 详细介绍下 Spring MVC 拦截器？2. Spring MVC 的拦截器可以做哪些事情？3. Spring MVC 的拦截器和Filter 过滤器有什么差别参考文章SpringMVC拦截器 1. 详细介绍下 Spring MVC 拦截器？ org.springframework.web.servlet.HandlerInterceptor ，拦截器接口。代码如下： // HandlerInterceptor.java /** * 拦截处理器，在 {@link HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)} 执行之前 */ default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { return true; } /** * 拦截处理器，在 {@link HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)} 执行成功之后 */ default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception { } /** * 拦截处理器，在 {@link HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)} 执行完之后，无论成功还是失败 * * 并且，只有该处理器 {@link #preHandle(HttpServletRequest, HttpServletResponse, Object)} 执行成功之后，才会被执行 */ default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception { } 一共有三个方法，分别为： preHandle（...）方法，调用Controller方法之前执行 postHandle（...）方法，调用Controller方法之后执行 afterCompletion(...)方法，处理完Controller 方法返回结果之后执行 例如：页面渲染后 注意：无论调用Controller方法是否成功，都会执行 举个例子： 当两个拦截器都实现放行操作是，执行顺序preHandle[1] => preHandle[2] => postHandle[2] => postHandle[1] => afterCompletion[2] => afterCompletion[1] 。 当第一个拦截器preHandle(...)返回false 的时候，也就是对其进行拦截时，第二个拦截器是完全不执行的，第一个拦截器只执行 #preHandle(...) 部分。 当第一个拦截器 #preHandle(...) 方法返回 true ，第二个拦截器 #preHandle(...) 返回 false ，执行顺序为 preHandle[1] => preHandle[2] => afterCompletion[1] 。 总结： preHandle(...)方法，按拦截器定义顺序调用，若任一拦截器返回false，则Controller方法不再调用 #postHandle(...) 和 #afterCompletion(...) 方法，按拦截器定义逆序调用。 #postHandler(...) 方法，在调用 Controller 方法之后执行。 #afterCompletion(...) 方法，只有该拦截器在 #preHandle(...) 方法返回 true 时，才能够被调用，且一定会被调用。为什么“且一定会被调用”呢？即使 #afterCompletion(...) 方法，按拦截器定义逆序调用时，前面的拦截器发生异常，后面的拦截器还能够调用，即无视异常。 2. Spring MVC 的拦截器可以做哪些事情？ 拦截器能做的事情非常非常多，例如： 记录访问日志 记录异常日志 需要登录的请求操作，拦截未登录的用户 ... 3. Spring MVC 的拦截器和Filter 过滤器有什么差别 功能相同：拦截器和Filter 都能实现相应的功能，谁都不比谁强 容器不同：拦截器构建在Spring MVC 体系中，Filter 构建在 Servlet 容器之上 使用便利性不同：拦截器提供了三个方法，分别在不同的时机执行；过滤器仅提供一个方法，当然也能现实拦截器的执行时机的效果，就是麻烦一些 参考文章 过滤器(Filter)和拦截器(Interceptor)的区别 "},"spring/springmvc/RESTful.html":{"url":"spring/springmvc/RESTful.html","title":"RESTful","keywords":"","body":"RESTful1. REST 是什么2. 资源是什么3. 什么是安全的 REST 操作4. 什么是幂等操作？为什么幂等操作如此重要？5. REST 是可扩展的或说是协同的吗？6. REST 用哪种 HTTP 方法呢？7. 删除的 HTTP 状态返回码是什么？8. REST API 是无状态的吗?9. REST 安全码？你能做什么来保护他10. RestTemplate 的优势是什么？11. HttpMessageConverter 在 Spring REST 中代表什么？12. 如何创建 HttpMessageConverter 的自定义实现来支持一种新的请求/响应？13. @PathVariable 注解，在Spring MVC 做了什么？为什么 REST 在 Spring 中如此有用？参考文章RESTful 1. REST 是什么 REST（REpresentational State Transfer） 直译就是：抽象状态转移。 他通过 URL定位资源，用HTTP动词（GET,POST,DELETE,DETC）描述操作。 看Url就知道要什么 看http method就知道干什么 看http status code就知道结果如何 2. 资源是什么 资源是指数据在 REST 架构中如何显示的。将实体作为资源公开，他允许客户端通过HTTP 方法如：GET、POST、PUT、DELETE 等读、写、修改和创建资源 3. 什么是安全的 REST 操作 REST 接口是通过 HTTP 方法完成操作 一些HTTP操作是安全的，如GET 和 HEAD, 他不能在服务端修改资源 PUT、POST、和 DELETE 是不安全的，因为他们能修改服务端的资源 所以，是否安全的界限，在于是否修改服务端的资源 4. 什么是幂等操作？为什么幂等操作如此重要？ 有一些HTTP方法，如：GET,不管你使用多少次他都能产生相同的结果，在没有任何一边影响的情况下，发送多个GET 请求到相同的 URI 将会产生相同的响应结果。因此。这就是所谓幂等操作 换句话说，POST 方法不是幂等操作，因为如果发送多个 POST 请求，他将在服务端创建不同资源。但是，假如你用PUT 更新资源，它将是幂等操作 5. REST 是可扩展的或说是协同的吗？ 是的，REST 是可扩展的和可协作的，他既不托管一种特定的技术选择，也不定在客户端或者服务端。你可以用JAVA,C++、Python 来创建 RESTful WEB 服务，也可以在客户端使用他们 6. REST 用哪种 HTTP 方法呢？ REST 能用任何的 HTTP 方法，但是比较受欢迎的是 用 GET 来检索服务端资源 用 POST 来创建服务端资源 用 PUT 来更新服务端资源 用 DELETE 来删除服务端资源 以上四个操作，分别对应日常的 CRUD 操作 7. 删除的 HTTP 状态返回码是什么？ 在删除成功之后，您的 REST API 应该返回什么状态代码，并没有严格的规则，他可以返回200 或204 没有内容 一般来说，如果删除操作成功，响应主体为空，返回204 如果删除请求成功且响应体不是空的，则返回 200。 8. REST API 是无状态的吗? 是的， REST API 应该是无状态的，因为他是基于 HTTP 的，他也是无状态的 9. REST 安全码？你能做什么来保护他 安全是一个宽泛的术语，他可能意味着消息的安全性，这是通过认证和授权提供的加密和访问限制提供的 REST 通常不是安全的，但是您可以通过使用 Spring Security 或者Shiro 来保护它 10. RestTemplate 的优势是什么？ 在Spring Framework 中，RestTemplate 类是 模板方法模式 的实现。跟其他主流的模板类相似，如 JdbcTemplate 或 JmsTempalte ，它将在客户端简化跟 RESTful Web 服务的集成。正如在 RestTemplate 例子中显示的一样，你能非常容易地用它来调用 RESTful Web 服务。 11. HttpMessageConverter 在 Spring REST 中代表什么？ HttpMessageConverter 是一种 策略接口，他指定了一个转换器，他可以转换 HTTP 请求和响应。Spring REST 用这个接口转换 HTTP 响应到多种格式，例如：JSON 或 XML。 每个 HttpMessageconverter 实现都有一种或几种相关联的 MIME 协议。Spring 使用 ”Accept“的标头来确定客户端所期待的内容类型 然后，他将尝试找到一个注册的 HTTPMessageConverter，他能够处理特定的内容类型，并使用它将响应转换成这种格式，然后再将其发送给客户端 12. 如何创建 HttpMessageConverter 的自定义实现来支持一种新的请求/响应？ 我们仅需要创建自定义的 AbstractHttpMessageConverter 的实现、并使用 WebMvcConfigurerAdaper 的 #extendMessageConverters(List> converters) 方法注中册它，该方法可以生成一种新的请求/ 响应类型 13. @PathVariable 注解，在Spring MVC 做了什么？为什么 REST 在 Spring 中如此有用？ @PathVariable 注解，是Spring MVC 中常用的注解之一，它允许您从 URI 读取值，比如查询参数。它使用 Spring 创建 RESTful Web 服务时特别有用，因为在 REST 中，资源标识符是URI 的一部分 参考文章 排名前20的REST和Spring MVC面试题 "},"spring/SpringBoot/":{"url":"spring/SpringBoot/","title":"Spring Boot常见知识点","keywords":"","body":"Spring Boot常见知识点1. Spring Boot 的自动配置是如何实现1.1 加载过程2. 什么是嵌入式服务器？我们为什么要使用嵌入式服务器呢？3. 微服务同时调用多个服务，怎么支持事务的？4. 个服务之间通信，对Restful 和RPC 这2中方式如何选择5. 怎么设计无状态服务5.1 什么是无状态5.2 如何设计5.3 场景说明6. Spring Cache 三种常用的缓存注解和意义？7. Spring Boot 如何设置支持跨域请求7.1 什么是跨域7.2 支持跨域8. JPA 和 Hibernate 有哪些区别？JPA 可以支持动态 SQL 吗？8.1 JPA 和 Hibernate 有哪些区别？8.2 JPA 可以支持动态 SQL 吗？9.Spring Boot 约定优先于配置（最大的优势）9.1 Spring Boot 中“约定优于配置”的具体体现在哪里9.2 Spring Boot 在启动的时候会做的几件事情10. Spring 、Spring Boot 和 Spring Cloud 的关系?Spring Boot常见知识点 1. Spring Boot 的自动配置是如何实现 Spring Boot 项目的启动注解是：@SpringBootApplication，其实他由下面三个注解组成的： @Configuration @ComponentScan @EnableAutoConfiguration 1.1 加载过程 其中@EnableAutoConfiguration 是实现自动配置的入口， 该注解又通过 @Import 注解导入了AutoConfigurationImportSelector，在该类中加载 META-INF/spring.factories 的配置信息。 然后筛选出以EnableAutoConfiguration 为key的数据，加入到IOC 容器中，实现自动配置功能 2. 什么是嵌入式服务器？我们为什么要使用嵌入式服务器呢？ 当我们创建一个可以部署的应用程序的时候，我们将会把服务器（例如：tomcat）嵌入到可部署的服务器中。 例如：对于一个 Spring Boot 应用程序来说，你可以生成一个包含 Embedded Tomcat 的应用程序 jar。你就可以像运行正常 Java 应用程序一样来运行 web 应用程序了。 嵌入式服务器就是我们的可执行单元包含服务器的二进制文件（例如，tomcat.jar）。 思考一下再你虚拟机部署应用程序需要什么 安装java 安装web 或者是应用程序的服务器（Tomcat、weblogic等等） 部署应用程序war 如果我们想简化这些步骤，应该怎么做呢？ 让我们来思考如何使服务器成为应用程序的一部分？ 你只需要一个安装了 Java 的虚拟机，就可以直接在上面部署应用程序了， 3. 微服务同时调用多个服务，怎么支持事务的？ 集成 Aatomikos 支持分布式事务 一般不建议这样使用，因为使用分布式事务会增加请求的响应时间，影响系统的TPS 消息的补偿机制来处理分布式事务 4. 个服务之间通信，对Restful 和RPC 这2中方式如何选择 在传统的SOA治理中，使用rpc的居多；Spring Cloud默认使用restful进行服务之间的通讯。rpc通讯效率会比restful要高一些，但是对于大多数公司来讲，这点效率影响甚微。我建议使用restful这种方式，易于在不同语言实现的服务之间通讯。 5. 怎么设计无状态服务 5.1 什么是无状态 如果一个数据需要被多个服务共享，才能完成一笔交易，那么这个数据被称为状态。进而依赖这个“状态”数据的服务被称为有状态服务，反之称为无状态服务 5.2 如何设计 无状态服务原则并不是说在微服务架构里就不允许存在状态，表达的真实意思是要把有状态的业务服务改变为无状态的计算类服务，那么状态数据也就相应的迁移到对应的“有状态数据服务”中 5.3 场景说明 例如我们以前在本地内存中建立的数据缓存、Session缓存，到现在的微服务架构中就应该把这些数据迁移到分布式缓存中存储，让业务服务变成一个无状态的计算节点。迁移后，就可以做到按需动态伸缩，微服务应用在运行时动态增删节点，就不再需要考虑缓存数据如何同步的问题。 6. Spring Cache 三种常用的缓存注解和意义？ @Cacheable： 用来声明方法是可缓存的，将结果存储到缓存中以便后续使用相同参数调用时不需执行实际的方法，直接从缓存中取值。 CachePut： 使用 @CachePut 标注的方法在执行前，不会去检查缓存中是否存在之前执行过的结果，而是每次都会执行该方法，并将执行结果以键值对的形式存入指定的缓存中。 @CacheEvict: 是用来标注在需要清除缓存元素的方法或类上的，当标记在一个类上时表示其中所有的方法的执行都会触发缓存的清除操作。 7. Spring Boot 如何设置支持跨域请求 7.1 什么是跨域 现代浏览器出于安全的考虑， HTTP 请求时必须遵守同源策略，否则就是跨域的 HTTP 请求，默认情况下是被禁止的，IP（域名）不同、或者端口不同、协议不同（比如 HTTP、HTTPS）都会造成跨域问题。 7.2 支持跨域 配置CoreFilter @Configuration public class CorsConfig { @Bean public CorsFilter corsFilter() { final UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); final CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(true); config.setAllowedOrigins(Arrays.asList(\"*\")); //http:www.a.com config.setAllowedHeaders(Arrays.asList(\"*\")); config.setAllowedMethods(Arrays.asList(\"*\")); config.setMaxAge(300l); source.registerCorsConfiguration(\"/**\", config); return new CorsFilter(source); } } 8. JPA 和 Hibernate 有哪些区别？JPA 可以支持动态 SQL 吗？ 8.1 JPA 和 Hibernate 有哪些区别？ JPA本身是一种规范，它的本质是一种ORM规范（不是ORM框架，因为JPA并未提供ORM实现，只是制定了规范）因为JPA是一种规范，所以，只是提供了一些相关的接口，但是接口并不能直接使用，JPA底层需要某种JPA实现，Hibernate 是 JPA 的一个实现集。 8.2 JPA 可以支持动态 SQL 吗？ JPA 是根据实体类的注解来创建对应的表和字段，如果需要动态创建表或者字段，需要动态构建对应的实体类，再重新调用Jpa刷新整个Entity。动态SQL，mybatis支持的最好，jpa也可以支持，但是没有Mybatis那么灵活。 9.Spring Boot 约定优先于配置（最大的优势） Spring Boot 的最大的优势是“约定优于配置“。“约定优于配置“是一种软件设计范式，开发人员按照约定的方式来进行编程，可以减少软件开发人员需做决定的数量，获得简单的好处，而又不失灵活性。 9.1 Spring Boot 中“约定优于配置”的具体体现在哪里 Spring Boot Starter、Spring Boot Jpa 都是“约定优于配置“的一种体现。都是通过“约定优于配置“的设计思路来设计的，Spring Boot Starter 在启动的过程中会根据约定的信息对资源进行初始化；Spring Boot Jpa 通过约定的方式来自动生成 Sql ，避免大量无效代码编写 9.2 Spring Boot 在启动的时候会做的几件事情 ① Spring Boot 在启动时会去依赖的 Starter 包中寻找 resources/META-INF/spring.factories 文件，然后根据文件中配置的 Jar 包去扫描项目所依赖的 Jar 包。 ② 根据 spring.factories 配置加载 AutoConfigure 类 ③ 根据 @Conditional 注解的条件，进行自动配置并将 Bean 注入 Spring Context 总结一下，其实就是 Spring Boot 在启动的时候，按照约定去读取 Spring Boot Starter 的配置信息，再根据配置信息对资源进行初始化，并注入到 Spring 容器中。这样 Spring Boot 启动完毕后，就已经准备好了一切资源，使用过程中直接注入对应 Bean 资源即可。 10. Spring 、Spring Boot 和 Spring Cloud 的关系? Spring 最初最核心的两大核心功能 Spring Ioc 和 Spring Aop 成就了 Spring，Spring 在这两大核心的功能上不断的发展，才有了 Spring 事务、Spring Mvc 等一系列伟大的产品，最终成就了 Spring 帝国，到了后期 Spring 几乎可以解决企业开发中的所有问题。 Spring Boot 是在强大的 Spring 帝国生态基础上面发展而来，发明 Spring Boot 不是为了取代 Spring ,是为了让人们更容易的使用 Spring 。 Spring Cloud 是一系列框架的有序集合。它利用 Spring Boot 的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用 Spring Boot 的开发风格做到一键启动和部署。 Spring Cloud 是为了解决微服务架构中服务治理而提供的一系列功能的开发框架，并且 Spring Cloud 是完全基于 Spring Boot 而开发，Spring Cloud 利用 Spring Boot 特性整合了开源行业中优秀的组件，整体对外提供了一套在微服务架构中服务治理的解决方案。 用一组不太合理的包含关系来表达它们之间的关系。 Spring ioc/aop > Spring > Spring Boot > Spring Cloud "},"spring/spring-in-action/Spring概述.html":{"url":"spring/spring-in-action/Spring概述.html","title":"Spring概述","keywords":"","body":"Spring概述1.简化Java开发1.1 Spring 如何简化Java开发Spring概述 Spring 的使命：简化Java开发 1.简化Java开发 1.1 Spring 如何简化Java开发 为了降低Java开发的复杂性，Spring采取了以下4种关键策略 基于POJO的轻量级和最小侵入性编程 通过依赖注入和面向接口实现松耦合 基于切面和惯例进行声明式编程 通过切面和模板减少样板式代码 Spirng 所做的任何事都可以追溯到上述的一条或多条策略 注：POJO（Plain old java object）简单老式java对象 ​ bean：Spring 用bean/JavaBean表示应用组件，组件可以是任意形式的POJO。广义上Javabean 和POJO 是同义词 1.1.1 激发POJO 的潜能 很多框架通过强迫应用继承他们的类或实现他们的接口从而导致应用与框架绑死 如：早起的struts，WebWork等 Spring竭力避免因自身API而弄乱你的应用代码，Spring不会强迫你实现Spring规范的接口或继承Spring规范的类。 最坏的场景是，一个类或许会使用Spring注解，但他依旧是POJO，但魔力在于Spring 通过DI来装配他们，保持应用对象彼此之间的松耦合 1.1.2 依赖注入 任何一个有实际意义的应用都会由两个或者更多类组成，类相互之间进行协作完成特定的业务逻辑。 传统做法面临的问题：每个对象负责管理与自己相互协作的对象（既他所依赖的对象）的引用，这将会导致高度耦合和难以测试的代码。 传统做法是如何导致高度耦合和为什么难以测试的呢? package com.springinaction.knights; public class DamselRescuingKnight implements Knight { private RescueDamselQuest quest; public DamselRescuingKnight() { this.quest = new RescueDamselQuest(); } public void embarkOnQuest() { quest.embark(); } } 高度耦合：DamselRescuingKnight负责管理自己相互协作的对象，他在构造函数中你创建了RescueDamselQuest对象，这就使得他们两个高度耦合在一起。这也极大的限制了骑士的能力，因为他只实现了RescueDamselQuest救援行动。如果美女需要救援骑士召之即来，如果遇上恶龙等就爱莫能助了 难以测试：在测试的时候，你要保证embarkOnQuest()被调用的时候，quest.embark()也要被调用 耦合具有的两面性： 一方面：紧密的耦合的代码难以测试，难以复用，难以理解 另一方面：一定的耦合又是必须的，完全没有耦合又是必须的，为了完成实际意义的功能，不同的类必须以适当的方式进行交互 总之：耦合是必须的，但应该被小心谨慎管理 DI依赖注入将会带来什么改变？ 通过DI，对象之间的依赖关系将由 系统中负责协调各对象的第三方组件中创建对象时设定，无需自行创建或管理他们的依赖 说白了就是，实际调用方传递进来 依赖注入方式一：构造器注入 package com.springinaction.knights; public class BraveKnight implements Knight { private Quest quest; public BraveKnight(Quest quest) { //谁使用，谁传递。而不是自己创建 this.quest = quest; } public void embarkOnQuest() { quest.embark(); } } 谁使用，谁传递。而不是自己创建。这样该类的扩展性就得到了提高。这个骑士可以接受各种任务而不是只能执行某一项任务 BraveKnight没有与任何任务Quest 实现发生耦合，对他来说，被要求调整的探险任务只要实现Quest接口，那么具体哪种类型的任务就无关紧要了 这就是DI带来的最大收益：松耦合 什么是松耦合 如果一个对象只通过接口（而不是具体实现或初始化过程）来表明依赖关系，那么这种依赖就能够在对象毫不知情的情况下，用不同的实现替换 DI依赖注入是如何工作的呢？ Spring通过应用上下文（Application Context）装载bean的定义并把他们组装起来，Spring 的应用上下文全权负责对象的创建和组装。Spring自带多种应用上下文的实现，他们主要的区别仅仅在于如何加载配置。 以ClassPathXmlApplicationContext为例： ClassPathXmlApplicationContext加载knights.xml并获得Knight的对象引用 public class KnightMain { public static void main(String[] args) throws Exception { ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( \"META-INF/spring/knights.xml\"); Knight knight = context.getBean(Knight.class); knight.embarkOnQuest(); context.close(); } } "},"spring/spring-in-action/面向切面.html":{"url":"spring/spring-in-action/面向切面.html","title":"面向切面","keywords":"","body":"面向切面1.为什么要使用AOP面向切面 DI能够让相互协作的软件组件保持松耦合，而面向切面编程（aspect-oriented-programming,AOP）允许你把遍布应用各处的功能分离出来形成可重用的组件 1.为什么要使用AOP 1.1 横切关注点 系统由许多不同的组件组成，每一个组件各负责一块特定的功能，除了实现自身核心的功能之外，这些组件还经常承担额外的职责。诸如日志，事物管理和安全这样的系统服务经常融入到自身具有核心业务逻辑的组件中去，这些系统服务通常称为横切关注点，因为他们会跨越多个组件 1.2 横切关注点分散到各个组件中，带来的双重复杂性 如果你要改变这些关注点逻辑，必须修改各个模块相关实现，即使你把这些关注点抽象到独立模块，其他模块只是调用他的方法，但方法的调用还是会重复的出现在各个模块中 组件会因为那些会自身核心业务无关的代码而变得混乱 1.3 AOP能带来什么？ 1.3.1基于切面和惯例进行申明式编程 AOP能够使这些服务模块化，并以申明的方式将他们应用到他们需要影响的应用组件中 优势： 组件具有更高的内聚性 更加关注自身的业务 不需要了解涉及系统服务带来的复杂性 确保POJO的简单性 1.3.2 使用模板消除样板式代码 有些代码总感觉以前曾经这么写过，这是因为他是样板式代码。通常为了实现通用和简单的任务，你不得不一遍遍重复编写这样的代码 造成的原因 很多事因为使用Java API导致的样板代码： 如JDBC查询数据库：（Spring 解决方案JdbcTemplate） 因为少量查询员工代码淹没在一堆JDBC代码中 首先要创建数据库连接 创建一个语句对象 最后才能查询 还要捕获异常 关闭数据库连接，语句、结果集 "},"spring/spring-in-action/summary/Spring容器.html":{"url":"spring/spring-in-action/summary/Spring容器.html","title":"Spring容器","keywords":"","body":"Spring容器1.Spring容器的职责2.Spring容器的实现3.应用上下文Spring容器 在Spring中，应用的生存依赖于Spring 容器（container），容器是Spirng框架的核心，Spring容器使用DI管理构成应用的组件，他会创建相互协作的组件之间的关联。 优势：这样创建的对象 更简单干净 更易于理解 更易于重用 更易于单元测试 1.Spring容器的职责 创建对象 装配 配置并管理他们的整个生命周期，从生存到思维（new到finalize（）） 2.Spring容器的实现 大体可以分成两种类型 bean工厂（由beanFactory接口定义） 最简单的容器，提供基本的DI支持 应用上下文（ApplicationContext接口定义） 更受欢迎 基于BeanFactory构建，并提供应用架构级别的服务 3.应用上下文 Spring自带了多种类型的应用上下文 AnnotationConfigApplicationContext:从一个或多个基于Java的配置类中加载Spring应用上下文 AnnotationConfigWebApplicationContext:从一个或多个基于Java的配置类中加载Spring Web 应用上下文 ClassPathXmlApplicationContext:从类路径下的一个或多个xml配置文件中加载上下文定义，把应用上下文的定义文件作为类资源 FileSystemXmlApplicationContext:从文件系统下的一个或多个Xml配置文件中加载上下文定义 XmlWebApplicationContext:从web 应用下的一个或多个xml配置文件中加载上下文定义‘ 应用上下文准备就绪之后，我们就可以调用上下文的getBean()方法从Spring容器中获取bean "},"spring/spring-in-action/summary/bean的生命周期.html":{"url":"spring/spring-in-action/summary/bean的生命周期.html","title":"bean的生命周期","keywords":"","body":"bean的生命周期1.传统java应用中，bean的生命周期2.Spring容器中，bean的生命周期推荐阅读bean的生命周期 1.传统java应用中，bean的生命周期 使用关键字new进行bean的实例化，然后就可以使用了，一旦bean不被使用，则由java自动进行垃圾回收 2.Spring容器中，bean的生命周期 Spring对bean 进行实例化 Spring将值和bean的引用注入到bean对应的属性中 如果bean 实现了beanNameAware接口，Spring将bean的id传递给setBeanName方法 如果bean 实现了BeanFactoryAware接口，Spring将调用setBeanFactory（）方法，将BeanFactory容器实例传入 如果bean 实现了ApplicationContextAware接口，Spring将调用setApplicationContext()方法，将bean所在的应用上下文的引用传入进来 如果bean实现了beanPostProcessor接口，Spring将调用他们的postProcessBeforeInitialization()方法 如果bean实现了InitializingBean接口，Spring将调用他们的afterPropertiesSet()方法，类似这，如果bean 使用init-method 声明了初始化方法，该方法也会被调用 如果bean实现了BeanpostProcessor接口，Spring将调用他们的postProcessorAfterInitialization(）方法 此时，bean已经准备就绪，可以被应用程序使用，他们将一直驻留在应用上下文，直到改应用上下文被销毁 如果bean实现了DisposableBean接口，Spring将调用他的destory（）接口方法，同样，如果bean使用destory-method声明销毁方法，该方法也会被调用 推荐阅读 Spring Bean的生命周期（非常详细） "},"spring/spring-in-action/summary/bean的生命周期实例.html":{"url":"spring/spring-in-action/summary/bean的生命周期实例.html","title":"bean的生命周期实例","keywords":"","body":"bean的生命周期实例实例bean的生命周期实例 但看生命周期可能比较难理解，可以结合实例加深印象 生命周期大体就是 初始化构造函数 设置对应属性 判断是否实现了各个接口 若实现BeanNameAware接口，则调用setBeanName（）方法 若实现BeanFactoryAware接口，则调用setBeanFactory()方法 若实现ApplicationContextAware接口，则调用setApplicationContext()方法 若实现BeanPostProcessor接口，则调用postProcessBeforeInitialization() 初始化接口方法 若实现InitializingBean接口，则调用afterPropertiesSet()方法 init-method 声明了初始化方法，该方法也会被调用 若实现BeanpostProcessor接口，则调用postProcessorAfterInitialization()方法 bean准备就绪，一直驻留在应用上下文，直到上下文销毁 上下文销毁，额外处理 若实现DisposableBean接口，调用destory方法 若 destroy-method 申明了销毁方法，调用销毁方法 实例 MyPerson 类 package com.zszdevelop; import org.springframework.beans.BeansException; import org.springframework.beans.factory.*; import org.springframework.beans.factory.config.BeanPostProcessor; import org.springframework.context.ApplicationContext; import org.springframework.context.ApplicationContextAware; /** * Created by zhangshengzhong on 2019/7/23. */ public class MyPerson implements BeanNameAware,BeanFactoryAware,ApplicationContextAware,BeanPostProcessor,InitializingBean,DisposableBean{ private String name; private int age; public MyPerson() { System.out.println(\"第1步：Spring调用bean 的构造器实例化\"); } public void setName(String name) { System.out.println(\"第2步：【注入属性】注入属性name:\"+name); this.name = name; } public String getName() { return name; } public int getAge() { return age; } public void setAge(int age) { System.out.println(\"第2步：【注入属性】注入属性age:\"+age); this.age = age; } @Override public void setBeanName(String s) { System.out.println(\"第3步：Spring调用 BeanNameAware 的setBeanName id:\"+s); } @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { System.out.println(\"第4步：Spring调用 BeanFactoryAware 的 setBeanFactory,并将beanFactory传递进来\"); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { System.out.println(\"第5步：Spring调用 ApplicationContextAware 的 setApplicationContext，并将applicationContext传递进来\"); } @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"第6步：Spring调用 BeanPostProcessor 的 postProcessBeforeInitialization()\"); return null; } @Override public void afterPropertiesSet() throws Exception { System.out.println(\"第7步：Spring调用 InitializingBean 的 afterPropertiesSet()\"); } // 通过的init-method属性指定的初始化方法 public void myInit() { System.out.println(\"第7步：调用的init-method属性指定的初始化方法\"); } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"第8步：Spring调用 BeanPostProcessor 的 postProcessAfterInitialization()\"); return null; } @Override public void destroy() throws Exception { System.out.println(\"第10步：Spring调用 DisposableBean 的 destroy()\"); } // 通过的destroy-method属性指定的初始化方法 public void myDestory() { System.out.println(\"第10步：调用的destroy-method属性指定的初始化方法\"); } } myPersonBeans.xml 测试类MyBeanLifeCycle package com.zszdevelop; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; /** * Created by zhangshengzhong on 2019/7/23. */ public class MyBeanLifeCycle { public static void main(String[] args) { System.out.println(\"现在开始初始化容器\"); ApplicationContext factory = new ClassPathXmlApplicationContext(\"myPersonBeans.xml\"); System.out.println(\"容器初始化成功\"); //得到Preson，并使用 MyPerson person = factory.getBean(\"myPersonBean\",MyPerson.class); System.out.println(person); System.out.println(\"现在开始关闭容器！\"); ((ClassPathXmlApplicationContext)factory).registerShutdownHook(); } } 运行结果 第1步：Spring调用bean 的构造器实例化 第2步：【注入属性】注入属性age:15 第2步：【注入属性】注入属性name:张三 第3步：Spring调用 BeanNameAware 的setBeanName id:myPersonBean 第4步：Spring调用 BeanFactoryAware 的 setBeanFactory,并将beanFactory传递进来 第5步：Spring调用 ApplicationContextAware 的 setApplicationContext，并将applicationContext传递进来 第7步：Spring调用 InitializingBean 的 afterPropertiesSet() 第7步：调用的init-method属性指定的初始化方法 容器初始化成功 com.zszdevelop.MyPerson@396e2f39 现在开始关闭容器！ 00:52:56.622 [Thread-0] DEBUG org.springframework.context.support.ClassPathXmlApplicationContext - Closing org.springframework.context.support.ClassPathXmlApplicationContext@383534aa, started on Wed Jul 24 00:52:56 CST 2019 第10步：Spring调用 DisposableBean 的 destroy() 第10步：调用的destroy-method属性指定的初始化方法 "},"spring/spring-in-action/DI/":{"url":"spring/spring-in-action/DI/","title":"依赖注入","keywords":"","body":"依赖注入依赖注入 "},"spring/spring-in-action/DI/装配Bean.html":{"url":"spring/spring-in-action/DI/装配Bean.html","title":"装配Bean","keywords":"","body":"装配Bean1.如何创建对象关联2.依赖注入的本质：装配（wiring）3.bean是如何装配在一起的？3.1 三种装配方式3.2 装配方式选择3.3 装配方式3.3.2 通过Java代码装配bean混合配置装配Bean 任何一个成功的应用都是由多个为了实现某一个业务目标而相互协作的组件构成的。这些组件必须彼此了解，并且相互协作完成工作 我们的程序也不可能都由一个对象来完成。需要相互之间的关联协作，才能更高效的完成任务。 创建应用对象之间协作关系的行为称之为装配，也就是依赖注入（DI） Spring 容器复责创建应用程序中的bean并通过DI来协调这些对象之间的关系 1.如何创建对象关联 传统方法（通过构造器或者查找） 缺点：对象之间高度耦合，难以复用和测试 Spring DI依赖注入 优势：无需自己查找或创建与所关联的对象，容器负责把需要相互协作的对象引用赋予各个对象 2.依赖注入的本质：装配（wiring） 装配（wiring）：创建应用对象之间协作关系的行为 3.bean是如何装配在一起的？ 3.1 三种装配方式 spring 提供了三种方式告诉spring要创建哪些bean。并且如何将其装配在一起 在XML中进行显示装配 在JAVA中进行显示装配 隐式的bean发现机制和自动装配 3.2 装配方式选择 尽可能的使用自动装配机制，显示配置越少越好。 当你必须使用显示配置bean的时候（源码不是你来维护，又需要代码配置bean）。更推荐使用类型安全并且比XML更加强大的JavaConfig 当你想使用便利的XML 命名空间，并且在JavaConfig中没有同样的实现时，才应该使用XML 3.3 装配方式 3.3.1 自动化装配 自动装配就是让Spring自动满足bean依赖的一种方法 优势：自动化装配最便利 Spring 从两个角度来实现自动化装配 组件扫描（component scanning）：spring会自动发现应用上下文中所创建的bean 自动装配（autowiring）：spring自动满足bean之间的依赖 注解 @Component：表明该类会作为组件类，并告知Spring要为这个类创建bean Spring 会为所以bean 指定一个id，默认是类名的第一个字母边小写 @ComponentScan：这个注解能够在Spring 中启用扫描。默认扫描与配置类相同的包 也可以使用xml: 通过ComponentScan（basePackages={\"image\",\"\"videos}）指定多个包 @Named：是java依赖注入规范提供的注解 大多数场景可以和@Component互换，更推荐@Component见名知意 @Autowired:申明要进行自动装配 可以设置在成员变量构造器，setter方法，或者其他方法上。spring会尝试满足方法上声明的依赖 可以设置非需要@Autowired(required=false) @Inject:来源于Java 依赖注入规范 用法同@Autowired 3.3.2 通过Java代码装配bean 使用时机 想要将第三方库中的组件装配到你的应用中，是没有办法在他的类上使用@Component和@Autowired的。这时候就需要采用显示装配的方式 JavaConfig对比XML优势： 更加强大，类型安全，并且对重构友好 JavaConfig 是配置代码，这意味着他不应该包含任何业务逻辑 注解 @Configuration:表明这个类是一个注解类，改类应该包含在spring应用上下文中如何创建bean的细节 @Bean：通过该注解声明bean 该注解会告诉Spring这个方法将会返回一个对象，该对象要注册为Spring 应用上下文中的bean。方法中包含了最终产生bean 实例的逻辑 bean 的ID与带有@bean注解的方法名是一样的，也可以指定name 属性 如果还存在其他依赖关系我们可以使用参数的形式表明 @Bean public CDPlayer cdplay(ComoactDisc disc){ return new CDPlayer(disc); } 3.3.3 通过XML 装配bean 在JavaConfig 中的配置规范是，必须创建一个带有@Configuration注解的类 在XML配置中，要创建一个XML文件，并且以元素为根 声明bean 通过声明一个bean，类似于JavaConfig中的@Bean注解，创建这个bean的类通过class 属性来制定，并且要使用全限定的类名 bean 的id是“com.Myclass#0”,#0是计数形式，区分其他bean，如果你要引用他的话，就需要制定id属性 XML 装配案例 有两种方案 construct-arg 装配集合 c-命名空间 p:compactDisc-ref=\"compactDisc\" > 使用c命名空间无法装配集合 混合配置 可以一部分xml一部分javaConfig 相互引用 "},"spring/spring-in-action/DI/高级装配.html":{"url":"spring/spring-in-action/DI/高级装配.html","title":"高级装配","keywords":"","body":"高级装配高级装配 "},"spring/spring-in-action/DI/高级装配/环境与profile.html":{"url":"spring/spring-in-action/DI/高级装配/环境与profile.html","title":"环境与profile","keywords":"","body":"环境与profile1.环境与profile1.1 使用Java注解配置1.2 在XML中配置profile1.3 激活profile环境与profile 1.环境与profile 环境迁移中某些环境相关的配置并不适合迁移到生产环境中，甚至迁移过去也无法正常工作 如 数据库配置 加密算法 1.1 使用Java注解配置 1.1.1 类级别@Profile @Configuration @Profile(\"dev\") public class DevConfig(){ ... @Bean public Datasource datasource(){ ... } } 配置类中的bean 只有在dev profile 激活才创建，如果没有激活那么带@Bean 注解的方法都会忽略掉。 方法级的@Profile 在Spring 3.2开始就支持方法级别的@Profile 注：没有指定profile 的bean始终都会被创建 1.2 在XML中配置profile 1.2.1 可以通过 元素的profile属性，配置profile bean ... 1.2.2 在中嵌套设置 这样就能将他们定义在一个xml，不需要重复定义 ... ... 1.3 激活profile 确定那个profile 处于激活状态，需要依赖两个独立的属性 spring.profiles.active spring.profiles.defaule 1.3.1 激活规则 如果设置了active属性的话，由active的值确定激活哪个profile 如果没设置active，那么就由defaule的值决定 active和defaule 都没有设置自，那就没有激活的profile，那么只会创建那些没有定义在profile 的bean 1.3.2 设置 spring 提供了多种设置方法 作为DispatcherServlet的初始化参数 作为Web应用的上下文参数 作为JNDI条目 作为环境变量 作为JVM的系统属性 在集成测试类上，使用@ActiveProfiles注解设置 1.4 使用Profile进行测试 Spring 提供了@ActiveProfiles注解指定运行测试时要激活哪个profile @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes={PersistenceTestConfig.class}) @ActiveProfiles(\"dev\") public class PersistenceTest{ } "},"spring/spring-in-action/DI/高级装配/条件化的bean.html":{"url":"spring/spring-in-action/DI/高级装配/条件化的bean.html","title":"条件化的bean","keywords":"","body":"条件化的bean实例条件化的bean 有些bean 我们只想在特定情况下创建，我们可以使用@Conditional注解 作用域：用来带有@Bean注解的方法上 作用效果： ​ 如果给定的条件计算结果为true，就会创建这个bean，否则这个bean 会被忽略 实例 @Bean @Conditional(MagocExistsConfitional.class)//条件化创建bean public MagicBean magicBean(){ retrun new MagicBean(); } 设置给Conditiona的类可以是实现任意实现Condition 的接口 Condition接口类 public interface Condition { boolean matches(ConditionContext var1, AnnotatedTypeMetadata var2); } 该接口实现起来简单直接，只需要提供matches() 方法的实现即可，如果返回true，那么久会创建带有@Conditionl 注解的bean，否则不会创建 处理自动装配的歧义性 "},"spring/spring-in-action/DI/高级装配/处理自动装配的歧义性.html":{"url":"spring/spring-in-action/DI/高级装配/处理自动装配的歧义性.html","title":"处理自动装配的歧义性","keywords":"","body":"处理自动装配的歧义性解决歧义性处理自动装配的歧义性 当有bean 自动装配出现了歧义性时，spring无法自动装配，会抛出NoUniqueBeanDefinitionException 解决歧义性 @Primary 首选项 当遇到歧义性的时候，Spring将选用首选的bean @Qualifier 限定符 在注入时指定想要注入哪个bean "},"spring/spring-in-action/DI/高级装配/bean的作用域.html":{"url":"spring/spring-in-action/DI/高级装配/bean的作用域.html","title":"bean的作用域","keywords":"","body":"bean的作用域单例面临的问题bean的作用域@Scope指定作用域使用会话和请求作用域bean的作用域 默认情况下，Spring应用上下文中所有的bean 都是单例（singleton）的形式创建的。 也就是说，不管给定的一个bean 被注入到其他bean多少次，每次注入的都是同一个实例 单例面临的问题 单例（适应大多数情况） 初始化和垃圾回收对象成本小 单例的局限性 但是有时候你所使用的类的异变的，他们会保持一些状态，因此重用是不安全的 bean的作用域 单例（Singleton） 在整个应用中，只会创建bean的一个实例 原型（Prototype） 每次注入或者通过Spring应用上下文取的时候，都会创建一个新的bean 会话（Session） 在web应用中，为每一个会话创建一个bean实例 请求（Request） 在web应用中，为每一个请求创建一个bean实例 @Scope指定作用域 使用@Scope指定作用域，他可以和@Component 和@Bean一起使用 结合@Component @Component @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) 结合@Bean @Bean @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) 使用Xml来配置 使用会话和请求作用域 典型应用 在电子商务网站中，有一个bean 代表用户购物车 单例情况 如果购物车是单例，那么导致所有用户都会向同一个购物车中添加商品 原型作用域 在应用某一处添加商品，在另一处就不能使用了，因为这里注入的是另一个原型购物车 会话作用域 最为合适，因为他和用户的关联最大 创建会话作用域 @Component @Scope( value=WebApplicationContext.SCOPE_SESSION proxyMode=SCopedproxyMode.INTERFACES ) public SHoppingCart cart(){ } "},"spring/spring-in-action/DI/高级装配/运行时值的注入.html":{"url":"spring/spring-in-action/DI/高级装配/运行时值的注入.html","title":"运行时值的注入","keywords":"","body":"运行时值的注入属性占位符（Property placeholder）运行时值的注入 Spring提供了两种在运行时求值的方法 属性占位符（Property placeholder） Spring表达式语言(SpEL) 属性占位符（Property placeholder） 1.使用Environment来检索属性 声明属性源并通过Spring的Enviroment来检索属性 @Configuration @PropertSource(\"classpath:/com/app.properties\") public class ExpressConfig{ @Autowired Environment env; @Bean public BlankDisc disc(){ return new BlankDisc( env.getPropert(\"disc.title\") ) } } 2.解析属性占位符 占位符使用${...}包装属性名称 @Value(\"${disc.title}\" String title) "},"spring/spring-in-action/SpringMVC/":{"url":"spring/spring-in-action/SpringMVC/","title":"Spring MVC","keywords":"","body":"Spring MVCSpring MVC SpringMVC 是构建在Spring理念之上的web框架 "},"spring/spring-in-action/SpringMVC/SpringMVC请求流程.html":{"url":"spring/spring-in-action/SpringMVC/SpringMVC请求流程.html","title":"SpringMVC请求流程","keywords":"","body":"SpringMVC请求流程SpringMVC请求流程 DispatcherServlet SpringMVC 所有请求都会通过一个单例的DispatcherServlet,委托给其他组件（controller）来执行实际的处理 处理器映射 背景： DispatcherServlet的任务就是将请求发送给Spring MVC控制器（controller） 应用程序可能存在多个控制器 具体步骤 DispatcherServlet会查询一个或多个处理器映射（handler mapping）来确定下一站在哪里，处理器会根据携带的url信息来进行决策 控制器（controller） 背景 一旦选择了合适的控制器，DispatcherServlet会将请求发送给选中的控制器。 具体步骤 到了控制器，请求会卸下其负载（用户提交的信息） 耐心等待控制器处理这些信息 实际上，设计良好的控制器本身只处理很少甚至不处理工作，而是将业务逻辑委托给一个或多个服务对象进行处理 模型及逻辑视图名 背景 控制器完成逻辑处理后，通常会产生一些信息，这些信息需要返回给用户并在浏览器上显示，这些信息被称为model。 返回可能是model ,也可能HTML，发送一个视图（view）通常是JSP 具体步骤 模型数据打包，并标示出用于渲染输出的视图名，接下来会将请求连同模型和视图名发送回DispatcherServlet 视图解析器 背景 控制器返回的是视图名，而不是视图。因为返回视图名这只是一个逻辑名，通过这个名字来查找真正的视图，这样控制器就不会与特定的视图耦合在一起了 具体步骤 DispathcherServlet将会使用视图解析器（view resolver）来将逻辑视图名匹配一个特定的视图实现。不一定是JSP 视图 背景 DispatcherServlet已经知道由哪个视图渲染结果，请求的任务基本完成 具体步骤 视图将使用模型数据渲染输出， 响应 视图解析渲染后的输出，会通过响应对象传递给客户端 "},"spring/spring-in-action/SpringMVC/RequestMapping处理请求.html":{"url":"spring/spring-in-action/SpringMVC/RequestMapping处理请求.html","title":"RequestMapping处理请求","keywords":"","body":"RequestMapping处理请求2.传递模型数据到视图中RequestMapping处理请求 在Spring MVC 中，控制器只是方法上添加了@RequestMapping注解的类，这个注解申明了他们所要处理的请求 @Controller //声明为一个控制器 public class HomeController { @RequestMapping(value = \"/\",method = RequestMethod.GET)// 处理对\"/\"的get请求 public String home(){ return \"home\";//视图名为hone } } @Controller 注解 看起来是用来声明控制器的，但实际上这个注解对Spring MVC本身的影响并不大 是一个构造性（stereotype）的注解，他基于@Component注解，目的是辅助实现组件扫描 @RequestMapping value 属性：指定了这个方法所要处理的请求路径 method属性：细化了他要处理的HTTP方法 2.传递模型数据到视图中 Model是我们需要返回给用户的结果信息 Model实际上就是一个Map(也可以用map)，他会传递给视图，这样数据就能渲染到客户端 "},"spring/spring-in-action/SpringMVC/接收请求参数.html":{"url":"spring/spring-in-action/SpringMVC/接收请求参数.html","title":"接收请求参数","keywords":"","body":"接收请求参数1.处理查询参数2.通过路径参数接受输入3.处理表单接收请求参数 Spring MVC 接收参数的方式 查询参数（Query Parameter） 表单参数（Form Paramter） 路径参数（Path Variable） 1.处理查询参数 @RequestMapping(method = RequestMethod.GET) public List getData(@RequestParam(value = \"max\",defaultValue = \"100\") long max, @RequestParam(value = \"count\",defaultValue = \"20\") int count ){ ... return data; } 使用@RequestParam获取请求参数 value ：参数名，没有额外添加时可以不要value defaultValue：设置默认值 2.通过路径参数接受输入 从面向资源的角度，要识别的资源应该通过url路径进行标示，而不是参数 为了实现这种路径变量，Spring MVC允许我们在@RequestMapping路径中添加占位符，占位符的名称要用大括号（”{“和”}“）括起来 @RequestMapping(value = \"/{spittleId}\",method = RequestMethod.GET) public String spittle(@PathVariable(\"spittleId\") long spittleId){ ... } 当参数名与 @PathVariable注解的值相同时可以去掉 3.处理表单 使用对象来接收表单参数 @RequestMapping(value = \"/register\",method = RequestMethod.POST) public String processRegister(Spitter spitter){ ... } "},"spring/spring-in-action/SpringMVC/校验数据.html":{"url":"spring/spring-in-action/SpringMVC/校验数据.html","title":"校验数据","keywords":"","body":"校验数据使用实例校验数据 从Spring 3.0开始提供对Java校验API(Java Validation API,又称JSR-303)的支持 Java校验API所提供的校验注解(所有注解都在javax.validation.constraints 包下) 使用实例 Spitter 简单POJO public class Spitter { // 非空，5-16个字符 @NotNull @Size(min = 5,max = 16) private String username; } processRegister注册方法 @RequestMapping(value = \"/register\",method = RequestMethod.POST) public String processRegister(@Valid Spitter spitter, Errors errors){ if (errors.hasErrors()){ return \"registerForm\"; } ... } "},"spring/spring-in-action/SpringMVC/重定向与转发.html":{"url":"spring/spring-in-action/SpringMVC/重定向与转发.html","title":"重定向与转发","keywords":"","body":"重定向与转发重定向与转发 当InternalResourceViewResolver 看到视图格式中的”redirect：“前缀时，他知道要将其解析为重定向规则，而不是视图名称 forward:前缀。请求将前往（forward指定的路径） "},"spring/spring-in-action/SpringMVC/测试控制器.html":{"url":"spring/spring-in-action/SpringMVC/测试控制器.html","title":"测试控制器","keywords":"","body":"测试控制器测试控制器 在Spring3.2 开始支持测试控制器 mock Spring MVC 针对控制器执行HTTP请求，在测试的控制器的时候，就没必要在启动Web服务器和Web浏览器 public class HomeControllerTest { @Test public void home() throws Exception { HomeController controller = new HomeController(); MockMvc mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); mockMvc.perform(MockMvcRequestBuilders.get(\"/\")) .andExpect(MockMvcResultMatchers.view().name(\"home\")); } } "},"spring/spring-in-action/viewResolver/":{"url":"spring/spring-in-action/viewResolver/","title":"渲染Web视图","keywords":"","body":"渲染Web视图Spring 是如何确定使用哪个视图实现渲染模型Spring MVC的ViewResolver解析流程Spring 自带的13个视图解析器渲染Web视图 控制器方法并没有直接产生浏览器中渲染所需的HTML controller 做的 将一些数据填充到模型中 然后将模型传递给一个用来渲染的视图 Spring 是如何确定使用哪个视图实现渲染模型 这就是视图解析器的作用 例如：InternalResureViewResolver视图解析器 ​ 使用\"WEB-INF/views/\"前缀和”jsp“后缀来确定渲染模型的JSP物理文件位置 Spring MVC的ViewResolver解析流程 Spring MVC定义了一个名为ViewResolver的接口 public interface ViewResolver { View resolveViewName(String var1, Locale var2) throws Exception; } 当给resolveViewName()方法传入一个视图名和Locale对象是，他会返回一个View 实例 public interface View { String getContentType(); void render(Map map, HttpServletRequest request, HttpServletResponse response) throws Exception; } View 接口的任务就是接受模型以及Servlet的request 和response对象，并将输出结果渲染到response中 如果我们要自定义ViewResolver 编写ViewResolver 和view 的实现，将要渲染的结果放到response中，进而展现到用户的浏览器中 Spring 自带的13个视图解析器 "},"spring/spring-in-action/RESTAPI/":{"url":"spring/spring-in-action/RESTAPI/","title":"REST API","keywords":"","body":"REST API1.REST API 是什么2.REST API 的特点33.REST API中的行为4.为什么要使用REST API数据为王REST API 1.REST API 是什么 以信息为中心的表述行状态转移（Representational Satte Transfer,REST) 表达性（Representational）：REST 资源实际上可以用各种形式来进行表达，包括XML，JSON,甚至HTML 状态（State）：当使用REST 的时候，我们更关注资源的状态而不是对资源采取的行为 转移（Transfer）：REST 涉及到转移资源数据，他以某种表达性形式从一个应用转移到另一个应用 传统的SOAP关注行为和处理，而REST 关注的是要处理的数据 2.REST API 的特点 在REST 中资源通过URL 来进行识别和定位，而结构并没有严格的规则， 但是URL应该能够识别资源，而不是简单的发一条命令到服务器上 关注的是事物（数据），而不是行为 33.REST API中的行为 他们通过Http 方法来定义的，也就是通过GET,POST,PUT,DELETE,PATCH构成REST API 的动作。 对应的CURD 操作 Create：post Read:GET Update:PUT或PATCH Delete：DELETE 4.为什么要使用REST API 数据为王 对于开发人员： ​ 关注与构建软件解决业务问题，数据只是软件完成工作时要处理的原材料 对于业务人员 ​ 数据是业务的生命之血，软件可替代，但积累的数据是永远无法替代的 "},"spring/spring-in-action/RESTAPI/表达性.html":{"url":"spring/spring-in-action/RESTAPI/表达性.html","title":"表达性","keywords":"","body":"表达性1.定义2.将资源转换为客户端的表述形式2.1 内容协商（Content negotiation）2.2 消息转换器（Message conversion）2.2.1 Spring 自带的各种转换器3.适配REST API 的调整3.1在响应体返回资源状态3.2 在请求体重接收资源状态3.2.1 @RequestMapping的consumes属性3.3 控制器设置默认转换表达性 1.定义 表达性（Representational）：REST 资源实际上可以用各种形式来进行表达，包括XML，JSON,甚至HTML 自己的理解 同一资源。你可以使用JSON表达，也可以使用XML，如果你在浏览器中也可以使用HTML。 这个资源时没有变化的，只是他的表达形式变了。这就是REST 的表达性 2.将资源转换为客户端的表述形式 内容协商（Content negotiation）： 选择一个视图，他能够将模型渲染为呈现给客户端的表达形式 消息转换器（Message conversion）： 通过一个消息转换器将控制器所返回的对象转换为呈现给客户端的表达形式 2.1 内容协商（Content negotiation） TODO 2.2 消息转换器（Message conversion） 更推荐使用此方式 当使用消息转换功能时，DispatcherServlet补在需要那么麻烦地将模型数据传递到视图中，实际上根本没有模型和视图，只有控制器产生的数据，以及消息转换器（message conversion）转换数据之后所产生的资源表达 2.2.1 Spring 自带的各种转换器 假如客户端请求的Accept头信息表明他能接受”application/json“并且 Jackson JSON 在类路径下，那么处理方法返回的对象将交给 MappingJacksonHttpMessageConverter,并由他转换为返回客户端的JSON表达形式 如果请求头信息表明客户端想要”text/xml“格式，那么Jaxb2RootElementHttpMessageConverter 将会为客户端产生xml响应 3.适配REST API 的调整 3.1在响应体返回资源状态 正常情况 返回java对象会放在模型中并在视图中渲染使用 使用了消息转换功能后 需要告诉Spring 跳过正常的模型/视图流程，并使用消息转换 最简单的方式使用@ResponseBody注解 3.1.1 @ResponseBody注解 会告诉Spring，我们将要返回的对象作为资源返回给客户端，并将其转换为客户端可接受的表述形式 DispatcherServlet 将会考虑请求中Accept 头部信息 并查找能为客户端提供所需表述形式的消息转换器 3.2 在请求体重接收资源状态 如何让控制器将客户端发送的JSON和XML 转换为他所使用的Java 对象？ 答案是：@RequestBody 他能告诉Spring 查找一个消息转换器，将来自客户端的资源表述转换为对象 Spring会查看请求中的Content-Type 头部信息 并查找能够将请求体转换为java 对象的消息转换器 3.2.1 @RequestMapping的consumes属性 我们将其设置为”application/json“,他会告诉Spring 这个方法方法只处理ContentType头部信息为”application/json“的请求，无法满足条件的话，将由其他方法来处理请求 3.3 控制器设置默认转换 @RestController 不必为每个方法添加@ResponseBody. "},"spring/spring-in-action/RESTAPI/异常处理并更改状态码.html":{"url":"spring/spring-in-action/RESTAPI/异常处理并更改状态码.html","title":"异常处理并更改状态码","keywords":"","body":"异常处理并更改状态码2.解决方案2.1异常处理并更改状态码 这里的异常，并不单单指的是报错的异常。同时也包含了某项业务的异常 例如： 查询某个id详情，却查询不到返回null。但是http 默认返回的是200，表示所有事情运行正常 2.解决方案 @SpringStatus 注解指定状态码 控制器方法返回ResponseEntity对象，该对象能够包含更太多响应相关的元数据 异常处理器能够对应错误场景，这样处理器方法就能关注于正常状况 2.1 "},"spring/spring-in-action/RESTAPI/在响应中设置头部信息.html":{"url":"spring/spring-in-action/RESTAPI/在响应中设置头部信息.html","title":"在响应中设置头部信息","keywords":"","body":"在响应中设置头部信息在响应中设置头部信息 创建一个HttpHeaders 实例，用来存放希望在响应中包含的头部信息值， HttpHeards 是MultiValueMap的特殊实现，他有一些便利的setter方法，来设置http头部信息 可以使用这个信息来创建ResponseEntity "},"spring/spring-in-action/NoSql/":{"url":"spring/spring-in-action/NoSql/","title":"NoSql","keywords":"","body":"NoSqlNoSql "},"spring/spring-in-action/NoSql/redis/":{"url":"spring/spring-in-action/NoSql/redis/","title":"redis","keywords":"","body":"redis1. 4种redis连接工厂2.RedisTemplate2.1 背景2.2 redis 模板redis redis 是一种特殊类型的数据库，被称为key-value存储，与hashmap有很大相似性 1. 4种redis连接工厂 redis连接工厂会生成到redis数据库服务器的连接 JedisConnectionFactory JredisConnectionFactory LettuceConnectionFactory SrpConnectionFactory 2.RedisTemplate 2.1 背景 redis 连接工厂会生成到redis key-value 存储的连接（redisConnection） 借助redisConnection 可以存储和读取数据 RedisConnectionFactory cf = null; RedisConnection conn =cf.getConnection(); conn.set(\"ss\".getBytes(),\"hello\".getBytes()); 但字节数组并不是我们希望看到的 2.2 redis 模板 Spring data redis提供了两个模板 RedisTemplate 不在局限于字节数据 StringRedisTemplate key和value 是string 类型 "},"spring/interview/Spring面试提问.html":{"url":"spring/interview/Spring面试提问.html","title":"Spring面试提问","keywords":"","body":"Spring面试提问1. 基础概念篇2. IoC篇3. Spring Bean篇3. AOP篇4. 事务篇5. SpringMVC篇6. Spring Boot篇Spring面试提问 作为面试官，关于Spring我常问的问题 1. 基础概念篇 看你各个项目都用到了Spring，说说你自己对Spring的理解？ 你对spring 的整体架构了解吗？说说都有哪些模块组成？ 按模块解释，并说出你常用的组件 那么Spring框架都用到了哪些设计模式？ 2. IoC篇 谈谈自己对Spring IoC的理解？ IoC 的初始化过程？ IoC容器有几种类型？他们的区别？ 有多少种方式完成依赖注入？他们的区别？ 简述一下Spring IoC的实现原理？ 说一说循环依赖问题？ 3. Spring Bean篇 Spring中bean 的作用域有哪些？ 说说Spring中单例Bean的线程安全问题? @Component 和@Bean的区别是什么？ Bean的生命周期 3. AOP篇 谈谈自己对Spring AOP的理解？ Spring AOP的原理？ Spring AOP和AspectJ AOP有什么区别？　 4. 事务篇 什么是事务 说说事务的特性？ 数据库是如何保证这些特性的呢？（数据库问题，非Spring） 并发事务可能带来哪些问题？ Spring事务的隔离级别有哪些？ @Transactional( rollbackFor = Exception.class) 注解 5. SpringMVC篇 说说\u0001Spring MVC的工作原理？ SpringMVC拦截器和filter过滤器有什么差别 6. Spring Boot篇 Spring boot 的自动配置是如何实现的？ 什么是嵌入式服务器？为什么要使用嵌入式服务器？ 微服务同时调用多个服务，怎么支持事务的？ "},"Mybatis/":{"url":"Mybatis/","title":"MyBatis","keywords":"","body":"MyBatisMyBatis Mybatis常见面试题 《MyBatis入门到精通》读书笔记 MyBatis简介 集成与使用 XML方式基本用法 select用法 Mapper接口动态代理实现原理 动态sql "},"Mybatis/interview/Mybatis常见面试题.html":{"url":"Mybatis/interview/Mybatis常见面试题.html","title":"Mybatis常见面试题","keywords":"","body":"Mybatis常见面试题1. 什么是Mybatis?2. 说一下Mybatis的优缺点3. Mybatis 的使用场合4. #{}和${} 的区别是什么？5. 当实体类中的属性名和表中的字段名不一样，怎么办5.1 方案1：通过在查询的sql语句中定义字段名的别名，让字段名和别名和实体类的属性名一致5.2 方案2：6. 通常一个Xml映射文件，都会有一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？7. 说一下resultMap 和 resultType8. 如何在mapper中如何传递多个参数？9. MyBatis动态sql有什么用？执行原理？有哪些动态sql？10. Mybatis的xml映射文件中，不同的Xml映射文件，id是否可以重复11. 一对一关联查询使用什么标签？一对多关联查询使用什么标签？12. 什么是Mybatis的一级、二级缓存，如何开启？13. 什么是Mybatis的接口绑定？有哪些实现方式？14. Mybaits 是如何进行分页的？分页插件的原理是什么？15. Mybatis 是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式15.1 方案1：标签15.2 方案2：使用sql列的别名16. 模糊查询like 语句该怎么写？16.1 方案1：在java代码中添加sql通配符16.2 方案2：在sql语句中拼接通配符、会引起sql 注入参考文章Mybatis常见面试题 1. 什么是Mybatis? Mybatis 是一个orm类型的半自动框架，执行了对JDBC的封装，是一个持久层框架，他可以通过XML文件或者注解来配置原生信息，不在需要去做更多繁琐重复的过程，如创建连接，加载驱动！ 2. 说一下Mybatis的优缺点 优点： 基于SQL语句编译，相当灵活，与JDBC相比，减少了50%的代码，很好的与各种数据库兼容，能够与Spring很好的集成，提供映射标签，支持对象关系组件维护 缺点： SQL语句的编写工作量较大，尤其字段多，关联表多时，对发开人员编写SQL语句的功底有一定要求！ SQL 语句依赖数据，导致数据库移植性差，不能随意更换数据库！ 3. Mybatis 的使用场合 Mysql 专注于SQL本身，是一个足够灵活的DAO层解决方案，对性能要求很高，或者需求变化较多的项目，如互联网项目！ 4. #{}和${} 的区别是什么？ #{} 是预编译处理，${} 是字符串替换 Mybatis 在处理#{} 时，会将sql中的#{} 替换为？号，调用PreparedStatement的set方法来赋值； Mybatis 在处理${}，就是把${}替换成变量的值 使用#{}可以有效的防止SQL注入，提高系统安全性 5. 当实体类中的属性名和表中的字段名不一样，怎么办 5.1 方案1：通过在查询的sql语句中定义字段名的别名，让字段名和别名和实体类的属性名一致 select order_id id, order_no orderno ,order_price price form orders where order_id=#{id}; 5.2 方案2： 通过来映射字段名和实体类属性名的一一对应关系 select * from orders where order_id=#{id} 6. 通常一个Xml映射文件，都会有一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？ Dao接口即Mapper接口，接口的全限名，就是映射文件的namespace的值，接口的方法名，就是映射文件中Mapper的Statement的id值，接口方法内的参数，就是传递个sql的参数！因为mapper接口是没有实现类的，所以在调用方法时，需要拿全限定路径名称加上方法名作为key值！ 不能重载 7. 说一下resultMap 和 resultType resultMap是手动提交、人为提交，resultType是自动提交 MyBatis中在查询进行select映射的时候，返回类型可以用resultType，也可以用resultMap，resultType是直接表示返回类型的，而resultMap则是对外部ResultMap的引用，但是resultType跟resultMap不能同时存在。 在Mybatis进行查询映射时，其实查询出来的每一个属性都是放在一个对应的Map里面的，其中键是属性名，值则是其对应的值 当提供的返回类型属性是resultType时，Mybatis会将Map里面的键值对取出赋给resultType所指定的对象对应的属性。所以其实Mybatis的每一个查询映射的返回类型都是ResultMap,只是当提供的返回类型属性是resultType的时候，Mybatis对自动的把对应的值赋给resultType所指定对象的属性 当提供的返回类型是resultMap时，因为Map不能很好表示领域模型，就需要自己再进一步把他转化为对应的对象，这常常在复杂查询中很有作用 8. 如何在mapper中如何传递多个参数？ 多个参数封装成map 映射文件的命名空间，SQL片段的id，就可以调用对应的映射文件的SQL 由于我们的参数超过2个，而方法只有一个Object参数搜集，因此我们使用Map集成来装载我们的参数 9. MyBatis动态sql有什么用？执行原理？有哪些动态sql？ 有九种动态sql标签：trim,where,set,foreach,if,choose,when,bind,otherwise 作用：动态sql可以在xml映射文件内，以便签的形式编写动态sql 执行原理：根据表达式的值，完成逻辑判断并动态拼接sql的功能 10. Mybatis的xml映射文件中，不同的Xml映射文件，id是否可以重复 看不同情况对待，不同的xml配置文件 如果配置了namespace，那么id可以重复 如果没有配置namespace，那么id不能重复 11. 一对一关联查询使用什么标签？一对多关联查询使用什么标签？ 一对多标签：collection MyBatis 中使用collection标签来解决一对多的关联查询 一对一标签：association 使用association标签来解决一对一的关联查询 12. 什么是Mybatis的一级、二级缓存，如何开启？ 一级缓存 一级缓存是基于PerpetualCache的hashmap本地缓存，其存储作用域为session，当Session flush后，默认开启一级缓存 二级缓存： 二级缓存和一级缓存的机制是相同的，默认也是采用PerpetualCache和hashmap本地缓存，不过他的存储作用在Mapper，而且可自定义存储源。要开启二级缓存，需要使用二级缓存属性类实现Serializable序列化的接口，可在他的映射文件中配置 缓存数据的更新机制，当某一个作用域（一级缓存session/二级缓存namespace）的进行了c/u/d操作后，默认该作用域下的所有select中的缓存将被clear 13. 什么是Mybatis的接口绑定？有哪些实现方式？ 接口绑定就是在mybatis中任意定义接口，然后把接口里面的方法和sql语句绑定，我们直接调用接口方法就可以，这样比原来sqlsession提供的方法我们可以更加灵活的选择和设置 两种实现方式 在接口的方法上面加上@select、@update等注解，里面包含sql语句来绑定 通过xml里面写sql语句来绑定，在这种情况下，要指定xml映射文件里面的namespace必须为接口的全路径名， 当sql语句比较简单的时候，用注解绑定，当sql语句比较复杂的时候，用下xml绑定，一般使用xml绑定的多 14. Mybaits 是如何进行分页的？分页插件的原理是什么？ Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。 15. Mybatis 是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式 15.1 方案1：标签 使用标签，逐一定义列名和对象属性名之间的映射关系 15.2 方案2：使用sql列的别名 使用sql列的别名功能，将列别名书写为对象的属性名，比如T_NAME AS NAME，对象属性名一般是name，小写，但是列名不区分大小写，Mybatis会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成T_NAME AS NaMe，Mybatis一样可以正常工作。 16. 模糊查询like 语句该怎么写？ 16.1 方案1：在java代码中添加sql通配符 string wildcardname = “%smi%”; list names = mapper.selectlike(wildcardname); select * from foo where bar like #{value} 16.2 方案2：在sql语句中拼接通配符、会引起sql 注入 string wildcardname = “smi”; list names = mapper.selectlike(wildcardname); select * from foo where bar like \"%\"#{value}\"%\" 参考文章 Mybatis 的常见面试题 复习Mybatis框架，面试题 "},"Mybatis/book/":{"url":"Mybatis/book/","title":"MyBatis简介","keywords":"","body":"MyBatis1. 简介2. MyBatis 的优势2.1 与其他ORM框架的优势2.2 与JDBC 的优势2.3 支持声明式数据缓存（declarative data caching）MyBatis 1. 简介 mybatis是一款优秀的持久层框架 支持自定义SQL查询，存储过程和高级映射 消除了几乎所有的 JDBC 代码和参数的手动设置以及结果集的检索 Mybatis 可以使用XML或注解进行配置和映射 Mybatis通过将参数映射到配置的SQL形成最终执行的sql语句，最后将执行SQL的结果映射成 Java 对象返回 2. MyBatis 的优势 2.1 与其他ORM框架的优势 Mybatis将 java方法与sql关联，而不是数据库 并没有将java 对象与数据库关联起来，而是将java方法与sql语句关联 Mybatis允许用户充分利用数据库的各种功能， 例如存储过程，视图，各种复杂的查询以及某数据库的特有特性 可以完全控制SQL执行 2.2 与JDBC 的优势 简化了相关代码 SQL语句在一行代码中能执行 声明式的sql语句 Mybatis提供了一个映射引擎，声明式的sql语句的执行结果与对象树映射起来 sql可以被动态生成 通过使用内建的类XML表达式语言，sql语句可以被动态生成 2.3 支持声明式数据缓存（declarative data caching） 当一条sql语句被标记为“可缓存”后， 首次执行他时从数据库获取的所有数据会被存储在高速缓存中， 后面再执行这条语句时就会从高速缓存中读取结果，而不是再次命中数据库 Mybatis提供了默认情况下基于java hashMap的缓存实现 "},"Mybatis/集成与使用.html":{"url":"Mybatis/集成与使用.html","title":"集成与使用","keywords":"","body":"集成与使用1. mybatis集成1.1 新建SpringBoot项目1.2 在pom文件中添加依赖1.3 在 application.yml配置datasource2.mybatis-generator 集成2.1 pom文件下添加插件2.2 添加配置文件2.3 生成mapping 文件集成与使用 1. mybatis集成 1.1 新建SpringBoot项目 1.2 在pom文件中添加依赖 org.mybatis.spring.boot mybatis-spring-boot-starter 2.1.0 mysql mysql-connector-java runtime 1.3 在 application.yml配置datasource spring: datasource: username: root password: zsz123456 driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://120.79.200.111:3306/mybatisdemo?useUnicode=true&characterEncoding=UTF-8&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC 2.mybatis-generator 集成 2.1 pom文件下添加插件 org.mybatis.generator mybatis-generator-maven-plugin 1.3.5 src/main/resources/config/mybatis-generator.xml true true 2.2 添加配置文件 在resource的config目录下添加 mybatis-generator.xml文件 2.3 生成mapping 文件 在idea 的右边mavenprojects 中的plugs中找到mybatis-generator 点击mybatis-generator就生成了 "},"Mybatis/xml/":{"url":"Mybatis/xml/","title":"XML方式基本用法","keywords":"","body":"XML方式基本用法1. 调用方式1.1 使用SqlSession 通过命名空间调用Mybatis方法1.2 使用接口调用方法（mybatis3.0支持）XML方式基本用法 MyBatis 的真正强大之处在于他的映射语句，所以映射器的XML文件就显得相对简单。 同样功能，相比jdbc减少了95%的代码量 1. 调用方式 1.1 使用SqlSession 通过命名空间调用Mybatis方法 SqlSession 通过命名空间调用Mybatis方法 首先需要用到命名空间和方法id组成的字符串来调用相应的方法 当参数多于1个的时候，需要将参数方法一个map对象中 通过map 传递多个参数 使用起来不方便，无法避免很多重复代码 1.2 使用接口调用方法（mybatis3.0支持） 使用接口调用方法会方便很多 Mybatis 使用Java的动态代理可以直接通过接口来调用相应的方法 不需要提供接口的实现类，更不需要在实现类中使用sqlSession以通过命名空间间接调用 当多个参数的时候，通过参数注解@Param设置参数的名字 省去了手动构造map参数的过程 在spring中使用的时候，可以配置为自动扫描所有的接口类，直接将接口注入需要用到的地方 "},"Mybatis/xml/select.html":{"url":"Mybatis/xml/select.html","title":"select用法","keywords":"","body":"select用法1. 案例2.接口中的方法和XML怎么关联的3. 标签与属性作用4. resultMap 标签4.1 属性4.2 标签5.id 和result 标签包含的属性select用法 1. 案例 UserMapper接口 public interface UserMapper { /** * 通过id 查询用户 * @param id * @return */ User selectById(Long id); } UserMapper.xml SELECT * FROM sys_user WHERE id = #{id} 2.接口中的方法和XML怎么关联的 XML中的select标签的id属性值和定义的接口方法名是一样的。 规则 当只使用XML 而不使用接口的时候，namespace 可以设置为任意不重复名称 标签的id属性值在任何时候都不能出现英文“.”,并且同一命名空间下不能出现重复的id 因为接口的方法是可以重载的，所以接口中可以出现多个同名参数名不同的方法，但是xml中id不能重复。因而接口中的所有同名方法会对应这xml中的同一个id的方法。 3. 标签与属性作用 : 映射查询语句使用的标签 id: 命名空间中的唯一标识符，可以用来代表这条语句 resultMap： 用于设置返回值的类型和映射关系 select标签中的select * from 是查询语句 #{id} Mybatis sql中预编译参数类型的一种方式，大括号中的id是传入的参数名 4. resultMap 标签 resultMap 标签用于配置java 对象的属性和查询结果列的对应关系，通过resultMap中配置的column 和property可以将查询列的值映射到type对象的属性上，因此当我们使用select * 查询所有列的时候，Mybatis 也可以将结果正确的映射到User 对象上 4.1 属性 id：必填。并且唯一。在select 标签中，resultMap指定的值即为此处的id所设置的值 type：必填，用于配置查询列所映射到的java对象类型 extends：选填，可以配置当前的resultMap 继承自其它的resultMap，属性值为继承resultMap的id autoMapping:选填，可选值为true或false，用于配置是否启用非映射字段（没有在resultMap 中配置的字段）的自动映射功能，该配置可以覆盖全局的autoMappingBehavior 配置 4.2 标签 constructor：配置使用构造方法注入结果，包含以下两个子标签 idArg:id参数，标记结果作为id（唯一值），可以帮助提高整体性能 arg: 注入到构造方法的一个普通结果 id：一个id结果，标记结果作为id（唯一值），可以帮助提高整体性能 result: 注入到java对象属性的普通结果 association： 一个复杂的类型关联，许多结果将包成这种类型 collection: 复杂类型集合 discriminator:根据结果值来决定使用哪个结果映射 case: 基于某些值的结果映射 5.id 和result 标签包含的属性 column:从数据库中得到的列名，或者是列的别名 property:映射到列结果的属性， 可以映射简单的如“username”，这样的属性， 也可以映射一些复杂对象的属性，例如“address.street.number”,这会通过“.”方式的属性嵌套赋值 javaType：一个Java类的完全限定名，或一个类型的别名（通过typeAlias配置或者默认的类型）。 如果映射到一个JavaBean，mybatis 通常可以自动判断类型， 如映射到HashMap则需要明确指定javaType类型 jdbcType：列对应的数据库类型 jdbc类型仅仅需要对插入、更新、删除操作可能为空的列进行处理（这是jdbc jdbcType要求，而不是mybatis） typeHandler：使用这个属性可以覆盖默认的类型处理器， ​ "},"Mybatis/Mapper接口动态代理实现原理.html":{"url":"Mybatis/Mapper接口动态代理实现原理.html","title":"Mapper接口动态代理实现原理","keywords":"","body":"Mapper接口动态代理实现原理Mapper接口动态代理实现原理 为什么Mapper 接口没有实现类，却能被正常调用呢？ 因为Mybatis在mapper接口上使用了动态代理的一种常规用法 假设有一个mapper接口 public interface UserMapper { List selectAll(); } 使用java动态代理方式创建一个代理类 public class MyMapperProxy implements InvocationHandler { private Class mapperInterface; private SqlSession sqlSession; public MyMapperProxy(Class mapperInterface, SqlSession sqlSession) { this.mapperInterface = mapperInterface; this.sqlSession = sqlSession; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 根据不同的sql类型，需要调用sqlSession的不同方法 // 接口方法中的参数可能有很多种情况，这里只考虑没有参数的情况 List list = sqlSession.selectList(mapperInterface.getCanonicalName() + \".\" + method.getName()); // 返回值也有很多情况，这里不做处理直接返回 return list; } } 测试代码 // 获取sqlSession SqlSession sqlSession = getSqlSession(); // 获取UserMapper 接口 MyMapperProxy userMapperMyMapperProxy = new MyMapperProxy(UserMapper.class,sqlSession); UserMapper userMapper = (UserMapper) Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), new Class[]{UserMapper.class}, userMapperMyMapperProxy); // 调用selectAll方法 List users = userMapper.selectAll(); 从这个代理类可以看到，当调用一个接口的方法是，会先通过接口的全限定名称和当前调用的方法名的组合得到一个方法id，这个id 的值就是映射到xml中的namespace和具体方法id 的组合 所以可以在代理方法中使用sqlSession以命名空间的方式调用方法，通过这种方式可以将接口和xml文件中的方法关联起来 "},"Mybatis/dynamicsql/":{"url":"Mybatis/dynamicsql/","title":"动态sql","keywords":"","body":"动态sql2.动态sql支持的标签动态sql Mybatis 的强大特性之一就是他的动态sql jdbc或其他框架的缺点 需要手动添加空格 注意省略列名列表最后的都好 2.动态sql支持的标签 if choose（when，otherwise） trim（where，set） foreach bind "},"Mybatis/dynamicsql/if.html":{"url":"Mybatis/dynamicsql/if.html","title":"if用法","keywords":"","body":"if用法1.1 在where条件中使用ifif用法 if 标签通常用于Where语句中，通过判断参数值来决定是否使用某个查询条件，他也经常用于Update语句中判断是否更新某一字段，还可以再Insert语句中用来判断是否插入某个字段的值 1.1 在where条件中使用if "},"redis/":{"url":"redis/","title":"Redis","keywords":"","body":"RedisRedis redis面试考点 redis事务 redis使用场景 Redis缓存场景 hash场景 Redis修改局部信息场景，如用户信息（hash,） list场景 Redis用作消息队列(list) Redis最新内容 （list） set场景 共同好友列表 (set) zset场景 Redis排行榜场景（zset） Redis热门服务场景(zset) Redis在线人数场景（zset） redis可能出现的问题 Redis缓存雪崩 Redis缓存穿透 如何解决 Redis 的并发竞争 Key 问题 Redis缓存预热 Redis保证缓存与数据库双写时的数据一致性 redis分布式锁 一步步实现单机redis的分布式锁（setnx） Redlock分布式锁 读书笔记 Redis简介 5种数据结构 String字符串 List列表 Set集合 Hash散列 Zset有序集合 发布与订阅(pub/sub) 排序SORT 事务 键的过期时间 持久化 快照(snapshotting) 只追加文件(append-only file,AOF) 安装 Redis安装 开启远程访问 Redis使用 Spring Boot集成redis使用 redis连接客户端选择：Jedis,Redisson,Lettuce JedisPool资源池优化 RedisUtil工具类的使用(仅供参考) "},"redis/interview/":{"url":"redis/interview/","title":"redis面试考点","keywords":"","body":"redis面试考点1. redis 简介2. 为什么要用redis/为什么要用缓存3. 为什么要用redis 而不用 map/guava 做缓存4. redis的线程模型4.1 线程模型的处理流程5. redis 单线程为什么效率也这么高6. 为什么Redis 是单线程的6.1 官方答案7. redis 和memcached的区别8. redis常见数据结构及使用场景分析9. redis 设置过期时间10. redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?)11. redis 持久化机制（怎么保证 redis 挂掉之后再重启数据可以进行恢复）11.1 快照（snapshotting）持久化（RDB）11.2 AOF（append-only file）持久化11.3 Redis 4.0 对于持久化机制的优化11.4 补充内容：AOF重写redis面试考点 1. redis 简介 redis 是一个基于内存的高性能key-value数据库 读写速度非常快，因此redis被广泛应用于缓存方向 redis提供了多种数据类型来支持不同的业务场景 与传统数据库不同的是redis的数据是存在内存中的，所以读写速度非常快，因此redis被广泛应用于缓存方向 另外，redis也经常用来做分布式锁 除此之外，redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 2. 为什么要用redis/为什么要用缓存 主要从“高性能”和“高并发”这两点来看待这个问题 高性能 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 3. 为什么要用redis 而不用 map/guava 做缓存 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 4. redis的线程模型 单线程的IO 多路复用线程模型 为什么是单线程：最终做事的是文件事件处理，他是单线程的 IO多路复用是干嘛的？：监听多个Socket,并将socket产生的事件放入队列 redis内部使用文件事件处理器file event handler,这个文件事件处理器是单线程的，所以redis才叫做单线程的模型。它采用 单线程的IO 多路复用机制同时监听多个socket。根据socket上的事件来选择对应的事件处理器进行处理 文件事件处理器的结构包含4个部分 多个Socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器，命令请求处理器，命令回复处理器） 4.1 线程模型的处理流程 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件， IO 多路复用程序会监听多个 socket，会将 socket 产生的事件放入队列中排队， 事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。 5. redis 单线程为什么效率也这么高 纯内存操作，内存的读写速度非常快 核心是基于非阻塞的IO 多路复用机制 单线程，省去了很多上下文切换的时间 6. 为什么Redis 是单线程的 6.1 官方答案 因为redis 是基于内存的操作，cpu不是redis的瓶颈，redis 的瓶颈最多可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且cpu不会称为性能瓶颈，那就顺理成章采用单线程方案 7. redis 和memcached的区别 redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的. Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。 8. redis常见数据结构及使用场景分析 8.1 String 常用命令: set,get,decr,incr,mget 等。 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等。 8.2 Hash 常用命令： hget,hset,hgetall 等。 hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。比如下面我就用 hash 类型存放了我本人的一些信息： key=JavaUser293847 value={ “id”: 1, “name”: “SnailClimb”, “age”: 22, “location”: “Wuhan, Hubei” } 8.3 List 常用命令: lpush,rpush,lpop,rpop,lrange等 list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。 Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。 8.4 Set 常用命令： sadd,spop,smembers,sunion 等 set 的是通过 hash table 实现的， 所以添加，删除，查找的复杂度都是 O(1) set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。 当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。 比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下： sinterstore key1 key2 key3 将交集存在key1内 8.5 Sorted Set 常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。 举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息，适合使用 Redis 中的 Sorted Set 结构进行存储。 9. redis 设置过期时间 Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。 10. redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?) redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 4.0版本后增加以下两种： volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key 11. redis 持久化机制（怎么保证 redis 挂掉之后再重启数据可以进行恢复） 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 11.1 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 11.2 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 11.3 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 11.4 补充内容：AOF重写 AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。 AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作 "},"redis/interview/redis事务.html":{"url":"redis/interview/redis事务.html","title":"redis事务","keywords":"","body":"redis事务redis事务 "},"redis/scene/redis使用场景.html":{"url":"redis/scene/redis使用场景.html","title":"Redis缓存场景","keywords":"","body":"Redis缓存场景1. 缓存加载时机2. 缓存使用2.1 CacheService 接口2.2 CacheServiceImpl 实现类2.3 RedisUtil 工具类2.3 MyConstant 定义前缀2.4 UserManager 定义管理类Redis缓存场景 1. 缓存加载时机 当应用初始化完成时加载 缓存数据更新时加载 2. 缓存使用 2.1 CacheService 接口 定义缓存接口 public interface CacheService { /** * 从缓存中获取用户 * * @param username 用户名 * @return User */ User getUser(String username) ; /** * 缓存用户信息 * * @param username 用户名 */ void saveUser(String username); /** * 删除用户信息 * * @param username 用户名 */ void deleteUser(String username) throws Exception; } 2.2 CacheServiceImpl 实现类 @Service public class CacheServiceImpl implements CacheService { @Autowired RedisUtil redisUtil; @Autowired private UserService userService; @Override public User getUser(String username) { User user = (User) redisUtil.getObject(MyConstant.USER_CACHE_PREFIX+username); return user; } @Override public void saveUser(String username) { User user = userService.findByName(username); this.deleteUser(username); redisUtil.set(MyConstant.USER_CACHE_PREFIX+username,user); } @Override public void deleteUser(String username) { username = username.toLowerCase(); redisUtil.delete(MyConstant.USER_CACHE_PREFIX+username); } } 2.3 RedisUtil 工具类 @Component public class RedisUtil { @Autowired private RedisTemplate redisTemplate; /** * 设置 String 类型 key-value * * @param key * @param value */ public void set(String key, Object value) { redisTemplate.opsForValue().set(key, value); } /** * 获取 String 类型 key-value * * @param key * @return */ public Object getObject(String key) { return redisTemplate.opsForValue().get(key); } /** * 获取 String 类型 key-value * * @param key * @return */ public String get(String key) { return (String) redisTemplate.opsForValue().get(key); } /** * 删除key * * @param key */ public void delete(String key) { redisTemplate.delete(key); } } 2.3 MyConstant 定义前缀 public interface MyConstant { String USER_CACHE_PREFIX = \"app.platform.cache.user.\"; } 2.4 UserManager 定义管理类 package com.zszdevelop.springredisdemo.manage; import com.zszdevelop.springredisdemo.domain.User; import com.zszdevelop.springredisdemo.function.CacheSelector; import com.zszdevelop.springredisdemo.service.CacheService; import com.zszdevelop.springredisdemo.service.UserService; import com.zszdevelop.springredisdemo.utils.AppUtils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; /** * 封装一些和 User相关的业务操作 * @author zhangshengzhong * @date 2019/10/5 */ @Service public class UserManager { @Autowired private CacheService cacheService; @Autowired private UserService userService; /** * 通过用户名获取用户基本信息 * * @param username 用户名 * @return 用户基本信息 */ public User getUser(String username) { User user = cacheService.getUser(username); if (user == null){ user = this.userService.findByName(username) } return user; } } "},"redis/scene/Redis修改局部信息场景.html":{"url":"redis/scene/Redis修改局部信息场景.html","title":"Redis修改局部信息场景，如用户信息（hash,）","keywords":"","body":"Redis修改局部信息场景，如用户信息（hash,）1. 应用场景2. 示例Redis修改局部信息场景，如用户信息（hash,） 1. 应用场景 我们要存储一个用户对象数据，包含如下信息：姓名，年龄、身份证信息 第一种方式：将id作为key，其他信息封装成一个对象以序列化存储 缺点：1. 增加了序列化/反序列化的开销 2. 修改一项，需要把整个对象取回 3. 并且修改操作需要对并发进行保护，引入CAS等复杂问题 第二种方式：用户id+对应属性的名称作为唯一标识来取得对应属性 虽然省去了序列化开销和并发问题，但是id为重复存储，如果存在大量这样的数据，内存房费 第三种方式：Redis 的hash hash 实际是内部存储的value为一个 HashMap,并提供直接存取这个Map成员的接口 也就是说，key 任然是用户id，vulue 是一个map，这个map 的key 是成员的属性名，value是属性值。这样对数据的修改和存取都可以直接通过其内部map 的key。(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。 2. 示例 User user = new User(10001,\"zsz\",18); HashMap map = new HashMap<>(); map.put(\"username\",user.getUsername()); map.put(\"age\",user.getAge()); String UserHashKey = \"userInfo\" + user.getUserId(); redisUtil.hPutAll(UserHashKey,map); Map userInfo = redisUtil.hGetAll(UserHashKey); System.out.println(userInfo); "},"redis/scene/Redis用作消息队列.html":{"url":"redis/scene/Redis用作消息队列.html","title":"Redis用作消息队列(list)","keywords":"","body":"Redis用作消息队列(list)Redis用作消息队列(list) redis 的 list 数据类型对于大部分使用者来说，是实现队列服务的最经济，最简单的方式。 "},"redis/scene/Redis最新内容.html":{"url":"redis/scene/Redis最新内容.html","title":"Redis最新内容 （list）","keywords":"","body":"Redis最新内容 （list）Redis最新内容 （list） 因为 list 结构的数据查询两端附近的数据性能非常好，所以适合一些需要获取最新数据的场景，比如新闻类应用的 “最近新闻”。 "},"redis/scene/共同好友列表.html":{"url":"redis/scene/共同好友列表.html","title":"共同好友列表 (set)","keywords":"","body":"共同好友列表 (set)1. set 的特性2. 应用场景共同好友列表 (set) 1. set 的特性 　set 数据类型是一个集合（没有排序，不重复），可以对 set 类型的数据进行添加、删除、判断是否存在等操作（时间复杂度是 O(1) ） 　　set 集合不允许数据重复，如果添加的数据在 set 中已经存在，将只保留一份。 　　set 类型提供了多个 set 之间的聚合运算，如求交集、并集、补集，这些操作在 redis 内部完成，效率很高。 2. 应用场景 set 类型的特点是——不重复且无序的一组数据，并且具有丰富的计算功能，在一些特定的场景中可以高效的解决一般关系型数据库不方便做的工作。 1. “共同好友列表” 　　社交类应用中，获取两个人或多个人的共同好友，两个人或多个人共同关注的微博这样类似的功能，用 MySQL 的话操作很复杂，可以把每个人的好友 id 存到集合中，获取共同好友的操作就可以简单到一个取交集的命令就搞定。 // 这里为了方便阅读，把 id 替换成姓名 sadd user:wade james melo paul kobe sadd user:james wade melo paul kobe sadd user:paul wade james melo kobe sadd user:melo wade james paul kobe // 获取 wade 和 james 的共同好友 sinter user:wade user:james /* 输出： * 1) \"kobe\" * 2) \"paul\" * 3) \"melo\" */ // 获取香蕉四兄弟的共同好友 sinter user:wade user:james user:paul user:melo /* 输出： * 1) \"kobe\" */ /* 类似的需求还有很多 , 必须把每个标签下的文章 id 存到集合中，可以很容易的求出几个不同标签下的共同文章； 把每个人的爱好存到集合中，可以很容易的求出几个人的共同爱好。 "},"redis/scene/Redis排行榜场景.html":{"url":"redis/scene/Redis排行榜场景.html","title":"Redis排行榜场景（zset）","keywords":"","body":"Redis排行榜场景（zset）1. 方案设计1.1 业务场景说明1.2 数据结构1.3 redis使用方案2. 功能实现2.1 前提准备2.2 用户上传积分2.3 获取个人排名2.4 获取个人周边用户积分及排行信息2.5 获取topn排行3. 测试4. 小结参考文章Redis排行榜场景（zset） 在一些游戏和活动中，当涉及到社交元素的时候，排行榜可以说是一个很常见的需求场景了，就我们通常见到的排行榜而言，会提供以下基本功能 全球榜单，对所有用户根据积分进行排名，并在榜单上展示前多少 个人排名，用户查询自己所在榜单的位置，并获知周边小伙伴的积分，方便自己比较和超越 实时更新，用户的积分实时更改，榜单也需要实时更新 上面可以说是一个排行榜需要实现的几个基本要素了，正好我们刚讲到了redis这一节，本篇则开始实战，详细描述如何借助redis来实现一份全球排行榜 1. 方案设计 在进行方案设计之前，先模拟一个真实的应用场景，然后进行辅助设计与实现 1.1 业务场景说明 以前一段时间特别&#x1F525;的跳一跳这个小游戏进行说明，假设我们这个游戏用户遍布全球，因此我们要设计一个全球的榜单，每个玩家都会根据自己的战绩在排行榜中获取一个排名，我们需要支持全球榜单的查询，自己排位的查询这两种最基本的查询场景；此外当我的分数比上一次的高时，我需要更新我的积分，重新获得我的排名； 此外也会有一些高级的统计，比如哪个分段的人数最多，什么分段是瓶颈点，再根据地理位置计算平均分等等 本篇博文主要内容将放在排行榜的设计与实现上；至于高级的功能实现，后续有机会再说 1.2 数据结构 因为排行榜的功能比较简单了，也不需要什么复杂的结构设计，也没有什么复杂的交互，因此我们需要确认的无非就是数据结构 + 存储单元 存储单元 表示排行榜中每一位上应该持有的信息，一个最简单的如下 // 用来表明具体的用户 long userId; // 用户在排行榜上的排名 long rank; // 用户的历史最高积分，也就是排行榜上的积分 long score; 数据结构 排行榜，一般而言都是连续的，借此我们可以联想到一个合适的数据结构LinkedList，好处在于排名变动时，不需要数组的拷贝 上图演示，当一个用户积分改变时，需要向前遍历找到合适的位置，插入并获取新的排名, 在更新和插入时，相比较于ArrayList要好很多，但依然有以下几个缺陷 问题1：用户如何获取自己的排名？ 使用LinkedList在更新插入和删除的带来优势之外，在随机获取元素的支持会差一点，最差的情况就是从头到尾进行扫描 问题2：并发支持的问题？ 当有多个用户同时更新score时，并发的更新排名问题就比较突出了，当然可以使用jdk中类似写时拷贝数组的方案 上面是我们自己来实现这个数据结构时，会遇到的一些问题，当然我们的主题是借助redis来实现排行榜，下面则来看下，利用redis可以怎么简单的支持我们的需求场景 1.3 redis使用方案 这里主要使用的是redis的ZSET数据结构，带权重的集合，下面分析一下可能性 set: 集合确保里面元素的唯一性 权重：这个可以看做我们的score，这样每个元素都有一个score； zset：根据score进行排序的集合 从zset的特性来看，我们每个用户的积分，丢到zset中，就是一个带权重的元素，而且是已经排好序的了，只需要获取元素对应的index，就是我们预期的排名 2. 功能实现 我们主要是借助zset提供的一些方法来实现排行榜的需求，下面的具体方法设计中，也会有相关说明 2.1 前提准备 首先准备好redis环境，spring项目搭建好，然后配置好redisTemplate @Configuration public class RedisConfig { @Bean(name = \"redisTemplate\") public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate redisTemplate = new RedisTemplate<>(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize 替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和 key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; } } 2.2 用户上传积分 上传用户积分，然而zset中有一点需要注意的是其排行是根据score进行升序排列，这个就和我们实际的情况不太一样了；为了和实际情况一致，可以将score取反；另外一个就是排行默认是从0开始的，这个与我们的实际也不太一样，需要+1 /** * 更新用户积分，并获取最新的个人所在排行榜信息 * * @param userId * @param score * @return */ public RankDO updateRank(Long userId, Float score) { // 因为zset默认积分小的在前面，所以我们对score进行取反，这样用户的积分越大，对应的score越小，排名越高 redisUtil.zAdd(RANK_PREFIX,String.valueOf(userId), -score); Long rank = redisUtil.zRank(RANK_PREFIX, String.valueOf(userId)); return new RankDO(userId,rank + 1, score); } 2.3 获取个人排名 获取个人排行信息，主要就是两个一个是排名一个是积分；需要注意的是当用户没有积分时（即没有上榜时），需要额外处理 /** * 获取用户的排行榜位置 * * @param userId * @return */ @Override public RankDO getRank(Long userId) { // 获取排行， 因为默认是0为开头，因此实际的排名需要+1 Long rank = redisUtil.zRank(RANK_PREFIX, String.valueOf(userId)); if (rank == null) { // 没有排行时，直接返回一个默认的 return new RankDO(-1L, userId,0F); } // 获取积分 Double score = redisUtil.zScore(RANK_PREFIX, String.valueOf(userId)); return new RankDO(userId,rank + 1, Math.abs(score.floatValue())); } 2.4 获取个人周边用户积分及排行信息 首先获取用户的个人排名，然后查询固定排名段的数据即可 /** * 获取用户所在排行榜的位置，以及排行榜中其前后n个用户的排行信息 * * @param userId * @param n * @return */ @Override public List getRankAroundUser(Long userId, int n) { // 首先是获取用户对应的排名 RankDO rank = getRank(userId); if (rank.getRank() > result = redisUtil.zRangeWithScores(RANK_PREFIX, Math.max(0, rank.getRank() - n - 1), rank.getRank() + n - 1); return buildRedisRankToBizDO(result, rank.getRank() - n); } private List buildRedisRankToBizDO(Set> result, long offset) { List rankList = new ArrayList<>(result.size()); long rank = offset; for (ZSetOperations.TypedTuple sub : result) { String userId = (String) sub.getValue(); rankList.add(new RankDO( Long.parseLong(userId),rank++, Math.abs(sub.getScore().floatValue()))); } return rankList; } 看下上面的实现，获取用户排名之后，就可以计算要查询的排名范围[Math.max(0, rank.getRank() - n - 1), rank.getRank() + n - 1] 其次需要注意的如何将返回的结果进行封装，上面写了个转换类，主要起始排行榜信息 2.5 获取topn排行 /** * 获取前n名的排行榜数据 * * @param n * @return */ @Override public List getTopNRanks(int n) { Set> result = redisUtil.zRangeWithScores(RANK_PREFIX, 0, n - 1); return buildRedisRankToBizDO(result, 1); } 3. 测试 首先准备一个测试脚本，批量的插入一下积分，用于后续的查询更新使用 public class RankInitTest { private Random random; private RestTemplate restTemplate; @Before public void init() { random = new Random(); restTemplate = new RestTemplate(); } private int genUserId() { return random.nextInt(1024); } private double genScore() { return random.nextDouble() * 100; } @Test public void testInitRank() { for (int i = 0; i 4. 小结 上面利用redis的zset实现了排行榜的基本功能，主要借助下面三个方法 range 获取范围排行信息 score 获取对应的score range 获取对应的排名 虽然实现了基本功能，但是问题还是有不少的 上面的实现，redis的复合操作，原子性问题 由原子性问题导致并发安全问题 性能怎么样需要测试 参考文章 SpringBoot应用篇之借助Redis实现排行榜功能 "},"redis/scene/Redis热门服务场景.html":{"url":"redis/scene/Redis热门服务场景.html","title":"Redis热门服务场景(zset)","keywords":"","body":"Redis热门服务场景(zset)Redis热门服务场景(zset) "},"redis/scene/Redis在线人数场景.html":{"url":"redis/scene/Redis在线人数场景.html","title":"Redis在线人数场景（zset）","keywords":"","body":"Redis在线人数场景（zset）2. 具体实现2.1 LoginControllerRedis在线人数场景（zset） 在登录的时候将在线人数和过期时间记录下来 退出或踢出时，将在线信息与token 信息去除 获取在线人数，将未过期的全部用户列出 2. 具体实现 2.1 LoginController package com.zszdevelop.springredisdemo.controller; import com.zszdevelop.springredisdemo.MyConstant; import com.zszdevelop.springredisdemo.domain.ActiveUser; import com.zszdevelop.springredisdemo.domain.JWTToken; import com.zszdevelop.springredisdemo.domain.User; import com.zszdevelop.springredisdemo.service.UserService; import com.zszdevelop.springredisdemo.utils.RedisUtil; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.util.StringUtils; import org.springframework.validation.annotation.Validated; import org.springframework.web.bind.annotation.*; import javax.servlet.http.HttpServletRequest; import javax.validation.constraints.NotBlank; import java.time.LocalDateTime; import java.util.Objects; import java.util.Set; /** * @author zhangshengzhong * @date 2019/10/6 */ @Validated @RestController @RequestMapping public class LoginController { @Autowired private UserService userService; @Autowired RedisUtil redisUtil; @PostMapping(\"/login\") public void login(@NotBlank(message = \"{required}\") String username, @NotBlank(message = \"{required}\") String password){ // TODO 登录逻辑 User user = this.userManager.getUser(username); // TODO 生成token String token = signToken(username, password); // 默认过期时间为1天 long expireTime = System.currentTimeMillis()+24*60*1000*1000; JWTToken jwtToken = new JWTToken(token, expireTime); saveTokenToRedis(user,jwtToken); } private void saveTokenToRedis(User user, JWTToken token) { String ip = \"TODO\"; // 构建在线用户 ActiveUser activeUser = new ActiveUser(); activeUser.setUsername(user.getUsername()); activeUser.setIp(ip); activeUser.setToken(token.getToken()); // zset 存储登录用户，score 为过期时间戳 this.redisUtil.zAdd(MyConstant.ACTIVE_USERS_ZSET_PREFIX, activeUser,token.getExipreAt()); } @GetMapping(\"logout/{id}\") public void logout(@NotBlank(message = \"{required}\") @PathVariable String id) throws Exception { this.kickout(id); } @DeleteMapping(\"kickout/{id}\") public void kickout(@NotBlank(message = \"{required}\") @PathVariable String id) throws Exception { long now = System.currentTimeMillis(); Set userOnlineStringSet = redisUtil.zRange(MyConstant.ACTIVE_USERS_ZSET_PREFIX, now, -1L); ActiveUser kickoutUser = null; Object kickoutUserString = null; for (Object userOnlineString : userOnlineStringSet) { ActiveUser activeUser = (ActiveUser) userOnlineString; if (Objects.equals(activeUser.getId(), id)) { kickoutUser = activeUser; kickoutUserString = userOnlineString; } } if (kickoutUser != null && StringUtils.isEmpty(kickoutUserString)) { // 删除 zset中的记录 redisUtil.zRemove(MyConstant.ACTIVE_USERS_ZSET_PREFIX, kickoutUserString); } } @GetMapping(\"online\") public Set userOnline(){ long now = System.currentTimeMillis(); Set userOnlineStringSet = redisUtil.zRange(MyConstant.ACTIVE_USERS_ZSET_PREFIX, now, -1L); return userOnlineStringSet; } } "},"redis/question/Redis缓存雪崩.html":{"url":"redis/question/Redis缓存雪崩.html","title":"Redis缓存雪崩","keywords":"","body":"Redis缓存雪崩1. 简介2. 解决办法Redis缓存雪崩 1. 简介 缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉 2. 解决办法 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略 事中：本地chcache缓存 + hystrix限流&降级，避免Mysql崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 "},"redis/question/Redis缓存穿透.html":{"url":"redis/question/Redis缓存穿透.html","title":"Redis缓存穿透","keywords":"","body":"Redis缓存穿透1. 简介2. 解决办法方案1：布隆过滤器方案2：返回数据为空也缓存Redis缓存穿透 1. 简介 一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉 2. 解决办法 方案1：布隆过滤器 将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉。从而避免了对底层存储系统的查询压力 方案2：返回数据为空也缓存 如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过5分钟 "},"redis/question/如何解决 Redis 的并发竞争 Key 问题.html":{"url":"redis/question/如何解决 Redis 的并发竞争 Key 问题.html","title":"如何解决 Redis 的并发竞争 Key 问题","keywords":"","body":"如何解决 Redis 的并发竞争 Key 问题1. 简介2.解决方案2.1 分布式锁如何解决 Redis 的并发竞争 Key 问题 1. 简介 所谓 redis 的并发竞争 key 的问题也就是多个系统同时对一个key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同 2.解决方案 2.1 分布式锁 分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 "},"redis/question/Redis缓存预热.html":{"url":"redis/question/Redis缓存预热.html","title":"Redis缓存预热","keywords":"","body":"Redis缓存预热1. 简介2.解决方案2.1 缓存刷新页面2.2 项目启动自动加载Redis缓存预热 1. 简介 缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统 这样可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 2.解决方案 2.1 缓存刷新页面 直接写个缓存刷新页面，上线时手工操作 2.2 项目启动自动加载 数据量不大，可以在项目启动的时候自动进行加载 目的就是在系统上线前，将数据加载到缓存中。 "},"redis/question/Redis保证缓存与数据库双写时的数据一致性.html":{"url":"redis/question/Redis保证缓存与数据库双写时的数据一致性.html","title":"Redis保证缓存与数据库双写时的数据一致性","keywords":"","body":"Redis保证缓存与数据库双写时的数据一致性1.引言2. 三种更新策略2.1 先更新数据库，再更新缓存2.2 先删缓存，在更新数据库（争议最大）2.3 先更新数据库，再删除缓存Redis保证缓存与数据库双写时的数据一致性 1.引言 在读取缓存方面的方案流程图 更新缓存方面 对于更新完数据库，是更新缓存，还是删除缓存，又或者是先删除缓存，再更新数据库，其实大家存在很大的争议。 2. 三种更新策略 先更新数据库，再更新缓存 先删除缓存，再更新数据库 先更新数据库，再删除缓存（推荐） 2.1 先更新数据库，再更新缓存 这套方案，大家是普遍反对的。为什么呢？有如下两点原因。 原因1：（线程安全角度） 同时有请求A和请求B进行更新操作，那么会出现 （1）线程A更新了数据库 （2）线程B更新了数据库 （3）线程B更新了缓存 （4）线程A更新了缓存 这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。 原因二（业务场景角度） 有如下两点： （1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。 （2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。 2.2 先删缓存，在更新数据库（争议最大） ）请求A进行写操作，删除缓存 （2）请求B查询发现缓存不存在 （3）请求B去数据库查询得到旧值 （4）请求B将旧值写入缓存 （5）请求A将新值写入数据库 上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。 那么，如何解决呢？采用延时双删策略 伪代码如下 public void write(String key,Object data){ redis.delKey(key); db.updateData(data); Thread.sleep(1000); redis.delKey(key); } 转化为中文描述就是 （1）先淘汰缓存 （2）再写数据库（这两步和原来一样） （3）休眠1秒，再次淘汰缓存 这么做，可以将1秒内所造成的缓存脏数据，再次删除。 那么，这个1秒怎么确定的，具体该休眠多久呢？ 针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 如果你用了mysql的读写分离架构怎么办？ ok，在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。 （1）请求A进行写操作，删除缓存 （2）请求A将数据写入数据库了， （3）请求B查询缓存发现，缓存没有值 （4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值 （5）请求B将旧值写入缓存 （6）数据库完成主从同步，从库变为新值 上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。 采用这种同步淘汰策略，吞吐量降低怎么办？ ok，那就将第二次删除作为异步的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。 第二次删除,如果删除失败怎么办？ 这是个非常好的问题，因为第二次删除失败，就会出现如下情形。还是有两个请求，一个请求A进行更新操作，另一个请求B进行查询操作，为了方便，假设是单库： （1）请求A进行写操作，删除缓存 （2）请求B查询发现缓存不存在 （3）请求B去数据库查询得到旧值 （4）请求B将旧值写入缓存 （5）请求A将新值写入数据库 （6）请求A试图去删除请求B写入对缓存值，结果失败了。 ok,这也就是说。如果第二次删除缓存失败，会再次出现缓存和数据库不一致的问题。 如何解决呢？ 具体解决方案，且看博主对第(3)种更新策略的解析。 2.3 先更新数据库，再删除缓存 首先，先说一下。老外提出了一个缓存更新套路，名为《Cache-Aside pattern》。其中就指出 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 这种情况不存在并发问题么？ 不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生 （1）缓存刚好失效 （2）请求A查询数据库，得一个旧值 （3）请求B将新值写入数据库 （4）请求B删除缓存 （5）请求A将查到的旧值写入缓存 ok，如果发生上述情况，确实是会发生脏数据。 然而，发生这种情况的概率又有多少呢？ 发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。 假设，有人非要抬杠，有强迫症，一定要解决怎么办？ 如何解决上述并发问题？ 首先，给缓存设有效时间是一种方案。其次，采用策略（2）里给出的异步延时删除策略，保证读请求完成以后，再进行删除操作。 还有其他造成不一致的原因么？ 有的，这也是缓存更新策略（2）和缓存更新策略（3）都存在的一个问题，如果删缓存失败了怎么办，那不是会有不一致的情况出现么。比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情况了。这也是缓存更新策略（2）里留下的最后一个疑问。 如何解决？ 提供一个保障的重试机制即可，这里给出两套方案。 方案一： 如下图所示 流程如下所示 （1）更新数据库数据； （2）缓存因为种种问题删除失败 （3）将需要删除的key发送至消息队列 （4）自己消费消息，获得需要删除的key （5）继续重试删除操作，直到成功 然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。 方案二： 流程如下图所示： （1）更新数据库数据 （2）数据库会将操作信息写入binlog日志当中 （3）订阅程序提取出所需要的数据以及key （4）另起一段非业务代码，获得该信息 （5）尝试删除缓存操作，发现删除失败 （6）将这些信息发送至消息队列 （7）重新从消息队列中获得该数据，重试操作。 "},"redis/lock/":{"url":"redis/lock/","title":"redis分布式锁","keywords":"","body":"redis分布式锁1. 介绍1.1 什么是分布式锁1.2 分布式系统需要具备哪些条件1.3 分布式锁的实现有哪些参考文章redis分布式锁 1. 介绍 分布式锁的实现分为两部分，一个是单机环境，另一个是集群环境下的Redis锁实现。 1.1 什么是分布式锁 分布式锁是控制分布式系统或不同系统之间共同访问共享资源的一种锁实现，如果不同的系统或同一个系统的不同主机之间共享了某个资源时，往往需要互斥来防止彼此干扰来保证一致性 1.2 分布式系统需要具备哪些条件 互斥性：在任意一个时刻，只有一个客户端持有锁 无死锁：即使持有锁的客户端崩溃或者其他意外事件，锁仍然可以被获取 容错：只要大部分 redis节点都活着，客户端就可以获取和释放锁 1.3 分布式锁的实现有哪些 数据库 Memcached（add命令） Redis（setnx命令） Zookeeper（临时节点） 等等 参考文章 redis系列：分布式锁 "},"redis/lock/一步步实现单机redis的分布式锁.html":{"url":"redis/lock/一步步实现单机redis的分布式锁.html","title":"一步步实现单机redis的分布式锁（setnx）","keywords":"","body":"一步步实现单机redis的分布式锁（setnx）1. 准备工作1.1 定义常量1.2 定义锁的抽象类2. 基础版本12.1 版本1 面临的问题3. 版本2 设置锁的过期时间3.2 版本2 可能出现的问题4. 版本3 设置锁的value4.1 可能面试i++ 问题5. 版本4- 具有原子性的释放锁6. 版本5-确保过期时间大于业务执行时间7. 测试7.1 测试结果一步步实现单机redis的分布式锁（setnx） 1. 准备工作 1.1 定义常量 public class LockConstants { public static final String OK = \"OK\"; /** NX|XX, NX -- Only set the key if it does not already exist. XX -- Only set the key if it already exist. **/ public static final String NOT_EXIST = \"NX\"; public static final String EXIST = \"XX\"; /** expx EX|PX, expire time units: EX = seconds; PX = milliseconds **/ public static final String SECONDS = \"EX\"; public static final String MILLISECONDS = \"PX\"; private LockConstants() {} } 1.2 定义锁的抽象类 抽象类RedisLock实现java.util.concurrent包下的Lock接口，然后对一些方法提供默认实现，子类只需实现lock方法和unlock方法即可。代码如下 public abstract class RedisLock implements Lock { protected Jedis jedis; protected String lockKey; protected String lockValue; public RedisLock(Jedis jedis,String lockKey) { this.jedis = jedis; this.lockKey = lockKey; this.lockValue = UUID.randomUUID().toString()+Thread.currentThread().getId(); } public void sleepBySencond(int sencond){ try { Thread.sleep(sencond*1000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public void lockInterruptibly() throws InterruptedException { } @Override public boolean tryLock() { return false; } @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException { return false; } @Override public Condition newCondition() { return null; } protected volatile boolean isOpenExpirationRenewal = true; /** * 开启定时刷新 */ protected void scheduleExpirationRenewal(){ Thread renewalThread = new Thread(new ExpirationRenewal()); renewalThread.start(); } /** * 刷新key的过期时间 */ private class ExpirationRenewal implements Runnable{ @Override public void run() { while (isOpenExpirationRenewal){ System.out.println(\"执行延迟失效时间中...\"); String checkAndExpireScript = \"if redis.call('get', KEYS[1]) == ARGV[1] then \" + \"return redis.call('expire',KEYS[1],ARGV[2]) \" + \"else \" + \"return 0 end\"; jedis.eval(checkAndExpireScript, 1, lockKey, lockValue, \"30\"); //休眠10秒 sleepBySencond(10); } } } } 2. 基础版本1 先来一个最基础的版本，代码如下 public class LockCase1 extends RedisLock{ public LockCase1(Jedis jedis, String lockKey) { super(jedis, lockKey); } @Override public void lock() { while(true){ String result = jedis.set(lockKey, \"value\", NOT_EXIST); if(OK.equals(result)){ System.out.println(Thread.currentThread().getId()+\"加锁成功!\"); break; } } } @Override public void unlock() { jedis.del(lockKey); } } LockCase1类提供了lock和unlock方法。 其中lock方法也就是在reids客户端执行如下命令 SET lockKey value NX 而unlock方法就是调用DEL命令将键删除。 2.1 版本1 面临的问题 客户端挂了，锁无法释放 假设有两个客户端A和B，A获取到分布式的锁。A执行了一会，突然A所在的服务器断电了（或者其他什么的），也就是客户端A挂了。这时出现一个问题，这个锁一直存在，且不会被释放，其他客户端永远获取不到锁。如下示意图 可以通过设置过期时间来解决这个问题 3. 版本2 设置锁的过期时间 @Override public void lock() { while(true){ String result = jedis.set(lockKey, \"value\", NOT_EXIST,SECONDS,30); if(OK.equals(result)){ System.out.println(Thread.currentThread().getId()+\"加锁成功!\"); break; } } } 类似的Redis命令如下 SET lockKey value NX EX 30 注：要保证设置过期时间和设置锁具有原子性 EX = seconds; PX = milliseconds 3.2 版本2 可能出现的问题 这时又出现一个问题，问题出现的步骤如下 客户端A获取锁成功，过期时间30秒。 客户端A在某个操作上阻塞了50秒。 30秒时间到了，锁自动释放了。 客户端B获取到了对应同一个资源的锁。 客户端A从阻塞中恢复过来，释放掉了客户端B持有的锁。 示意图如下 这时会有两个问题 过期时间如何保证大于业务执行时间? 如何保证锁不会被误删除? 先来解决如何保证锁不会被误删除这个问题 这个问题可以通过设置value为当前客户端生成的一个随机字符串，且保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一 4. 版本3 设置锁的value 抽象类RedisLock增加lockValue字段，lockValue字段的默认值为UUID随机值假设当前线程ID。 public abstract class RedisLock implements Lock { //... protected String lockValue; public RedisLock(Jedis jedis,String lockKey) { this(jedis, lockKey, UUID.randomUUID().toString()+Thread.currentThread().getId()); } public RedisLock(Jedis jedis, String lockKey, String lockValue) { this.jedis = jedis; this.lockKey = lockKey; this.lockValue = lockValue; } //... } 复制代码 加锁代码 public void lock() { while(true){ String result = jedis.set(lockKey, lockValue, NOT_EXIST,SECONDS,30); if(OK.equals(result)){ System.out.println(Thread.currentThread().getId()+\"加锁成功!\"); break; } } } 复制代码 解锁代码 public void unlock() { String lockValue = jedis.get(lockKey); if (lockValue.equals(lockValue)){ jedis.del(lockKey); } } 这时看看加锁代码，好像没有什么问题啊。 再来看看解锁的代码，这里的解锁操作包含三步操作：获取值、判断和删除锁。这时你有没有想到在多线程环境下的i++操作? 4.1 可能面试i++ 问题 i++操作也可分为三个步骤：读i的值，进行i+1，设置i的值。 如果两个线程同时对i进行i++操作，会出现如下情况 i设置值为0 线程A读到i的值为0 线程B也读到i的值为0 线程A执行了+1操作，将结果值1写入到内存 线程B执行了+1操作，将结果值1写入到内存 此时i进行了两次i++操作，但是结果却为1 在多线程环境下有什么方式可以避免这类情况发生? 解决方式有很多种，例如用AtomicInteger、CAS、synchronized等等。 这些解决方式的目的都是要确保i++ 操作的原子性。那么回过头来看看解锁，同理我们也是要确保解锁的原子性。我们可以利用Redis的lua脚本来实现解锁操作的原子性。 5. 版本4- 具有原子性的释放锁 lua脚本内容如下 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 复制代码 这段Lua脚本在执行的时候要把的lockValue作为ARGV[1]的值传进去，把lockKey作为KEYS[1]的值传进去。现在来看看解锁的java代码 public void unlock() { // 使用lua脚本进行原子删除操作 String checkAndDelScript = \"if redis.call('get', KEYS[1]) == ARGV[1] then \" + \"return redis.call('del', KEYS[1]) \" + \"else \" + \"return 0 \" + \"end\"; jedis.eval(checkAndDelScript, 1, lockKey, lockValue); } 6. 版本5-确保过期时间大于业务执行时间 抽象类RedisLock增加一个boolean类型的属性isOpenExpirationRenewal，用来标识是否开启定时刷新过期时间。 在增加一个scheduleExpirationRenewal方法用于开启刷新过期时间的线程。 public abstract class RedisLock implements Lock { //... protected volatile boolean isOpenExpirationRenewal = true; /** * 开启定时刷新 */ protected void scheduleExpirationRenewal(){ Thread renewalThread = new Thread(new ExpirationRenewal()); renewalThread.start(); } /** * 刷新key的过期时间 */ private class ExpirationRenewal implements Runnable{ @Override public void run() { while (isOpenExpirationRenewal){ System.out.println(\"执行延迟失效时间中...\"); String checkAndExpireScript = \"if redis.call('get', KEYS[1]) == ARGV[1] then \" + \"return redis.call('expire',KEYS[1],ARGV[2]) \" + \"else \" + \"return 0 end\"; jedis.eval(checkAndExpireScript, 1, lockKey, lockValue, \"30\"); //休眠10秒 sleepBySencond(10); } } } } 复制代码 加锁代码在获取锁成功后将isOpenExpirationRenewal置为true，并且调用scheduleExpirationRenewal方法，开启刷新过期时间的线程。 public void lock() { while (true) { String result = jedis.set(lockKey, lockValue, NOT_EXIST, SECONDS, 30); if (OK.equals(result)) { System.out.println(\"线程id:\"+Thread.currentThread().getId() + \"加锁成功!时间:\"+LocalTime.now()); //开启定时刷新过期时间 isOpenExpirationRenewal = true; scheduleExpirationRenewal(); break; } System.out.println(\"线程id:\"+Thread.currentThread().getId() + \"获取锁失败，休眠10秒!时间:\"+LocalTime.now()); //休眠10秒 sleepBySencond(10); } } 复制代码 解锁代码增加一行代码，将isOpenExpirationRenewal属性置为false，停止刷新过期时间的线程轮询。 public void unlock() { //... isOpenExpirationRenewal = false; } 7. 测试 public class LockCase5Test { private String lockName=\"my+lockName\"; @Test public void testLockCase5() { //定义线程池 ThreadPoolExecutor pool = new ThreadPoolExecutor(0, 10, 1, TimeUnit.SECONDS, new SynchronousQueue<>()); //添加10个线程获取锁 for (int i = 0; i { try { Jedis jedis = new Jedis(\"120.79.200.111\"); LockCase5 lock = new LockCase5(jedis, lockName); lock.lock(); //模拟业务执行15秒 lock.sleepBySencond(15); lock.unlock(); } catch (Exception e){ e.printStackTrace(); } }); } //当线程池中的线程数为0时，退出 while (pool.getPoolSize() != 0) {} } } 7.1 测试结果 "},"redis/lock/Redlock分布式锁.html":{"url":"redis/lock/Redlock分布式锁.html","title":"Redlock分布式锁","keywords":"","body":"Redlock分布式锁1. 什么是Redlock2. 怎么在单节点上实现分布式锁2.1 加锁2.2 释放锁3. Redlock 算法4. 失败重试5. 放锁6. 性能、崩溃恢复和 fsyncRedlock分布式锁 1. 什么是Redlock redis 官方提出的一种权威的基于 Redis 实现分布式锁的方式名叫Redlock, 此种方式比原先的单节点的方法更安全。它可以保证以下特性 安全性：互斥访问，既永远只有一个client 能拿到锁 避免死锁：最终client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的client crash 了或者出现网络分区 容错性：只要大部分Redis 节点存活就能提供正常服务 2. 怎么在单节点上实现分布式锁 2.1 加锁 SET resource_name my_random_value NX PX 30000 主要依靠上述命令，该命令仅当 Key 不存在时（NX保证）set 值，并且设置过期时间 3000ms （PX保证）。值 my_random_value 必须是所有 client 和所有锁请求发生期间唯一的 2.2 释放锁 if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 上述实现可以避免释放另一个client创建的锁， 如果只有del 命令的话，那么如果client拿到lock1 之后因为某些操作阻塞了很长时间，此时redis端的lock1已经过期了并且重新分配给了client2，那么 client1 此时再去释放这把锁就会造成 client2 原本获取到的锁被 client1 无故释放了，但现在为每个 client 分配一个 unique 的 string 值可以避免这个问题。至于如何去生成这个 unique string，方法很多随意选择一种就行了。 3. Redlock 算法 起5个 master节点，分布在不同的机房尽量保证可用性。为了获得锁，client 会进行如下操作： 得到当前时间，微妙单位 尝试顺序地在5个实例上申请锁，当然需要使用相同的key 和random value。这里一个client 需要合理设置与master节点沟通的timeout大小。避免长时间和一个fail了的节点浪费时间 当client 在大于等于3个master 上成功申请到锁的时候，且他会计算申请锁消耗了多少时间，这部分消耗的时间采用获取锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了 如果锁申请到了，那么锁真正的lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间 如果 client 申请锁失败了，那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态 4. 失败重试 如果一个 client 申请锁失败了，那么他需要稍等一会在重试避免多个 client 同时申请锁的情况，做好的情况是一个client 需要几乎同时向5个master 发起锁申请。另外就是如果 client 申请锁失败了他需要尽快在他曾经申请到锁的master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费。当然如果这时候网络分区使得client 无法联系上这些master，那么这种浪费就是不得不付出的代价了 5. 放锁 放锁操作很简单，就是依次释放所有节点上的锁就行了 6. 性能、崩溃恢复和 fsync 如果我们的节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这是注意 整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！ 违反了互斥性。如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。 "},"redis/interview/redis面试问题.html":{"url":"redis/interview/redis面试问题.html","title":"redis面试问题","keywords":"","body":"redis面试问题1. redis概念篇2. 数据结构和使用场景3. redis过期时间4. 持久化篇5. 线程模型篇6. 使用redis可能面临的问题？redis面试问题 1. redis概念篇 谈谈自己对redis 的理解？ 为什么要使用redis？ 为什么要用redis而不用map/guava做缓存 2. 数据结构和使用场景 说说redis 常见的数据结构和使用场景 3. redis过期时间 你知道redis设置key过期时间的命令吗？ redis怎么对这批key进行删除？ 大量过期key堆积在内存中，导致内存耗尽，如何解决？ 4. 持久化篇 怎么保证redis挂掉之后再重启数据可以进行恢复？ 5. 线程模型篇 说说redis 的线程模型？ 线程模型的处理流程？ redis为什么是单线程的？ redis单线程为什么效率也这么高？ 6. 使用redis可能面临的问题？ 如果缓存同一时间大面积失效怎么办？（缓存雪崩） 如果被不法分子故意请求缓存中不存在的数据，我们要怎么处理？（缓存穿透） 你们是怎么做缓存预热的？ 你们是如何保证缓存与数据库双写时的数据一致性？ "},"redis/book/":{"url":"redis/book/","title":"Redis简介","keywords":"","body":"Redis简介1. Redis是什么2. 与其他数据库软件的对比3.附加特性3.1 持久化存储3.2 主从复制4. 为什么使用redisRedis简介 1. Redis是什么 redis 是一个速度非常快的非关系数据库 它可以存储键（key）与5种不同类型的值（value）之间的映射（mapping） 可以将存储在内存中的键值对数据持久化到硬盘 可以使用复制特性来扩展读性能 可以通过客户端分片来扩展写性能 2. 与其他数据库软件的对比 与关系型数据库对比 redis不使用表， 不会预定义或者强制要求用户对redis存储的数据进行关联 与memcached 共同点：都可以存储键值映射，性能相差无几 不同点： 能够自动写入硬盘 除了字符串还能存储其他4种数据结构 3.附加特性 3.1 持久化存储 当使用redis这种内存数据库时。一个首要考虑的问题就是“当服务器被关闭时，服务器存储的数据将何去何从” 时间点转储（point-in-time-dump） 转存操作既可以在指定时间内有指定数据的写操作执行 追加（append-only） 将所有的修改了数据库的命令写入一个只追加文件里面 追加方式 从不同步 每秒同步一次 写入一个命令就同步一次 3.2 主从复制 尽管redis性能很好，但受限于redis的内存存储设计，有时候只使用一个redis服务并不能处理所有请求。为了扩展redis 的读特性，并为redis提供故障转移（failover）支持。redist 实现了主从复制 执行复制的从服务器会连接上主服务器。接收主服务器发送的整个数据库初始副本（copy） 之后主服务器执行写命令，都会发送给所有连接这从服务器去执行，从而实时更新从服务器的数据集。 从服务器的数据会不断的进行更新，所以客户端可以向任意一个从服务器发送读请求 因此避免对主服务器进行集中式的访问 4. 为什么使用redis 相比memcached： ​ 可以让代码更简短，更易懂，更易维护，而且还可以使代码运行速度更快 相比关系型数据库 ​ 效率和易用性高 "},"redis/5种数据结构.html":{"url":"redis/5种数据结构.html","title":"5种数据结构","keywords":"","body":"5种数据结构1. 5种数据结构5种数据结构 1. 5种数据结构 STRING（字符串） LIST（列表） SET （集合） HASH (散列) ZSET (有序集合) "},"redis/string/":{"url":"redis/string/","title":"String字符串","keywords":"","body":"String字符串1.操作命令1.1 基础操作命令1.2 自增自减命令1.3 处理子串String字符串 字符串可以存储以下3种类型的值 字节串(byte string) 整数 浮点数 1.操作命令 1.1 基础操作命令 命令 行为 GET 获取存储在给定键中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值 1.2 自增自减命令 命令 用例 描述 INCR INCR key-name 将键存储的值加上1 DECR DECR key-name 将键存储的值减去1 INCRBY INCRBY key-name amount 将键存储的值加上整数amount DECRBY DECRBY key-name amount 将键存储的值减去整数amount INCRBYFLOAT INCRBYFLOAT key-name amount 将键存储的值加上浮点数amount（redis2.6以上） 1.3 处理子串 命令 用例 描述 APPEND APPEND key-name value 将值value追加到给定键key-name当前值的末尾 GETRANGE GETRANGE key-name start end 获取一个由偏移量start至偏移量end范围内组成的子串，包括start和end在内 SETRANGE SETRANGE key-name offset value 将从start偏移量开始的子串设置为给定值 GetBit GETBIT key-name offset 将字符串看做hi二进制位串（bit string） SetBit BitCount Bittop "},"redis/list/":{"url":"redis/list/","title":"List列表","keywords":"","body":"List列表1.操作命令1.1 常用命令1.2 阻塞式的列表弹出命令1.3 列表之间移动元素List列表 redis的列表允许用户从序列两端推入或者弹出元素 1.操作命令 1.1 常用命令 命令 用例 描述 LPUSH LPUSH key-name value [value ...] 将一个或多个值推入列表的左端 RPUSH RPUSH key-name value [value ...] 将一个或多个值推入列表的右端 LPOP LPOP key-name 移除并返回列表最左端的元素 RPOP RPOP key-name 移除并返回列表最右端的元素 LINDEX LINDEX key-name offset 返回列表中偏移量为offset的元素 LRANGE LRANGE key-name start end 返回列表从start偏移量到end偏移量范围内的所有元素(包含start,end) LTRIM LTRIM key-name start end 对列表进行修剪，只保留start和end偏移量之间的元素（包含start,end） 1.2 阻塞式的列表弹出命令 命令 用例 描述 BLPOP BLPOP key-name [key-name …] timeout 从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现 BRPOP BRPOP key-name [key-name …] timeout 从第一个非空列表中弹出位于最右端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现 1.3 列表之间移动元素 命令 用例 描述 RPOPLPUSH RPOPLPUSH source-key dest-key 从souce-key列表中弹出位于最右端的元素，然后将这个元素推入desr-key最左端。并向用户返回这个元素 BRPOPLPUSH BRPOPLPUSH source-key dest-key 从source-key列表中弹出位于最右端的元素，然后将这个元素推入dest-key列表最左端。并向用户返回这个元素。如果soutce-key为空，那么在timeout秒之内之内阻塞并等待可弹出的元素出现 "},"redis/set/":{"url":"redis/set/","title":"Set集合","keywords":"","body":"Set集合1.操作命令1.1 基本命令1.2 处理多个集合Set集合 redis 的集合和列表 共同点 都可以存储多个字符串 不同点 列表可以存储多个相同的字符串，而集合则通过使用散列表来保证自己存储的每个字符串各不相同 redis集合使用无序（unordered）方式存储元素，所以用户 1.操作命令 1.1 基本命令 命令 行为 描述 SADD SADD key-name item [item...] 将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存储于集合里面的元素数量 SREM SREM key-name item [item ...] 从集合里面移除一个或多个元素，并返回被移除元素的数量 SISMEMBER SISMEMBER key-name item 检查元素item 是否存在与集合key-name里 SISMEMBER SCARD key-name 返回集合包含元素的数量 SMEMBERS SMEMBERS key-name 返回集合包含的所有元素 SRANDMEMBER SRANDMEMBER key-name [count] 从集合里面随机地返回一个或多个元素。当count为正数时，命令返回的随机元素不会重复，当count为负数时，命令返回的随机元素可能出现重复 SPOP SPOP key-name 随机地移除集合中的一个元素，并返回被移除的元素 SMOVE SMOVE source-key dest-key item 如果集合source-key包含元素item，那么从集合source-key里面移除元素item，并将元素item添加到集合dest-keyzhong 。如果成功移除返回1，否则返回0 SINTER 交集计算 SINTER 交集计算 1.2 处理多个集合 命令 用例 描述 SDIFF SDIFF key-name [key-name ...] 返回那些存在于第一个集合，但不存在于其他集合中的元素（差集运算） SDIFFSTORE SDIFFSTORE dest-key key-name [key-name ...] 将那些存在与第一个集合但并不存在与其他集合中的元素（差集）存储到dest-key 键里面 SINTER SINTER key-name [key-name ...] 返回那些同时存在于所有集合中的元（交集运算） SINTERSTORE SINTERSTORE dest-key key-name [key-name ...] 将那些同时存在于所欲集合的元素存储到dest-key键里面 SUNION SUNION key-name [key-name ...] 返回那些至少存在于一个集合中的元素（并集运算） SUNIONSTORE SUNIONSTORE dest-key key-name [key-name ...] 将那么至少存在与一个集合中的元素（并集）存储到dest-key键里面 "},"redis/hash/":{"url":"redis/hash/","title":"Hash散列","keywords":"","body":"Hash散列1. 操作命令1.1 基础操作命令1.2 高级特性Hash散列 redis的散列可以存储多个键值对之间的映射，散列在很多方面就想是一个微缩版的redis 1. 操作命令 1.1 基础操作命令 命令 用例 描述 HGET HGET key-name key 在散列里面获取一个键的值（HMGET单参数版本） HSET HSET key-name key value 为散列里面的一个键设置值（HMSET单参数版本） HMGET HMGET key-name key [key...] 在散列里面获取一个或多个键的值 HMSET HMSET key-name key value [key value ...] 为散列里面的一个或多个键设置值 HDEL HDEL key-name key [key ...] 删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量 HLEN HLEN key-name 返回散列包含的键值对数量 1.2 高级特性 命令 用例 描述 HEXISTE HEXISTE key-name key 检查给定键是否存在与散列中 HKEYS HKEYS key-name 获取散列包含的所有键 HVALS HVALS key-name 获取散列包含的所有值 HGETALL HGETALL key-name 获取散列包含的所有键值对 HINCRBY HINCRBY key-name key increment 将键key存储的值加上整数increment HINCRBYFLOAT HINCRBYFLOAT key-name key increment 将键key存储的值加上浮点数 increment "},"redis/zset/":{"url":"redis/zset/","title":"Zset有序集合","keywords":"","body":"Zset有序集合1 操作命令1.1 基础命令1.2 其他命令Zset有序集合 和散列存储这键和值之间的映射类似，有序集合也存储着成员与分值之间的映射，并且提供了分值处理命令，以及根据分值大小有序的获取（fetch）或扫描（scan）成员和分值的命令 键：被称为member 每个成员都是各不相同的 值：称为分值（score） 分值必须为浮点数 1 操作命令 1.1 基础命令 命令 用例 描述 ZADD ZADD key-name score member [score member ...] 将带有给定分值的成员添加到有序集合里面 ZREM ZREM key-name member [member ...] 从有序集合里面移除给定的成员，并返回被移除成员数量 ZINCRBY ZINCRBY key-name increment member 将member成员的分值加上increment ZCOUNT ZCOUNT key-name min max 返回分值介于min和max之间的成员数量 ZRANK ZRANK key-name member 返回成员member在有序集合中的排名 ZSCORE ZSCORE key-name member 返回成员member的分值 ZRANGE ZRANGE key-name start stop [WITHSCORES] 返回有序集合中排名介于start和stop之间的成员，如果给定了可选的WITHSCORES，那么命令会将成员的分值也一并返回 1.2 其他命令 命令 用例 描述 ZREVRANK ZREVRANGE ZRANGEBYSCORE ZREVERANGEBYSCORE ZREMANGEBYRANK ZREMRANGEBYSCORE ZINTERSTORE ZUNIONSTORE "},"redis/发布与订阅.html":{"url":"redis/发布与订阅.html","title":"发布与订阅(pub/sub)","keywords":"","body":"发布与订阅(pub/sub)1.操作命令1.1 基础操作命令2 发布订阅的弊端发布与订阅(pub/sub) 发布与订阅（又称pub/sub）的特点： 订阅者（listener）负责订阅频道（channel） 发送者（publisher）负责向频道发送二进制字符串消息（binary string message） 每当有消息被发送至给定频道时，频道所有订阅者丢回收到消息 1.操作命令 1.1 基础操作命令 命令 用例 描述 SUBSCRIBE SUBSCRIBE channel [channel ...] 订阅给定的一个或多个频道 UNSUBSCRIBE UNSUBSCRIBE [channel [channel ...]] 退订给定的一个或多个频道，如果执行时没有给定任何频道，那么退订所有频道 PUBLISH PUBLISH channel message 向给定频道发送消息 PSUBSCRIBE PSUBSCRIBE pattern [pattern ...] 订阅与给定模式相匹配的所有渠道 PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern ...]] 退订给定的模式，如果执行时没有给定任何模式，退订所有 2 发布订阅的弊端 原因1：redis的稳定性 对于旧版redis，客户端订阅了某些频道 但是读取消息的速度却不够快的话，那么不断积压的消息就会使得redis输出缓冲区的体积变得越打越大 导致redis速度变慢，甚至奔溃，也可能导致redis 被系统杀死 新版的不会出现这种问题，会自动断开不符合client-output-buffer-limit pubsub 配置选项要求的订阅客户端 原因2：数据传输的可靠性 任何网络系统在执行操作时都可能会遇上短信情况 客户端将丢失在断线期间的所有消息 "},"redis/排序.html":{"url":"redis/排序.html","title":"排序SORT","keywords":"","body":"排序SORT1.操作命令排序SORT SORT可以根据字符串、列表、集合、有序集合、散列这5中键里面存储着的数据，对列表、集合以及有序集合进行排序。 SORT 可以看做sql语言里的order by 1.操作命令 命令 用例 描述 SORT SORT source-key [BY pattern] [LIMIT offset count] [get pattern [get pattern …]] [asc\\ desc] [alpha] [store dest-key] 根据给定的选项，对输入列表，集合或者有序集合进行排序，然后返回或者存储排序的结果 "},"redis/事务.html":{"url":"redis/事务.html","title":"事务","keywords":"","body":"事务事务 redis 基础事务（basic transaction）需要使用到MULTI 命令和EXEX命令，这种事务可以让一个客户端在不被其他客户端打断的情况下执行多个命令 redis事务的特点 和关系数据库的回滚（rollback）的事务不同，在redis里面被MULTI 命令和EXEX命令包围的所有命令会一个接一个的执行，直到所有命令都执行完毕为止。当一个事务执行完毕之后，redis才会处理其他客户端命令 "},"redis/键的过期时间.html":{"url":"redis/键的过期时间.html","title":"键的过期时间","keywords":"","body":"键的过期时间键的过期时间 在使用redis存储数据的时候，有些数据可能在某个时间点之后就不会再使用了，用户可以使用DEL命令显示的删除这些无用数据，也可以通过设置过期时间（expiration）特性来让一个键在给定的时限（timeout）之后自动被删除 命令 示例 描述 PERSIST PERSIST key-name 移除键的过期时间 TTL TTL key-name 查看给定键距离过期还有多少秒 EXPIRE EXPIRE key-name seconds 让给定键在指定的秒数之后过期 EXPIREAT EXPIREAT key-name timestamp 将给定键的过期时间设置为给定的UNIX时间戳 PTTL PTTL key-name 查看给定键距离过期时间还有多少毫秒（redis 2.6） PEXPIRE PEXPIRE key-name milliseconds 让给定键在指定的毫秒数之后过期（redis2.6） PEXPIREAT PEXPIREAT key-name timestamp-milliseconds 将一个毫秒级精度的unix时间戳设置给定键的过期时间（reids2.6） "},"redis/persistence/":{"url":"redis/persistence/","title":"持久化","keywords":"","body":"持久化1.简介2.为什么需要持久化3. redis 持久化配置选项3.1 快照持久化选项3.2 AOF 持久化选项持久化 1.简介 redis 提供了两种不同的持久化方法来将数据存储到硬盘 快照（snapshotting） 他可以将存在于某一时刻的所有数据都写入硬盘里面 只追加文件（append-only file,AOF） 他会在执行写命令时，将被执行的写命令复制到硬盘里面 这两种方案既可以同时使用，也可以单独使用 2.为什么需要持久化 为什么需要将内存中的数据存储到硬盘的一个主要原因 重用数据 防止系统故障而将数据备份到远程位置 3. redis 持久化配置选项 3.1 快照持久化选项 save 60 1000 stop-writes-on-bgsave-error no rdbcompression yes dbfilename dump.rdb dir ./ 3.2 AOF 持久化选项 appendonly no appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb dir ./ "},"redis/persistence/快照.html":{"url":"redis/persistence/快照.html","title":"快照(snapshotting)","keywords":"","body":"快照(snapshotting)1. 快照配置2.创建快照的方式3. 快照面临的问题快照(snapshotting) redis可以通过 创建快照来获得存储在内存里面的数据在某个时间点上的副本 在创建快照之后，用户可以对快照进行备份。 可以将快照复制到其他服务器从而创建具有相同服务数据的服务器副本， 可以将快照留在原地等以便服务重启时使用 1. 快照配置 save 60 1000 stop-writes-on-bgsave-error no rdbcompression yes dbfilename dump.rdb dir ./ 快照将被写入dbfilename 指定的文件里面 存储在dir选项的路径上 如果在新的快照文件创建完毕之前。redis、系统、或者硬件这三者之一的任意一个奔溃，那么redis将丢失最近一次创建快照之后写入的所有数据 2.创建快照的方式 客户端向redis发送BGSAVE 创建快照 redis会调用fork来创建一个子进程，然后子进程复制将快照写入硬盘，而父进程则继续处理命令请求（不支持windows） 客户端向redis发送SAVE 创建快照（不常用） 接收到save命令的redis服务器在快照创建完毕之前将不再响应其他命令 （只会在没有足够的内存区执行bgsave的情况下和即使等到持久化操作执行完毕也无所谓的情况下，才执行） 设置save选项（save 60 10000） redis最近一次创建快照之后开始算起，当60秒之内有10000次写入，这个条件满足时，redis就会自动护法BGSAVE命令 设置多个，满足其中一个就会触发一个BGSAVE 当redis通过shutdown命令接收到关闭服务器请求/接收到标准TERM信号 会执行一个SAVE命令，阻塞所有客户端，不在执行客户端发送的任何命令 并在SAVE命令执行完毕之后关闭服务器 当redis服务器连接另一个redis服务器，并向对方发送SYNC命令来开始一次复制操作的时候 如果主服务器目前没有在执行BGSAVE操作，或者主服务器并非刚刚执行完BGSAVE操作，那么主服务器就会执行BGSAVE命令 3. 快照面临的问题 如果系统真的发生奔溃，用户将丢失最近一次生成快照之后更改的所有数据。 因此，快照持久化只适合那些即使丢失一部分数据也不会赵成问题的应用程序 "},"redis/persistence/只追加文件.html":{"url":"redis/persistence/只追加文件.html","title":"只追加文件(append-only file,AOF)","keywords":"","body":"只追加文件(append-only file)1. 简介2.文件同步步骤3.同步频率4. AOF 重写/压缩3.1 AOF面临的问题3.2 解决文件过大问题3.3 BGREWRITEAOF 命令原理只追加文件(append-only file) 1. 简介 AOF 持久化会将被执行的写命令写到AOF文件末尾，已此来记录数据发生的变化。 因此，redis只要从头到尾重新执行一次AOP文件包含的所有写命令，就可以恢复AOF文件所记录的数据集 2.文件同步步骤 在向硬盘写入文件时，至少发生3件事 当调用file.write()方法对文件写入时， 写入的内容会被存储到缓冲区 然后操作系统会在将来的某个时候将缓冲区存储的内容写入硬盘（数据只有被写入硬盘，才算是真正的保存到硬盘里面） 3.同步频率 选项 同步频率 特点 always 每个redis写命令都要同步写入硬盘，这样会严重降低redis的速度 每个redis命令都会被写入硬盘，丢失最少，需要对用哦按大量写入。处理命令受到限制（机械硬盘200个写命令，固态几万个写命令） everysec 每秒执行一次同步，显示地将多个写命令同步到硬盘 性能和不使用相差无几（推荐） no 让操作系统来决定应该何时进行同步 由系统决定何时进行AOF同步，系统奔溃的数据丢失数量不确定（不推荐） 4. AOF 重写/压缩 3.1 AOF面临的问题 文件的体积大小。 体积不断增大的AOF文件是指可能会用完硬盘所有的可用空间 文件太大操作时间长 3.2 解决文件过大问题 发送BGREWRITEAOF命令 这个命令会移除AOF文件中的冗余命令来重写（rewrite）AOF文件 通过设置auto-aof-rewrite-percentage选项和auto-aof-rewrite-min-size选项来自动执行BGREWRITEAOF 3.3 BGREWRITEAOF 命令原理 redis 创建一个子进程，然后由子进程负责对AOF文件重写 （也可能会导致性能问题和内存占用问题） "},"redis/install/":{"url":"redis/install/","title":"Redis安装","keywords":"","body":"Redis安装1. 具体安装步骤1.1 使用wget命令下载1.2 拷贝到/usr/local 下1.3 解压源码1.4 编译1.5 redis的配置文件2. bin目录结构3. 启动redis3.1 前端模式启动3.2 后端模式启动4. 连接redis5.关闭redis5.1 强行终止redis(不推荐)6. 开机自启动Redis安装 1. 具体安装步骤 1.1 使用wget命令下载 wget http://download.redis.io/releases/redis-5.0.5.tar.gz 具体下载哪个版本可以在redis官网上选择 1.2 拷贝到/usr/local 下 cp redis-5.0.5.tar.gz /usr/local 1.3 解压源码 tar -zxvf redis-5.0.5.tar.gz 1.4 编译 cd /usr/local/redis-5.0.5 make PREFIX=/usr/local/redis install 编译后的redis在 /usr/local/redis目录下 1.4.1 如遇安装异常cc: command not found /bin/sh: cc: command not found 解决方案：安装gcc命令 yum install gcc 1.4.2 make时报如下错误 原因是jemalloc重载了Linux下的ANSI C的malloc和free函数。解决办法：make时添加参数。 1.5 redis的配置文件 redis.conf是redis的配置文件，redis.conf在redis源码目录。 拷贝配置文件到安装目录下 进入源码目录，里面有一份配置文件 redis.conf，然后将其拷贝到安装路径下 cp /usr/local/redis-5.0.5/redis.conf /usr/local/redis/bin/ 2. bin目录结构 进入安装目录bin下 cd /usr/local/redis/bin 目录结构 redis-benchmark: redis 性能检测工具 redis-check-aof： AOF文件修复工具 redis-check-rdb： RDB文件修复工具 redis-cli： 客户端命令行 redis.conf： redis配置文件 redis-sentinel： redis集群 redis-server： redis 服务进程 3. 启动redis 3.1 前端模式启动 直接运行redis-server将以前端模式启动 缺点：ssh命令窗口关闭则redis-server程序结束，不推荐 ./redis-server 如图 3.2 后端模式启动 修改redis.conf 配置文件。daemonize yes 以后端模式启动 vim /usr/local/redis/bin/redis.conf 执行如下命令 # 相对路径情况执行 ./redis-server redis.conf # 全路径执行 /usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf 4. 连接redis /usr/local/redis/bin/redis-cli 5.关闭redis 强行终止redis进程可能会导致redis持久化数据丢失。正确停止Redis的方式应该是向Redis发送SHUTDOWN命令，命令为： cd /usr/local/redis ./bin/redis-cli shutdown 5.1 强行终止redis(不推荐) 会造成数据丢失 pkill redis-server 6. 开机自启动 在/etc/rc.local编辑 vim /etc/rc.local 添加 # redis 开机自启动 /usr/local/redis/bin/redis-server /usr/local/redis/etc/redis-conf "},"redis/install/开启远程访问.html":{"url":"redis/install/开启远程访问.html","title":"开启远程访问","keywords":"","body":"开启远程访问1.1 注释 bind 127.0.0.11.2 protected-mode 改为no1.3 设置远程连接密码1.4 重启redis开启远程访问 1.1 注释 bind 127.0.0.1 注释掉bind 127.0.0.1可以使所有的ip访问redis 若是想指定多个ip访问，但并不是全部的ip访问，可以bind vim /usr/local/redis/bin/redis.conf #bind 127.0.0.1 1.2 protected-mode 改为no 在redis3.2之后，redis增加了protected-mode，在这个模式下，即使注释掉了bind 127.0.0.1，再访问redisd时候还是报错， 将protected-mode 改成no protected-mode no 1.3 设置远程连接密码 如果不设置远程连接密码，那么不用密码就能登录，非常危险，很可能会被挖矿等程序入侵 找到requirepass 去掉注释，改成你要的密码 # requirepass foobared 1.4 重启redis ./redis-server redis.conf "},"redis/use/SpringBoot集成redis使用.html":{"url":"redis/use/SpringBoot集成redis使用.html","title":"Spring Boot集成redis使用","keywords":"","body":"Spring Boot集成redis使用1. 基本集成使用1.1 引入依赖1.2 配置参数1.3 访问测试2. 使用jedis客户端2.1 添加依赖2.2 application.yml配置2.3 Redis配置实例2.4 定义测试类2.5 测试Spring Boot集成redis使用 1. 基本集成使用 1.1 引入依赖 org.springframework.boot spring-boot-starter-data-redis 1.2 配置参数 在application.yml中加入redis服务端的相关配置 spring: redis: host: 120.79.200.111 port: 6379 password: timeout: 200 1.3 访问测试 编写测试用例 @RunWith(SpringRunner.class) @SpringBootTest public class RedisdemoApplicationTests { @Test public void contextLoads() { } @Autowired private StringRedisTemplate stringRedisTemplate; @Test public void test() throws Exception { // 保存字符串 stringRedisTemplate.opsForValue().set(\"aaa\", \"111\"); Assert.assertEquals(\"111\", stringRedisTemplate.opsForValue().get(\"aaa\")); } } 通过上面这段极为简单的测试案例演示了如何通过自动配置的StringRedisTemplate对象进行Redis的读写操作，该对象从命名中就可注意到支持的是String类型。如果有使用过spring-data-redis的开发者一定熟悉RedisTemplate接口，StringRedisTemplate就相当于RedisTemplate的实现。 2. 使用jedis客户端 在spring-boot-starter-data-redis 中默认使用lettuce客户端，我们可以改成使用jedis客户端 2.1 添加依赖 org.springframework.boot spring-boot-starter-data-redis io.lettuce lettuce-core com.alibaba fastjson 1.2.31 org.apache.commons commons-pool2 true redis.clients jedis 2.2 application.yml配置 spring: redis: host: 120.79.200.111 port: 6379 password: jedis: pool: min-idle: 8 max-idle: 500 max-active: 2000 max-wait: 10000 timeout: 0 2.3 Redis配置实例 使用Jackson2JsonRedisSerialize 替换默认序列化，这样就可以直接存储对象 package com.zszdevelop.redisdemo02; import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.connection.RedisPassword; import org.springframework.data.redis.connection.RedisStandaloneConfiguration; import org.springframework.data.redis.connection.jedis.JedisClientConfiguration; import org.springframework.data.redis.connection.jedis.JedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; import org.springframework.util.StringUtils; import redis.clients.jedis.JedisPool; import redis.clients.jedis.JedisPoolConfig; import java.time.Duration; /** * @author zhangshengzhong * @date 2019/10/5 */ @Configuration public class RedisConfig { @Value(\"${spring.redis.host}\") private String host; @Value(\"${spring.redis.port}\") private int port; @Value(\"${spring.redis.password}\") private String password; @Value(\"${spring.redis.timeout}\") private int timeout; @Value(\"${spring.redis.jedis.pool.max-idle}\") private int maxIdle; @Value(\"${spring.redis.jedis.pool.max-wait}\") private long maxWaitMillis; @Bean public JedisPool redisPoolFactory() { JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxIdle(maxIdle); jedisPoolConfig.setMaxWaitMillis(maxWaitMillis); if (!StringUtils.isEmpty(password)){ return new JedisPool(jedisPoolConfig, host, port, timeout, password);} else{ return new JedisPool(jedisPoolConfig, host, port, timeout);} } @Bean JedisConnectionFactory jedisConnectionFactory() { RedisStandaloneConfiguration redisStandaloneConfiguration = new RedisStandaloneConfiguration(); redisStandaloneConfiguration.setHostName(host); redisStandaloneConfiguration.setPort(port); redisStandaloneConfiguration.setPassword(RedisPassword.of(password)); JedisClientConfiguration.JedisClientConfigurationBuilder jedisClientConfiguration = JedisClientConfiguration.builder(); jedisClientConfiguration.connectTimeout(Duration.ofMillis(timeout)); jedisClientConfiguration.usePooling(); return new JedisConnectionFactory(redisStandaloneConfiguration, jedisClientConfiguration.build()); } @Bean(name = \"redisTemplate\") public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate redisTemplate = new RedisTemplate<>(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize 替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和 key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; } } 2.4 定义测试类 需要实现Serializable 和 实现默认构造器 public class User implements Serializable{ private static final long serialVersionUID = -1L; private String username; private Integer age; public User() { } public User(String username, Integer age) { this.username = username; this.age = age; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } } 2.5 测试 @RunWith(SpringRunner.class) @SpringBootTest public class Redisdemo02ApplicationTests { @Test public void contextLoads() { } @Autowired private RedisTemplate template; @Test public void tests() { User user = new User(\"象拔蚌ceside\",1); template.opsForValue().set(user.getUsername(),user); //原本opsForValue()是只能操作字符串的.现在就可以操作对象了 User result = (User) template.opsForValue().get(user.getUsername()); System.out.println(result.toString()); } } "},"redis/use/redis连接客户端选择.html":{"url":"redis/use/redis连接客户端选择.html","title":"redis连接客户端选择：Jedis,Redisson,Lettuce","keywords":"","body":"redis连接客户端选择：Jedis,Redisson,Lettuce1. redis 常见连接客户端2. lettuce和jedis比较redis连接客户端选择：Jedis,Redisson,Lettuce 在spring boot2之后，对redis连接的支持，默认就采用了lettuce。这就一定程度说明了lettuce 和Jedis的优劣。 1. redis 常见连接客户端 Jedis：是老牌的Redis的Java实现客户端，提供了比较全面的Redis命令的支持 优势：比较全面的提供了Redis的操作特性 Redisson：实现了分布式和可扩展的Java数据结构。 优势：促使使用者对Redis的关注分离，提供很多分布式相关操作服务，例如，分布式锁，分布式集合，可通过Redis支持延迟队列 Lettuce：高级Redis客户端，用于线程安全同步，异步和响应使用，支持集群，Sentinel，管道和编码器。 优势：基于Netty框架的事件驱动的通信层，其方法调用是异步的。Lettuce的API是线程安全的，所以可以操作单个Lettuce连接来完成各种操作 2. lettuce和jedis比较 jedis是直接连接redis server,如果在多线程环境下是非线程安全的，这个时候只有使用连接池，为每个jedis实例增加物理连接 ； lettuce的连接是基于Netty的，连接实例（StatefulRedisConnection）可以在多个线程间并发访问，StatefulRedisConnection是线程安全的，所以一个连接实例可以满足多线程环境下的并发访问，当然这也是可伸缩的设计，一个连接实例不够的情况也可以按需增加连接实例。 Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 总结： 优先使用Lettuce，如果需要分布式锁，分布式集合等分布式的高级特性，添加Redisson结合使用，因为Redisson本身对字符串的操作支持很差。 "},"redis/use/JedisPool资源池优化.html":{"url":"redis/use/JedisPool资源池优化.html","title":"JedisPool资源池优化","keywords":"","body":"JedisPool资源池优化1. 背景2. 使用方法2.1 依赖关系3. 参数说明3.1 资源设置和使用3.2 空闲资源监测4. 资源池大小(maxTotal)、空闲(maxIdle minIdle)设置建议4.1.maxTotal：最大连接数4.2. maxIdle minIdle4.3.监控参考文章JedisPool资源池优化 1. 背景 合理的jedisPool 资源池参数设置能为业务使用Redis保驾护航，本文将对JedisPool的使用、资源池的参数进行详细说明，最后给出“最合理”的配置 2. 使用方法 2.1 依赖关系 以官方的2.9.0为例子(Jedis Release)，Maven依赖如下： redis.clients jedis 2.9.0 compile org.apache.commons commons-pool2 true Jedis使用apache commons-pool2对Jedis资源池进行管理，所以在定义JedisPool时一个很重要的参数就是资源池GenericObjectPoolConfig，使用方式如下，其中有很多资源管理和使用的参数(具体看第二节) 注意：后面会提到建议用JedisPoolConfig代替GenericObjectPoolConfig GenericObjectPoolConfig jedisPoolConfig = new GenericObjectPoolConfig(); jedisPoolConfig.setMaxTotal(..); jedisPoolConfig.setMaxIdle(..); jedisPoolConfig.setMinIdle(..); jedisPoolConfig.setMaxWaitMillis(..); ... JedisPool的初始化如下： // redisHost和redisPort是实例的IP和端口 // redisPassword是实例的密码 // timeout，这里既是连接超时又是读写超时，从Jedis 2.8开始有区分connectionTimeout和soTimeout的构造函数 JedisPool jedisPool = new JedisPool(jedisPoolConfig, redisHost, redisPort, timeout, redisPassword); 执行命令如下： Jedis jedis = null; try { jedis = jedisPool.getResource(); //具体的命令 jedis.executeCommand() } catch (Exception e) { logger.error(e.getMessage(), e); } finally { //注意这里不是关闭连接，在JedisPool模式下，Jedis会被归还给资源池。 if (jedis != null) jedis.close(); } 3. 参数说明 JedisPool保证资源在一个可控范围内，并且提供了线程安全，但是一个合理的GenericObjectPoolConfig配置能为应用使用Redis保驾护航，下面将对它的一些重要参数进行说明和建议： 在当前环境下，Jedis连接就是资源，JedisPool管理的就是Jedis连接。 3.1 资源设置和使用 序号 参数名 含义 默认值 使用建议 1 maxTotal 资源池中最大连接数 8 设置建议见下节 2 maxIdle 资源池允许最大空闲的连接数 8 设置建议见下节 3 minIdle 资源池确保最少空闲的连接数 0 设置建议见下节 4 blockWhenExhausted 当资源池用尽后，调用者是否要等待。只有当为true时，下面的maxWaitMillis才会生效 true 建议使用默认值 5 maxWaitMillis 当资源池连接用尽后，调用者的最大等待时间(单位为毫秒) -1：表示永不超时 不建议使用默认值 6 testOnBorrow 向资源池借用连接时是否做连接有效性检测(ping)，无效连接会被移除 false 业务量很大时候建议设置为false(多一次ping的开销)。 7 testOnReturn 向资源池归还连接时是否做连接有效性检测(ping)，无效连接会被移除 false 业务量很大时候建议设置为false(多一次ping的开销)。 8 jmxEnabled 是否开启jmx监控，可用于监控 true 建议开启，但应用本身也要开启 3.2 空闲资源监测 空闲Jedis对象检测，下面四个参数组合来完成，testWhileIdle是该功能的开关。 序号 参数名 含义 默认值 使用建议 1 testWhileIdle 是否开启空闲资源监测 false true 2 timeBetweenEvictionRunsMillis 空闲资源的检测周期(单位为毫秒) -1：不检测 建议设置，周期自行选择，也可以默认也可以使用下面JedisPoolConfig中的配置 3 minEvictableIdleTimeMillis 资源池中资源最小空闲时间(单位为毫秒)，达到此值后空闲资源将被移除 100060 30 = 30分钟 可根据自身业务决定，大部分默认值即可，也可以考虑使用下面JeidsPoolConfig中的配置 4 numTestsPerEvictionRun 做空闲资源检测时，每次的采样数 3 可根据自身应用连接数进行微调,如果设置为-1，就是对所有连接做空闲监测 为了方便使用，Jedis提供了JedisPoolConfig，它本身继承了GenericObjectPoolConfig设置了一些空闲监测设置 public class JedisPoolConfig extends GenericObjectPoolConfig { public JedisPoolConfig() { // defaults to make your life with connection pool easier :) setTestWhileIdle(true); // setMinEvictableIdleTimeMillis(60000); // setTimeBetweenEvictionRunsMillis(30000); setNumTestsPerEvictionRun(-1); } } 所有默认值可以从org.apache.commons.pool2.impl.BaseObjectPoolConfig中看到。 4. 资源池大小(maxTotal)、空闲(maxIdle minIdle)设置建议 4.1.maxTotal：最大连接数 实际上这个是一个很难回答的问题，考虑的因素比较多： 业务希望Redis并发量 客户端执行命令时间 Redis资源：例如 nodes(例如应用个数) * maxTotal 是不能超过redis的最大连接数。 资源开销：例如虽然希望控制空闲连接，但是不希望因为连接池的频繁释放创建连接造成不必靠开销。 以一个例子说明，假设: 一次命令时间（borrow|return resource + Jedis执行命令(含网络) ）的平均耗时约为1ms，一个连接的QPS大约是1000 业务期望的QPS是50000 那么理论上需要的资源池大小是50000 / 1000 = 50个。但事实上这是个理论值，还要考虑到要比理论值预留一些资源，通常来讲maxTotal可以比理论值大一些。 但这个值不是越大越好，一方面连接太多占用客户端和服务端资源，另一方面对于Redis这种高QPS的服务器，一个大命令的阻塞即使设置再大资源池仍然会无济于事。 4.2. maxIdle minIdle maxIdle实际上才是业务需要的最大连接数，maxTotal是为了给出余量，所以maxIdle不要设置过小，否则会有new Jedis(新连接)开销，而minIdle是为了控制空闲资源监测。 连接池的最佳性能是maxTotal = maxIdle ,这样就避免连接池伸缩带来的性能干扰。但是如果并发量不大或者maxTotal设置过高，会导致不必要的连接资源浪费。 可以根据实际总OPS和调用redis客户端的规模整体评估每个节点所使用的连接池。 4.3.监控 实际上最靠谱的值是通过监控来得到“最佳值”的，可以考虑通过一些手段(例如jmx)实现监控，找到合理值。 参考文章 JedisPool资源池优化 "},"redis/use/RedisUtil工具类的使用.html":{"url":"redis/use/RedisUtil工具类的使用.html","title":"RedisUtil工具类的使用(仅供参考)","keywords":"","body":"RedisUtil工具类的使用(仅供参考)1. 用法一、keys相关命令二、String数据类型操作三、Hash相关的操作四、List相关的操作五、Set相关的操作六、zset数据类型操作2. 代码RedisUtil工具类的使用(仅供参考) 1. 用法 一、keys相关命令 NO 方法 描述 1 void delete(String key) key存在时删除key 2 void delete(Collection keys) 批量删除key 3 byte[] dump(String key) 序列化key，返回被序列化的值 4 Boolean hasKey(String key) 检查key是否存在 5 Boolean expire(String key, long timeout, TimeUnit unit) 设置过期时间 6 Boolean expireAt(String key, Date date) 设置过期时间 7 Set keys(String pattern) 查找所有符合给定模式(pattern)的key 8 Boolean move(String key, int dbIndex) 将当前数据库的key移动到给定的数据库db当中 9 Boolean persist(String key) 移除key的过期时间，key将持久保持 10 Long getExpire(String key, TimeUnit unit) 返回key的剩余的过期时间 11 Long getExpire(String key) 返回key的剩余的过期时间 12 String randomKey() 从当前数据库中随机返回一个key 13 void rename(String oldKey, String newKey) 修改key的名称 14 Boolean renameIfAbsent(String oldKey, String newKey) 仅当newkey不存在时，将oldKey改名为 newkey 15 DataType type(String key) 返回key所储存的值的类型 TimeUnit是时间单位，可选值有：  天:TimeUnit.DAYS、小时:TimeUnit.HOURS、分钟:TimeUnit.MINUTES、秒:TimeUnit.SECONDS、毫秒:TimeUnit.MILLISECONDS。 二、String数据类型操作 NO 方法 描述 1 String get(String key) 获取指定key的值 2 String getRange(String key, long start, long end) 返回key中字符串值的子字符 3 String getAndSet(String key, String value) 将key的值设为value，并返回key旧值 4 Boolean getBit(String key, long offset) 对key所储存的值，获取指定位置上的bit 5 List multiGet(Collection keys) 批量获取 添加相关 6 void set(String key, String value) 设置指定key的值 7 boolean setBit(String key, long offset, boolean value) 设置指定位置上的ASCII码 8 void setEx(String key,String value,long timeout,TimeUnit unit) 将值value关联到key，并设置key过期时间 9 boolean setIfAbsent(String key, String value) 只有在 key 不存在时设置 key 的值 10 void setRange(String key, String value, long offset) 用value覆写key的值，从偏移量offset开始 11 void multiSet(Map maps) 批量添加 12 boolean multiSetIfAbsent(Map maps) 批量添加，仅当所有key都不存在 其他方法 13 Integer append(String key, String value) 追加到末尾 14 Long incrBy(String key, long increment) 增加(自增长), 负数则为自减 15 Double incrByFloat(String key, double increment) 增加(自增长), 负数则为自减 16 Long size(String key) 获取字符串的长度 关于上面xxBit方法的使用：  例如字符'a'的ASCII码是97，转为二进制是'01100001'，setBit方法就是把第offset位置上变成0或者1，true是1，false是0。 三、Hash相关的操作 NO 方法 描述 1 Object hGet(String key, String field) 获取存储在哈希表中指定字段的值 2 Map hGetAll(String key) 获取所有给定字段的值 3 List hMultiGet(String key, Collection fields) 获取所有给定字段的值 添加相关 4 void hPut(String key, String hashKey, String value) 添加字段 5 void hPutAll(String key, Map maps) 添加多个字段 6 Boolean hPutIfAbsent(String key,String hashKey,String value) 仅当hashKey不存在时才设置 其他方法 7 Long hDelete(String key, Object... fields) 删除一个或多个哈希表字段 8 boolean hExists(String key, String field) 查看哈希表key中指定的字段是否存在 9 Long hIncrBy(String key, Object field, long increment) 为哈希表key中指定字段的值增加increment 10 Double hIncrByFloat(String key, Object field, double delta) 为哈希表key中指定字段的值增加increment 11 Set hKeys(String key) 获取所有哈希表中的字段 12 Long hSize(String key) 获取哈希表中字段的数量 13 List hValues(String key) 获取哈希表中所有值 14 Cursor hScan(String key, ScanOptions options) 迭代哈希表中的键值对 引入序列化依赖 com.dyuproject.protostuff protostuff-core 1.0.8 com.dyuproject.protostuff protostuff-runtime 1.0.8 15 getListCache(final String key, Class targetClass) 获取缓存中的List，targetClass是序列化的类 16 putListCacheWithExpireTime(String key, List objList, final long expireTime) 把List放到缓存，expireTime是过期策略 四、List相关的操作 NO 方法 描述 1 String lIndex(String key, long index) 通过索引获取列表中的元素 2 List lRange(String key, long start, long end) 获取列表指定范围内的元素 添加相关 3 Long lLeftPush(String key, String value) 存储在list头部 4 Long lLeftPushAll(String key, String... value) 存储在list头部 5 Long lLeftPushAll(String key, Collection value) 存储在list头部 6 Long lLeftPushIfPresent(String key, String value) 当list存在的时候才加入 7 lLeftPush(String key, String pivot, String value) 如果pivot存在,再pivot前面添加 8 Long lRightPush(String key, String value) 存储在list尾部 9 Long lRightPushAll(String key, String... value) 存储在list尾部 10 Long lRightPushAll(String key, Collection value) 存储在list尾部 11 Long lRightPushIfPresent(String key, String value) 当list存在的时候才加入 12 lRightPush(String key, String pivot, String value) 在pivot元素的右边添加值 13 void lSet(String key, long index, String value) 通过索引设置列表元素的值 删除相关 14 String lLeftPop(String key) 移出并获取列表的第一个元素 15 String lBLeftPop(String key,long timeout,TimeUnit unit) 移出并获取第一个元素,没有则阻塞直到超时或有为止 16 String lRightPop(String key) 移除并获取列表最后一个元素 17 String lBRightPop(String key,long timeout,TimeUnit unit) 移出并获取最后个元素,没有则阻塞直到超时或有为止 18 String lRightPopAndLeftPush(String sKey,String dKey) 移除最后一个元素并加到另一个列表并返回 19 String lBRightPopAndLeftPush(sKey,dKey,timeout,unit) 移除最后个元素并加到另个列表并返回,阻塞超时或有 20 Long lRemove(String key, long index, String value) 删除集合中值等于value得元素 21 void lTrim(String key, long start, long end) 裁剪list 其他方法 22 Long lLen(String key) 获取列表长度 五、Set相关的操作 NO 方法 描述 1 Set sMembers(String key) 获取集合所有元素 2 Long sSize(String key) 获取集合大小 3 Boolean sIsMember(String key, Object value) 判断集合是否包含value 4 String sRandomMember(String key) 随机获取集合中的一个元素 5 List sRandomMembers(String key, long count) 随机获取集合count个元素 6 Set sDistinctRandomMembers(String key, long count) 随机获取count个元素并去除重复的 7 Cursor sScan(String key, ScanOptions options) 使用迭代器获取元素 8 Set sIntersect(String key, String otherKey) 获取两个集合的交集 9 Set sIntersect(String key, Collection otherKeys) 获取key集合与多个集合的交集 10 Long sIntersectAndStore(String key, String oKey, String dKey) key集合与oKey的交集存储到dKey中 11 Long sIntersectAndStore(String key,Collection oKeys,String dKey) key与多个集合的交集存储到dKey中 12 Set sUnion(String key, String otherKeys) 获取两个集合的并集 13 Set sUnion(String key, Collection otherKeys) 获取key集合与多个集合的并集 14 Long sUnionAndStore(String key, String otherKey, String destKey) key集合与oKey的并集存储到dKey中 15 Long sUnionAndStore(String key,Collection oKeys,String dKey) key与多个集合的并集存储到dKey中 16 Set sDifference(String key, String otherKey) 获取两个集合的差集 17 Set sDifference(String key, Collection otherKeys) 获取key集合与多个集合的差集 18 Long sDifference(String key, String otherKey, String destKey) key与oKey集合的差集存储到dKey中 19 Long sDifference(String key,Collection otherKeys,String dKey) key与多个集合的差集存储到dKey中 添加相关 20 Long sAdd(String key, String... values) 添加 删除相关 21 Long sRemove(String key, Object... values) 移除 22 String sPop(String key) 随机移除一个元素 23 Boolean sMove(String key, String value, String destKey) 将key集合中value移到destKey中 六、zset数据类型操作 NO 方法 描述 1 Set zRange(String key, long start, long end) 获取元素,小到大排序,s开始e结束位置 2 Set zRangeWithScores(String key, long start, long end) 获取集合元素, 并且把score值也获取 3 Set zRangeByScore(String key, double min, double max) 根据score范围查询元素,从小到大排序 4 Set zRangeByScoreWithScores(key,double min,double max) 根据score范围查询元素,并返回score 5 Set zRangeByScoreWithScores(key,double min,max,long start,end) 根据score查询元素,s开始e结束位置 6 Set zReverseRange(String key, long start, long end) 获取集合元素, 从大到小排序 7 Set zReverseRangeWithScores(key, long start, long end) 获取元素,从大到小排序,并返回score 8 Set zReverseRangeByScore(String key, double min, double max) 根据score范围查询元素,从大到小排序 9 Set zReverseRangeByScoreWithScores(key,double min,double max) 根据score查询,大到小排序返回score 10 Set zReverseRangeByScore(key, double min, max, long start, end) 根据score查询,大到小,s开始e结束 11 Long zRank(String key, Object value) 返回元素在集合的排名,score由小到大 12 Long zReverseRank(String key, Object value) 返回元素在集合的排名,score由大到小 13 Long zCount(String key, double min, double max) 根据score值范围获取集合元素的数量 14 Long zSize(String key) 获取集合大小 15 Long zZCard(String key) 获取集合大小 16 Double zScore(String key, Object value) 获取集合中value元素的score值 17 Long zUnionAndStore(String key, String otherKey, String destKey) 获取key和oKey的并集并存储在dKey中 18 Long zUnionAndStore(String key,Collection otherKeys,String dKey) 获取key和多个集合并集并存在dKey中 19 Long zIntersectAndStore(String key, String otherKey, String destKey) 获取key和oKey交集并存在destKey中 20 Long zIntersectAndStore(String key,Collection oKeys,String dKey) 获取key和多个集合交集并存在dKey中 21 Cursor zScan(String key, ScanOptions options) 使用迭代器获取 添加相关 22 Boolean zAdd(String key, String value, double score) 添加元素,zSet按score由小到大排列 23 Long zAdd(String key, Set values) 批量添加,TypedTuple使用见下面介绍 删除相关 24 Long zRemove(String key, Object... values) 移除 25 Double zIncrementScore(String key, String value, double delta) 增加元素的score值,并返回增加后的值 26 Long zRemoveRange(String key, long start, long end) 移除指定索引位置的成员 27 Long zRemoveRangeByScore(String key, double min, double max) 根据指定的score值的范围来移除成员 批量添加时TypedTuple的使用： TypedTuple typedTuple = new DefaultTypedTuple(value,score) 2. 代码 package com.zszdevelop.redisdemo02; import org.springframework.dao.DataAccessException; import org.springframework.data.redis.connection.DataType; import org.springframework.data.redis.connection.RedisConnection; import org.springframework.data.redis.core.*; import org.springframework.stereotype.Component; import java.util.*; import java.util.concurrent.TimeUnit; /** * Redis工具类 * @author zhangshengzhong * @date 2019/10/5 */ @Component public class RedisUtil { @Autowire private StringRedisTemplate redisTemplate; public void setRedisTemplate(StringRedisTemplate redisTemplate) { this.redisTemplate = redisTemplate; } public StringRedisTemplate getRedisTemplate() { return this.redisTemplate; } /** -------------------key相关操作--------------------- */ /** * 删除key * * @param key */ public void delete(String key) { redisTemplate.delete(key); } /** * 批量删除key * * @param keys */ public void delete(Collection keys) { redisTemplate.delete(keys); } /** * 序列化key * * @param key * @return */ public byte[] dump(String key) { return redisTemplate.dump(key); } /** * 是否存在key * * @param key * @return */ public Boolean hasKey(String key) { return redisTemplate.hasKey(key); } /** * 设置过期时间 * * @param key * @param timeout * @param unit * @return */ public Boolean expire(String key, long timeout, TimeUnit unit) { return redisTemplate.expire(key, timeout, unit); } /** * 设置过期时间 * * @param key * @param date * @return */ public Boolean expireAt(String key, Date date) { return redisTemplate.expireAt(key, date); } /** * 查找匹配的key * * @param pattern * @return */ public Set keys(String pattern) { return redisTemplate.keys(pattern); } /** * 将当前数据库的 key 移动到给定的数据库 db 当中 * * @param key * @param dbIndex * @return */ public Boolean move(String key, int dbIndex) { return redisTemplate.move(key, dbIndex); } /** * 移除 key 的过期时间，key 将持久保持 * * @param key * @return */ public Boolean persist(String key) { return redisTemplate.persist(key); } /** * 返回 key 的剩余的过期时间 * * @param key * @param unit * @return */ public Long getExpire(String key, TimeUnit unit) { return redisTemplate.getExpire(key, unit); } /** * 返回 key 的剩余的过期时间 * * @param key * @return */ public Long getExpire(String key) { return redisTemplate.getExpire(key); } /** * 从当前数据库中随机返回一个 key * * @return */ public String randomKey() { return redisTemplate.randomKey(); } /** * 修改 key 的名称 * * @param oldKey * @param newKey */ public void rename(String oldKey, String newKey) { redisTemplate.rename(oldKey, newKey); } /** * 仅当 newkey 不存在时，将 oldKey 改名为 newkey * * @param oldKey * @param newKey * @return */ public Boolean renameIfAbsent(String oldKey, String newKey) { return redisTemplate.renameIfAbsent(oldKey, newKey); } /** * 返回 key 所储存的值的类型 * * @param key * @return */ public DataType type(String key) { return redisTemplate.type(key); } /** -------------------string相关操作--------------------- */ /** * 设置指定 key 的值 * @param key * @param value */ public void set(String key, String value) { redisTemplate.opsForValue().set(key, value); } /** * 获取指定 key 的值 * @param key * @return */ public String get(String key) { return redisTemplate.opsForValue().get(key); } /** * 返回 key 中字符串值的子字符 * @param key * @param start * @param end * @return */ public String getRange(String key, long start, long end) { return redisTemplate.opsForValue().get(key, start, end); } /** * 将给定 key 的值设为 value ，并返回 key 的旧值(old value) * * @param key * @param value * @return */ public String getAndSet(String key, String value) { return redisTemplate.opsForValue().getAndSet(key, value); } /** * 对 key 所储存的字符串值，获取指定偏移量上的位(bit) * * @param key * @param offset * @return */ public Boolean getBit(String key, long offset) { return redisTemplate.opsForValue().getBit(key, offset); } /** * 批量获取 * * @param keys * @return */ public List multiGet(Collection keys) { return redisTemplate.opsForValue().multiGet(keys); } /** * 设置ASCII码, 字符串'a'的ASCII码是97, 转为二进制是'01100001', 此方法是将二进制第offset位值变为value * * @param key * 位置 * @param value * 值,true为1, false为0 * @return */ public boolean setBit(String key, long offset, boolean value) { return redisTemplate.opsForValue().setBit(key, offset, value); } /** * 将值 value 关联到 key ，并将 key 的过期时间设为 timeout * * @param key * @param value * @param timeout * 过期时间 * @param unit * 时间单位, 天:TimeUnit.DAYS 小时:TimeUnit.HOURS 分钟:TimeUnit.MINUTES * 秒:TimeUnit.SECONDS 毫秒:TimeUnit.MILLISECONDS */ public void setEx(String key, String value, long timeout, TimeUnit unit) { redisTemplate.opsForValue().set(key, value, timeout, unit); } /** * 只有在 key 不存在时设置 key 的值 * * @param key * @param value * @return 之前已经存在返回false,不存在返回true */ public boolean setIfAbsent(String key, String value) { return redisTemplate.opsForValue().setIfAbsent(key, value); } /** * 用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始 * * @param key * @param value * @param offset * 从指定位置开始覆写 */ public void setRange(String key, String value, long offset) { redisTemplate.opsForValue().set(key, value, offset); } /** * 获取字符串的长度 * * @param key * @return */ public Long size(String key) { return redisTemplate.opsForValue().size(key); } /** * 批量添加 * * @param maps */ public void multiSet(Map maps) { redisTemplate.opsForValue().multiSet(maps); } /** * 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在 * * @param maps * @return 之前已经存在返回false,不存在返回true */ public boolean multiSetIfAbsent(Map maps) { return redisTemplate.opsForValue().multiSetIfAbsent(maps); } /** * 增加(自增长), 负数则为自减 * * @param key * @param increment * @return */ public Long incrBy(String key, long increment) { return redisTemplate.opsForValue().increment(key, increment); } /** * * @param key * @param increment * @return */ public Double incrByFloat(String key, double increment) { return redisTemplate.opsForValue().increment(key, increment); } /** * 追加到末尾 * * @param key * @param value * @return */ public Integer append(String key, String value) { return redisTemplate.opsForValue().append(key, value); } /** -------------------hash相关操作------------------------- */ /** * 获取存储在哈希表中指定字段的值 * * @param key * @param field * @return */ public Object hGet(String key, String field) { return redisTemplate.opsForHash().get(key, field); } /** * 获取所有给定字段的值 * * @param key * @return */ public Map hGetAll(String key) { return redisTemplate.opsForHash().entries(key); } /** * 获取所有给定字段的值 * * @param key * @param fields * @return */ public List hMultiGet(String key, Collection fields) { return redisTemplate.opsForHash().multiGet(key, fields); } public void hPut(String key, String hashKey, String value) { redisTemplate.opsForHash().put(key, hashKey, value); } public void hPutAll(String key, Map maps) { redisTemplate.opsForHash().putAll(key, maps); } /** * 仅当hashKey不存在时才设置 * * @param key * @param hashKey * @param value * @return */ public Boolean hPutIfAbsent(String key, String hashKey, String value) { return redisTemplate.opsForHash().putIfAbsent(key, hashKey, value); } /** * 删除一个或多个哈希表字段 * * @param key * @param fields * @return */ public Long hDelete(String key, Object... fields) { return redisTemplate.opsForHash().delete(key, fields); } /** * 查看哈希表 key 中，指定的字段是否存在 * * @param key * @param field * @return */ public boolean hExists(String key, String field) { return redisTemplate.opsForHash().hasKey(key, field); } /** * 为哈希表 key 中的指定字段的整数值加上增量 increment * * @param key * @param field * @param increment * @return */ public Long hIncrBy(String key, Object field, long increment) { return redisTemplate.opsForHash().increment(key, field, increment); } /** * 为哈希表 key 中的指定字段的整数值加上增量 increment * * @param key * @param field * @param delta * @return */ public Double hIncrByFloat(String key, Object field, double delta) { return redisTemplate.opsForHash().increment(key, field, delta); } /** * 获取所有哈希表中的字段 * * @param key * @return */ public Set hKeys(String key) { return redisTemplate.opsForHash().keys(key); } /** * 获取哈希表中字段的数量 * * @param key * @return */ public Long hSize(String key) { return redisTemplate.opsForHash().size(key); } /** * 获取哈希表中所有值 * * @param key * @return */ public List hValues(String key) { return redisTemplate.opsForHash().values(key); } /** * 迭代哈希表中的键值对 * * @param key * @param options * @return */ public Cursor> hScan(String key, ScanOptions options) { return redisTemplate.opsForHash().scan(key, options); } /** ------------------------list相关操作---------------------------- */ /** * 通过索引获取列表中的元素 * * @param key * @param index * @return */ public String lIndex(String key, long index) { return redisTemplate.opsForList().index(key, index); } /** * 获取列表指定范围内的元素 * * @param key * @param start * 开始位置, 0是开始位置 * @param end * 结束位置, -1返回所有 * @return */ public List lRange(String key, long start, long end) { return redisTemplate.opsForList().range(key, start, end); } /** * 存储在list头部 * * @param key * @param value * @return */ public Long lLeftPush(String key, String value) { return redisTemplate.opsForList().leftPush(key, value); } /** * * @param key * @param value * @return */ public Long lLeftPushAll(String key, String... value) { return redisTemplate.opsForList().leftPushAll(key, value); } /** * * @param key * @param value * @return */ public Long lLeftPushAll(String key, Collection value) { return redisTemplate.opsForList().leftPushAll(key, value); } /** * 当list存在的时候才加入 * * @param key * @param value * @return */ public Long lLeftPushIfPresent(String key, String value) { return redisTemplate.opsForList().leftPushIfPresent(key, value); } /** * 如果pivot存在,再pivot前面添加 * * @param key * @param pivot * @param value * @return */ public Long lLeftPush(String key, String pivot, String value) { return redisTemplate.opsForList().leftPush(key, pivot, value); } /** * * @param key * @param value * @return */ public Long lRightPush(String key, String value) { return redisTemplate.opsForList().rightPush(key, value); } /** * * @param key * @param value * @return */ public Long lRightPushAll(String key, String... value) { return redisTemplate.opsForList().rightPushAll(key, value); } /** * * @param key * @param value * @return */ public Long lRightPushAll(String key, Collection value) { return redisTemplate.opsForList().rightPushAll(key, value); } /** * 为已存在的列表添加值 * * @param key * @param value * @return */ public Long lRightPushIfPresent(String key, String value) { return redisTemplate.opsForList().rightPushIfPresent(key, value); } /** * 在pivot元素的右边添加值 * * @param key * @param pivot * @param value * @return */ public Long lRightPush(String key, String pivot, String value) { return redisTemplate.opsForList().rightPush(key, pivot, value); } /** * 通过索引设置列表元素的值 * * @param key * @param index * 位置 * @param value */ public void lSet(String key, long index, String value) { redisTemplate.opsForList().set(key, index, value); } /** * 移出并获取列表的第一个元素 * * @param key * @return 删除的元素 */ public String lLeftPop(String key) { return redisTemplate.opsForList().leftPop(key); } /** * 移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 * * @param key * @param timeout * 等待时间 * @param unit * 时间单位 * @return */ public String lBLeftPop(String key, long timeout, TimeUnit unit) { return redisTemplate.opsForList().leftPop(key, timeout, unit); } /** * 移除并获取列表最后一个元素 * * @param key * @return 删除的元素 */ public String lRightPop(String key) { return redisTemplate.opsForList().rightPop(key); } /** * 移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 * * @param key * @param timeout * 等待时间 * @param unit * 时间单位 * @return */ public String lBRightPop(String key, long timeout, TimeUnit unit) { return redisTemplate.opsForList().rightPop(key, timeout, unit); } /** * 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 * * @param sourceKey * @param destinationKey * @return */ public String lRightPopAndLeftPush(String sourceKey, String destinationKey) { return redisTemplate.opsForList().rightPopAndLeftPush(sourceKey, destinationKey); } /** * 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 * * @param sourceKey * @param destinationKey * @param timeout * @param unit * @return */ public String lBRightPopAndLeftPush(String sourceKey, String destinationKey, long timeout, TimeUnit unit) { return redisTemplate.opsForList().rightPopAndLeftPush(sourceKey, destinationKey, timeout, unit); } /** * 删除集合中值等于value得元素 * * @param key * @param index * index=0, 删除所有值等于value的元素; index>0, 从头部开始删除第一个值等于value的元素; * index sIntersect(String key, String otherKey) { return redisTemplate.opsForSet().intersect(key, otherKey); } /** * 获取key集合与多个集合的交集 * * @param key * @param otherKeys * @return */ public Set sIntersect(String key, Collection otherKeys) { return redisTemplate.opsForSet().intersect(key, otherKeys); } /** * key集合与otherKey集合的交集存储到destKey集合中 * * @param key * @param otherKey * @param destKey * @return */ public Long sIntersectAndStore(String key, String otherKey, String destKey) { return redisTemplate.opsForSet().intersectAndStore(key, otherKey, destKey); } /** * key集合与多个集合的交集存储到destKey集合中 * * @param key * @param otherKeys * @param destKey * @return */ public Long sIntersectAndStore(String key, Collection otherKeys, String destKey) { return redisTemplate.opsForSet().intersectAndStore(key, otherKeys, destKey); } /** * 获取两个集合的并集 * * @param key * @param otherKeys * @return */ public Set sUnion(String key, String otherKeys) { return redisTemplate.opsForSet().union(key, otherKeys); } /** * 获取key集合与多个集合的并集 * * @param key * @param otherKeys * @return */ public Set sUnion(String key, Collection otherKeys) { return redisTemplate.opsForSet().union(key, otherKeys); } /** * key集合与otherKey集合的并集存储到destKey中 * * @param key * @param otherKey * @param destKey * @return */ public Long sUnionAndStore(String key, String otherKey, String destKey) { return redisTemplate.opsForSet().unionAndStore(key, otherKey, destKey); } /** * key集合与多个集合的并集存储到destKey中 * * @param key * @param otherKeys * @param destKey * @return */ public Long sUnionAndStore(String key, Collection otherKeys, String destKey) { return redisTemplate.opsForSet().unionAndStore(key, otherKeys, destKey); } /** * 获取两个集合的差集 * * @param key * @param otherKey * @return */ public Set sDifference(String key, String otherKey) { return redisTemplate.opsForSet().difference(key, otherKey); } /** * 获取key集合与多个集合的差集 * * @param key * @param otherKeys * @return */ public Set sDifference(String key, Collection otherKeys) { return redisTemplate.opsForSet().difference(key, otherKeys); } /** * key集合与otherKey集合的差集存储到destKey中 * * @param key * @param otherKey * @param destKey * @return */ public Long sDifference(String key, String otherKey, String destKey) { return redisTemplate.opsForSet().differenceAndStore(key, otherKey, destKey); } /** * key集合与多个集合的差集存储到destKey中 * * @param key * @param otherKeys * @param destKey * @return */ public Long sDifference(String key, Collection otherKeys, String destKey) { return redisTemplate.opsForSet().differenceAndStore(key, otherKeys, destKey); } /** * 获取集合所有元素 * * @param key * @return */ public Set setMembers(String key) { return redisTemplate.opsForSet().members(key); } /** * 随机获取集合中的一个元素 * * @param key * @return */ public String sRandomMember(String key) { return redisTemplate.opsForSet().randomMember(key); } /** * 随机获取集合中count个元素 * * @param key * @param count * @return */ public List sRandomMembers(String key, long count) { return redisTemplate.opsForSet().randomMembers(key, count); } /** * 随机获取集合中count个元素并且去除重复的 * * @param key * @param count * @return */ public Set sDistinctRandomMembers(String key, long count) { return redisTemplate.opsForSet().distinctRandomMembers(key, count); } /** * * @param key * @param options * @return */ public Cursor sScan(String key, ScanOptions options) { return redisTemplate.opsForSet().scan(key, options); } /**------------------zSet相关操作--------------------------------*/ /** * 添加元素,有序集合是按照元素的score值由小到大排列 * * @param key * @param value * @param score * @return */ public Boolean zAdd(String key, String value, double score) { return redisTemplate.opsForZSet().add(key, value, score); } /** * * @param key * @param values * @return */ public Long zAdd(String key, Set> values) { return redisTemplate.opsForZSet().add(key, values); } /** * * @param key * @param values * @return */ public Long zRemove(String key, Object... values) { return redisTemplate.opsForZSet().remove(key, values); } /** * 增加元素的score值，并返回增加后的值 * * @param key * @param value * @param delta * @return */ public Double zIncrementScore(String key, String value, double delta) { return redisTemplate.opsForZSet().incrementScore(key, value, delta); } /** * 返回元素在集合的排名,有序集合是按照元素的score值由小到大排列 * * @param key * @param value * @return 0表示第一位 */ public Long zRank(String key, Object value) { return redisTemplate.opsForZSet().rank(key, value); } /** * 返回元素在集合的排名,按元素的score值由大到小排列 * * @param key * @param value * @return */ public Long zReverseRank(String key, Object value) { return redisTemplate.opsForZSet().reverseRank(key, value); } /** * 获取集合的元素, 从小到大排序 * * @param key * @param start * 开始位置 * @param end * 结束位置, -1查询所有 * @return */ public Set zRange(String key, long start, long end) { return redisTemplate.opsForZSet().range(key, start, end); } /** * 获取集合元素, 并且把score值也获取 * * @param key * @param start * @param end * @return */ public Set> zRangeWithScores(String key, long start, long end) { return redisTemplate.opsForZSet().rangeWithScores(key, start, end); } /** * 根据Score值查询集合元素 * * @param key * @param min * 最小值 * @param max * 最大值 * @return */ public Set zRangeByScore(String key, double min, double max) { return redisTemplate.opsForZSet().rangeByScore(key, min, max); } /** * 根据Score值查询集合元素, 从小到大排序 * * @param key * @param min * 最小值 * @param max * 最大值 * @return */ public Set> zRangeByScoreWithScores(String key, double min, double max) { return redisTemplate.opsForZSet().rangeByScoreWithScores(key, min, max); } /** * * @param key * @param min * @param max * @param start * @param end * @return */ public Set> zRangeByScoreWithScores(String key, double min, double max, long start, long end) { return redisTemplate.opsForZSet().rangeByScoreWithScores(key, min, max, start, end); } /** * 获取集合的元素, 从大到小排序 * * @param key * @param start * @param end * @return */ public Set zReverseRange(String key, long start, long end) { return redisTemplate.opsForZSet().reverseRange(key, start, end); } /** * 获取集合的元素, 从大到小排序, 并返回score值 * * @param key * @param start * @param end * @return */ public Set> zReverseRangeWithScores(String key, long start, long end) { return redisTemplate.opsForZSet().reverseRangeWithScores(key, start, end); } /** * 根据Score值查询集合元素, 从大到小排序 * * @param key * @param min * @param max * @return */ public Set zReverseRangeByScore(String key, double min, double max) { return redisTemplate.opsForZSet().reverseRangeByScore(key, min, max); } /** * 根据Score值查询集合元素, 从大到小排序 * * @param key * @param min * @param max * @return */ public Set> zReverseRangeByScoreWithScores( String key, double min, double max) { return redisTemplate.opsForZSet().reverseRangeByScoreWithScores(key, min, max); } /** * * @param key * @param min * @param max * @param start * @param end * @return */ public Set zReverseRangeByScore(String key, double min, double max, long start, long end) { return redisTemplate.opsForZSet().reverseRangeByScore(key, min, max, start, end); } /** * 根据score值获取集合元素数量 * * @param key * @param min * @param max * @return */ public Long zCount(String key, double min, double max) { return redisTemplate.opsForZSet().count(key, min, max); } /** * 获取集合大小 * * @param key * @return */ public Long zSize(String key) { return redisTemplate.opsForZSet().size(key); } /** * 获取集合大小 * * @param key * @return */ public Long zZCard(String key) { return redisTemplate.opsForZSet().zCard(key); } /** * 获取集合中value元素的score值 * * @param key * @param value * @return */ public Double zScore(String key, Object value) { return redisTemplate.opsForZSet().score(key, value); } /** * 移除指定索引位置的成员 * * @param key * @param start * @param end * @return */ public Long zRemoveRange(String key, long start, long end) { return redisTemplate.opsForZSet().removeRange(key, start, end); } /** * 根据指定的score值的范围来移除成员 * * @param key * @param min * @param max * @return */ public Long zRemoveRangeByScore(String key, double min, double max) { return redisTemplate.opsForZSet().removeRangeByScore(key, min, max); } /** * 获取key和otherKey的并集并存储在destKey中 * * @param key * @param otherKey * @param destKey * @return */ public Long zUnionAndStore(String key, String otherKey, String destKey) { return redisTemplate.opsForZSet().unionAndStore(key, otherKey, destKey); } /** * * @param key * @param otherKeys * @param destKey * @return */ public Long zUnionAndStore(String key, Collection otherKeys, String destKey) { return redisTemplate.opsForZSet() .unionAndStore(key, otherKeys, destKey); } /** * 交集 * * @param key * @param otherKey * @param destKey * @return */ public Long zIntersectAndStore(String key, String otherKey, String destKey) { return redisTemplate.opsForZSet().intersectAndStore(key, otherKey, destKey); } /** * 交集 * * @param key * @param otherKeys * @param destKey * @return */ public Long zIntersectAndStore(String key, Collection otherKeys, String destKey) { return redisTemplate.opsForZSet().intersectAndStore(key, otherKeys, destKey); } /** * * @param key * @param options * @return */ public Cursor> zScan(String key, ScanOptions options) { return redisTemplate.opsForZSet().scan(key, options); } // /** // * 获取Redis List 序列化 // * @param key // * @param targetClass // * @param // * @return // */ // public List getListCache(final String key, Class targetClass) { // byte[] result = redisTemplate.execute(new RedisCallback() { // @Override // public byte[] doInRedis(RedisConnection connection) throws DataAccessException { // return connection.get(key.getBytes()); // } // }); // if (result == null) { // return null; // } // return ProtoStuffSerializerUtil.deserializeList(result, targetClass); // } // // /*** // * 将List 放进缓存里面 // * @param key // * @param objList // * @param expireTime // * @param // * @return // */ // public boolean putListCacheWithExpireTime(String key, List objList, final long expireTime) { // final byte[] bkey = key.getBytes(); // final byte[] bvalue = ProtoStuffSerializerUtil.serializeList(objList); // boolean result = redisTemplate.execute(new RedisCallback() { // @Override // public Boolean doInRedis(RedisConnection connection) throws DataAccessException { // connection.setEx(bkey, expireTime, bvalue); // return true; // } // }); // return result; // } } "},"db/":{"url":"db/","title":"数据库","keywords":"","body":"数据库数据库 MySql CentosMySQL安装 存储引擎 字符集与排序规则 索引 索引常见的数据结构 B+TREE索引的优势 索引实现 联合索引 事务 锁 锁机制 大表优化 常用操作 MySQL配置文件 性能优化 Explain使用分析 一条SQL语句在MySQL中如何执行的 问题集锦 无法连接远端Mysql Oracle 序列 "},"db/mysql/":{"url":"db/mysql/","title":"MySql","keywords":"","body":"MySqlMySql "},"db/mysql/CentosMySQL安装.html":{"url":"db/mysql/CentosMySQL安装.html","title":"CentosMySQL安装","keywords":"","body":"CentosMySQL安装1. 具体步骤1.1 检查系统是否安装了MySQL1.2 下载安装包1.2.1 下载Mariadb1.2.2 下载mysql的repo 源（MySQL 5.7）1.3 安装MySQL1.4 登录1.5 重置密码1.6 数据库编码参考文章CentosMySQL安装 1. 具体步骤 1.1 检查系统是否安装了MySQL rpm -qa | grep mysql 返回空，说明没有安装 1.2 下载安装包 1.2.1 下载Mariadb Cenntos-7 默认下载的是MariaDB,所以执行下面的命令下载到的是MariaDB yum install mysql 删除 yum remove mysql 1.2.2 下载mysql的repo 源（MySQL 5.7） wget http://repo.mysql.com/mysql57-community-release-el7-9.noarch.rpm 安装mysql57-community-release-el7-9.noarch.rpm包 sudo rpm -ivh mysql57-community-release-el7-9.noarch.rpm 安装这个包后，在/etc/yum.repos.d/下 会获得两个mysql 的yum repo源 1.3 安装MySQL sudo yum install mysql-server 根据提示安装 1.3.1 检查安装是否成功 执行 rpm -qa |grep mysql 看到如下返回，证明安装成功了 1.4 登录 安装完成后，没有密码，需要重置密码 重置前需要登录 mysql -u root 1.4.1 登录异常一 ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) 执行如下命令 sudo chown -R openscanner:openscanner /var/lib/mysql 如果报 chown: invalid user: ‘openscanner:openscanner’ 更换命令 chown root /var/lib/mysql/， 并用 ll 查看目录权限列表 chown root /var/lib/mysql/ ll 重启服务 service mysqld restart 若还异常 chmod 777 /var/lib/mysql 1.4.2 登录异常2(默认密码) Access denied for user 'root'@'localhost' (using password: NO) 查找默认密码 grep \"temporary password\" /var/log/mysqld.log 返回结果最后冒号后面的字符串就是root的默认密码。 [root@iZwz97t3ru69kye3l7uj70Z lib]# grep \"temporary password\" /var/log/mysqld.log 2019-09-08T13:57:32.059634Z 1 [Note] A temporary password is generated for root@localhost: xUu1.5 重置密码 SET PASSWORD = PASSWORD('123456'); 重启之后才生效 exit; service mysqld restart 1.5.1 重置异常1 提示： ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 密码强度不足,执行 set global validate_password_policy=0; 1.6 数据库编码 查看数据库编码格式 show variables like \"%char%\"; 修改编码,在 vim /etc/my.cnf` 新增服务端默认字符集 [mysqld] # 服务端使用的字符集默认为UTF8 character-set-server=utf8 参考文章 CentOS7 64位安装mysql教程 "},"db/mysql/存储引擎.html":{"url":"db/mysql/存储引擎.html","title":"存储引擎","keywords":"","body":"存储引擎1. MyISAM 和InnoDB 区别1.1 适合场景1.2 两者对比1.3 MVCC2. 查看存储引擎2.1 查看MySQL提供的所有存储引擎2.2 查看MySQL 当前默认的存储引擎2.3 查看表的存储引擎存储引擎 1. MyISAM 和InnoDB 区别 MyISAM （MySQL 5.5 之前的默认数据库引擎） 性能极佳，并且提供了大量特性，包括全文索引、压缩、空间函数等 不支持事务和行级锁 最大缺陷崩溃后无法安全恢复 InnoDB(MySQL 5.5 之后的默认存储引擎)： 事务性数据库引擎 1.1 适合场景 InnoDB存储引擎 大多数场景，但是某些情况下使用 也是适合的如 MyISAM 存储引擎 读密集的情况下（不介意MyISAM 崩溃恢复问题） 1.2 两者对比 MyISAM InnoDB 是否支持行级锁 只有表级锁（table-level locking） 支持行级锁（row-level locking）和表级锁，默认行级锁 是否支持事务 不支持 支持 崩溃后的安全恢复 不支持 支持 支持外键 不支持 支持 支持MVCC 不支持 支持 1.3 MVCC 在应对高并发事务，MVCC 比单纯的加锁更高效， MVCC 只在READ COMMITED 和 REPEATABLE READ 两个隔离级别下工作。 MVCC 可以使用乐观（optimistic）锁和 悲观（pessimistic）锁来实现 2. 查看存储引擎 2.1 查看MySQL提供的所有存储引擎 mysql> show engines; 从上图我们可以出Mysql 当前的默认存储引擎是InnoDB,也提示了innoDB 支持事务，行级锁等特性 2.2 查看MySQL 当前默认的存储引擎 show variables like '%storage_engine%'; 2.3 查看表的存储引擎 show table status like \"t_user\"; "},"db/mysql/字符集与排序规则.html":{"url":"db/mysql/字符集与排序规则.html","title":"字符集与排序规则","keywords":"","body":"字符集与排序规则1. 是什么1.1 排序规则的命名规则2. 指定字符集3. 查询字符集和排序规则3.1 查询各级的字符集3.2 查询对应的排序规则4. 修改字符集和排序规则4.1 未创建数据库4.2 已创建数据库无数据4.3 已创建且有数据的数据库字符集与排序规则 1. 是什么 字符集character set）：用来定义存储字符串的方式 定义了字符以及字符编码 字符集分为几个等级： server, database, table, 和 column 。 排序规则（collations）：用来定义比较字符串的方式 定义了字符的比较规则 MySQL采用类似继承的方式制定字符集默认值，每个数据库每张表都有自己的默认值，他们逐层继承。如：某个库中所有表的默认字符集将是该数据库所指定的字符集（这些表在没有指定字符集的情况下，才会采用默认字符集） 1.1 排序规则的命名规则 字符集名_[语言名_]类型 （语言名并非一定有的，后缀为 _bin 的就没有），并且可通过后缀来区分类型： _ci ：大小写不敏感 _cs ：大小写敏感 _bin ：标识比较是基于字符编码的值，而与语言无关 2. 指定字符集 我们在命令行创建一个新的数据库时，可以通过如下命令 CREATE DATABASE 数据库名; 此时会使用默认的字符集及默认排序规则来创建数据库，而这个默认值可以在Mysql安装的根目录下的my.ini （或者 my-defualt.ini ）中进行配置，例如都设为 utf8： [mysqld] # 服务端使用的字符集默认为UTF8 character-set-server=utf8 [mysql] # 设置mysql客户端默认字符集(可能会有问题，只需设置上面的) default-character-set=utf8 [client] default-character-set=utf8 如果要在创建数据库时指定字符集和排序规则 CREATE DATABASE 数据库名 CHARACTER SET '字符集，如：utf8' COLLATE '排序规则，如：utf8_bin'; 3. 查询字符集和排序规则 对于已创建的数据库结构，可以通过指令来查询其使用的字符集信息。 3.1 查询各级的字符集 mysql> SHOW VARIABLES LIKE '%char%'; +--------------------------+----------------------------------+ | Variable_name | Value | +--------------------------+----------------------------------+ | character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | | character_sets_dir | /usr/local/mysql/share/charsets/ | +--------------------------+----------------------------------+ 8 rows in set (0.00 sec) 3.2 查询对应的排序规则 mysql> SHOW VARIABLES LIKE '%collation_%'; +----------------------+-----------------+ | Variable_name | Value | +----------------------+-----------------+ | collation_connection | utf8_general_ci | | collation_database | utf8_general_ci | | collation_server | utf8_general_ci | +----------------------+-----------------+ 3 rows in set (0.00 sec) 4. 修改字符集和排序规则 4.1 未创建数据库 可以通过在创建命令中指定字符集的方式实现修改，也可以通过修改MySQL 安装根目录下的 my.ini （或者 my-defualt.ini ）中的配置实现修改。 4.2 已创建数据库无数据 可以使用如下指令进行修改： ALTER DATABASE 数据库名 CHARACTER SET '字符集，如：utf8' COLLATE '排序规则，如：utf8_bin'; 4.3 已创建且有数据的数据库 直接修改的话只会对新创建的表或者记录有效，已存入的数据不会被修改。假如需要修改所有数据，需要将原表导出，创建新表再将旧表数据迁移过来。 "},"db/mysql/index/":{"url":"db/mysql/index/","title":"索引","keywords":"","body":"索引1. 什么是索引2. 索引常见的数据结构索引 1. 什么是索引 索引：加速查询的数据结构 2. 索引常见的数据结构 "},"db/mysql/index/索引常见的数据结构.html":{"url":"db/mysql/index/索引常见的数据结构.html","title":"索引常见的数据结构","keywords":"","body":"索引常见的数据结构1. 顺序查找2. 二叉树查找(binary tree search)3. hash索引4. 二叉树、红黑树5. B-Tree5.1 检索原理：5.2 B-Tree缺点6. B+Tree6.1 与B-Tree的不同点6.2 B+Tree数据结构6.3 MySQL 中的B+Tree参考文档索引常见的数据结构 1. 顺序查找 算法复杂度：O(n) 最基本的查询算法，复杂度O(n),大数据量时此算法效率非常糟糕 2. 二叉树查找(binary tree search) 算法复杂度：O(log2n) 左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址。（逻辑相邻的记录在磁盘上也并不一定是物理相邻）。 为了加快col2 的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在O(log2n)的复杂度内获取响应的数据 3. hash索引 无法满足范围查找 4. 二叉树、红黑树 算法复杂度：O(h) 这将导致树的高度非常高。（平衡二叉树一个节点只能有左子树和右子树），逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，IO次数多查找慢，效率低（逻辑上相邻节点没法直接通过顺序指针关联，可能需要迭代回上层节点重复向下遍历查找对应的节点，效率低） 5. B-Tree 结构：B-Tree 每个节点都是一个二元数组，所有的节点都可以存储数据，key为索引key，data为除key 指代的数据。结构如下 5.1 检索原理： 首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或未找到节点返回null指针。 5.2 B-Tree缺点 插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质。造成IO操作频繁 区间查找可能需要返回上层节点重复遍历，IO操作繁琐。 6. B+Tree B+Tree 是B-Tree的变种 6.1 与B-Tree的不同点 非叶子节点不存储data，只存储索引key； 只有叶子节点才存储data 6.2 B+Tree数据结构 6.3 MySQL 中的B+Tree 6.3.1 概述 在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 6.3.2 详细 在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。这样就提高了区间访问性能： 6.3.3 案例： 如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率(无需返回上层父节点重复遍历查找减少IO操作)。 6.3.4 数据结构 参考文档 MYSQL-B+TREE索引原理 "},"db/mysql/index/B+TREE索引的优势.html":{"url":"db/mysql/index/B+TREE索引的优势.html","title":"B+TREE索引的优势","keywords":"","body":"B+TREE索引的优势1. 影响索引查询效率的主要原因2. 为什么磁盘存储慢2.1 磁盘存取原理3. 磁盘局部性原理与磁盘预读4.B-/B+Tree索引的优势B+TREE索引的优势 1. 影响索引查询效率的主要原因 索引存储在磁盘上 索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上 磁盘I/O存取慢 索引在查找过程汇总要产生磁盘I/O消耗，相对于内存存储，I/O存储的消耗要高几个数量级 所以索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，提升索引效率 2. 为什么磁盘存储慢 2.1 磁盘存取原理 索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O 操作，与主存不同，磁盘I/O存在机械运动耗费。因此磁盘I/O的时间消耗时巨大的 2.1.1 磁盘的组成 一个磁盘由大小相同且同轴的圆形盘片组成 磁盘可以转动（各个磁盘必须同步转动）。 在磁盘的一则有磁头支架 磁头支架固定了一组磁头 每个磁头负责存储存取一个磁盘的内容 磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动） 每个磁头同一时刻必须是同轴的 2.1.2 磁盘组成和工作原理 磁道 每个同心环叫做一个 扇区 磁盘的最小存取单元 2.1.2.1 确定数据位置 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址。既确定要读的数据在哪个磁道，哪个扇区 2.1.2.2 磁头寻道 为了读取这个扇区的数据，需要将磁头放在这个扇区上方，为了实现这一点，磁头需要移动对准响应的磁道，这个过程叫做寻道，所耗费的时间叫寻道时间， 2.1.2.3 磁盘旋转到对应扇区 然后磁盘旋转将目标扇区旋转到磁头下,这个过程耗费的时间叫做旋转时间 3. 磁盘局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存储就比主存慢很多，在加上机械运动耗费，磁盘的存取速度往往是主存的几百分之一，因此为了提高效率，要尽量减少磁盘I/O，为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读可以提高I/O效率。预读的长度一般为页（page:计算机管理存储器的逻辑块-通常为4k）的整数倍。主存和磁盘以页为单位交换数据。当程序要去读的数据不再主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中 4.B-/B+Tree索引的优势 一般使用磁盘的I/O 次数评价索引优势 那么BTree如何减少磁盘次数的呢 结合操作系统存储结构优化处理：MySQL巧妙运用操作系统存储结构（一个节点分配到一个存储页中->尽量减少I/O操作）&磁盘预读（缓存预读->加速预读马上要用到的数据） 详解: Mysql设计利用了磁盘预读原理，将一个b+tree节点大小设为一个页大小，在新建节点时直接申请一个页的控件，这样就能保证一个节点物理上存储在一个页里，加之计算机存储分配都是按页对其，这样就实现了每个Node节点只需要一次IO操作 B+Tree单个节点能放多个子节点，相同IO次数，检索出更多东西 这也是B+Tree相比B-Tree能查询出更多数据的原因 详解 单个节点能放多个子节点，查询IO次数相同(mysql查询IO次数最多3-5次-所以需要每个节点需要存储很多数据) B+Tree只在叶子节点存储数据&所有叶子节点包含一个链指针&其他内层非叶子节点只存储索引数据，只利用索引快速定位数据索引范围，先定位索引再通过索引高效快速定位数据 B+Tree更适合外存索引，原因和内节点出度d有关。从上面分析可以看到，d越大索引的性能越好，而出度的上限取决于节点内key和data的大小： B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。只利用索引快速定位数据索引范围，先定位索引再通过索引高效快速定位数据。 dmax=floor(pagesize/(keysize+datasize+pointsize)) "},"db/mysql/index/索引实现.html":{"url":"db/mysql/index/索引实现.html","title":"索引实现","keywords":"","body":"索引实现1. 聚簇索引2. MyISAM 索引原理2.1 底层存储结构2.2 myISAM 索引结构特性2.3 MyISAM 索引查找流程3. InnoDB3.1 InnoDB优势3.2 InnoDB特性3.3 组件功能3.4 InnoDB物理存储结构4. InnoDB 索引原理4.1 InnoDB 特点4.2 底层存储结构4.3 为什么InnoDB 一定要有主键4.4 辅助索引需要检索两遍4.5 聚簇索引结构4.6 索引的查找流程索引实现 1. 聚簇索引 MyISAM 和 InnoDB 都使用B+Tree索引结构，但底层索引存储不同，MyISAM 采用非聚簇索引，而InnoDB采用聚簇索引 聚簇索引：索引和数据文件为同一个文件 非聚簇索引：索引和数据文件 分开的索引 2. MyISAM 索引原理 2.1 底层存储结构 frm：定义表 myi: myisam索引 myd：myisam数据 2.2 myISAM 索引结构特性 采用非聚簇索引 MyISAM myi索引文件和myd 数据文件分离 索引文件仅保存数据记录的指针地址。 叶子节点data域存储指向数据记录的指针地址 2.3 MyISAM 索引查找流程 MyISAM 索引按照B+Tree搜索， 如果指定的key存在，则取出其data域值 然后以data阈值-数据地址去读取响应的数据记录， 辅助索引和主索引在结构上没有任何区别，只是主索引要求key 是唯一的，而辅助索引的key 可以重复 3. InnoDB 3.1 InnoDB优势 高扩展性 充分开发硬件性能 Crash Safe 支持事务 可以在线热备份 3.2 InnoDB特性 事务支持（ACID） 扩展性优良 读写不冲突 缓存加速 3.3 组件功能 redo/undo 异步IO MVCC 行级别锁 Page Cache（LRU） 3.4 InnoDB物理存储结构 表空间(ibd文件) InnoDB 以表空间Tablespace（idb文件）结构进行组织 段(Segment) 每个Tablespce 包含多个Segment段 分为2种段：叶子节点Segment 和非叶子节点Segment Extent 一个Segment段包含多个Extent 一个Extent占用占用1M空间包含64个page（每个Page 16K） Page(16K) InnoDB B-Tree 一个逻辑节点扣分配一个物理Page，一个节点一次IO操作 Row 一个Page里包含很多有序数据Row行数据 Field Row行数据中包含Field 属性数据等信息 3.5 表插入数据扩展原理 一次扩张一个Extent空间（IM）,64个page，按顺序结构向每个page中插入数据 3.6 InnoDB 逻辑组织结构 每个索引一个B+Tree，一个B+Tree节点 = 一个物理Page（16K） 数据按16KB切片为Page 并编号，编号可映射到物流文件偏移（16KN）,B+Tree树叶子节点前后形成双向链表，数据按主键聚簇，二级索引叶节点存储主键值，通过叶节点主键值*回表查找数据 4. InnoDB 索引原理 4.1 InnoDB 特点 采用聚簇索引 InnoDB 数据&索引文件为idb文件， 表数据文件本身就是就是主索引 相邻的索引临近存储。 叶节点data域保存了完整的数据记录（数据[除了主键id外其他data]+主索引） 叶子节点直接存储数据记录，以主键id为key，叶子节点直接存储数据记录 4.2 底层存储结构 frm: 表定义 idb: innoDB数据&索引文件 4.3 为什么InnoDB 一定要有主键 由于InnoDB 采用聚簇索引结构存储，索引InnoDB 的数据文件需要按照主键聚集。因此InnoDB 要求表必须有主键（MyISAM可以没有）。 如果没有指定mysql会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这样的列，mysql自动为innoDB表生成一个隐含字段（6个字节长整型）作为主键。innoDB所有辅助索引都引用数据记录的主键 作为data 域 4.4 辅助索引需要检索两遍 聚簇索引这种实现方式使得主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引 首先检索辅助索引获得数据记录主键 然后用主键到主索引中检索获得数据记录 4.5 聚簇索引结构 4.6 索引的查找流程 2.6.1 索引精确查找： select * from user_info where id = 23 确定定位条件，找到根节点的PageNo 根节点读到内存 逐层向下查找 读取叶子节点的Page 通过二分查找找到记录或未命中。 2.6.2 索引范围查找 select * from user_info where id >= 18 and id 读取根节点至内存 确定索引定位条件 id=18 找到满足条件第一个叶子节点， 顺序扫描所有结果，直到终止条件满足id>=22 2.6.3 全表扫描 select * from user_info where name = 'abc' 直接读取叶子节点头结点 顺序扫描 返回符合条件记录，到最终节点结束 2.6.4 二级索引查找 Create table table_x(int id primary key, varchar(64) name,key sec_index(name) ) • Select * from table_x where name = “d”; 通过二级索引查出主键 拿主键回表查主键索引得到数据 二级索引可筛选掉大量无效记录，提高效率 "},"db/mysql/index/联合索引.html":{"url":"db/mysql/index/联合索引.html","title":"联合索引","keywords":"","body":"联合索引1. 什么是联合索引2. 需要遵循的规则3. 原理3.1 B+Tree结构3.2 实例4. 多列索引的应用4.1 多列索引在and查询中的应用4.2 多列索引在范围查询中应用4.3 多列索引在排序中应用5. 总结联合索引 1. 什么是联合索引 两个或更多个列上的索引被称为联合索引，联合索引又叫复合索引。 对于联合索引： MySql从左到右使用索引中字段 一个查询可以只使用索引中的一部分，但只能是最左部分（最左前缀） 例如： 索引是key index（a,b,c） 可以支持a|a,b|a,b,c，三种组合查找，但不支持b,c 进行查找。当最左侧字段是常量引用时，索引就十分有效 2. 需要遵循的规则 需要加索引的字段，要在where条件中 数据量少的字段不需要加索引 如何where条件中是or关系，加索引不起作用 符合最左前缀原则 3. 原理 3.1 B+Tree结构 每一个磁盘快在mysql中是一个页，页大小是固定的，mysql innodb的默认的页大小是16k。每个索引会分配在页上的数量是由字段的大小决定。当字段值长度越长，每一页上的数量就会越少，因此在一定数据量的情况下，所以的深度会越深，影响索引查找效率 对于复合索引（多列b+tree,使用多列值组合而成的b+tree索引）。遵循最左前缀原则，从左到右的使用索引中的字段，一个查询可以只使用索引中的一部分，但只能是做左侧部分 3.2 实例 创建表test create table test( a int, b int, c int, KEY a(a,b,c) ); 比如（a,b,c）的时候，b+tree是按照从左到右的顺序来建立搜索树的 当（a =? and b= ? and c=?) 这样的数据来检索的时候 b+树会优先比较a列来确定下一步的方向 如果a列相同再依次比较b列和c列 最后得到检索数据 但当（b=? and c =?）这样没有a列的数据来的时候 b+树就不知道下一步该查那个节点，因为建立搜索树的时候a列就是第一个比较因子。必须要先跟a列来搜索才能知道下一步去哪里查询 当（a=? and c =?）这样的数据来检索时 b+树可以用a列来制定搜索方向，但下一个字段b列的缺失，只能吧a列的数据找到 然后在匹配c列的数据 4. 多列索引的应用 4.1 多列索引在and查询中的应用 select * from test where a=? and b=? and c=?； 查询效率最高，索引全覆盖。 select * from test where a=? and b=? 索引覆盖a和b。 select * from test where b=? and a=? 经过mysql的查询分析器的优化，索引覆盖a和b。 select * from test where a=?； 索引覆盖a。 select * from test where b=? and c=? 没有a列，不走索引，索引失效。 select * from test where c=?； 没有a列，不走索引，索引失效。 4.2 多列索引在范围查询中应用 select * from test where a=? and b between ? and ? and c=?； 索引覆盖a和b，因b列是范围查询，因此c列不能走索引。 select * from test where a between ? and ? and b=?； a列走索引，因a列是范围查询，因此b列是无法使用索引。 select * from test where a between ? and ? and b between ? and ? and c=?； a列走索引，因a列是范围查询，b列是范围查询也不能使用索引。 4.3 多列索引在排序中应用 select * from test where a=? and b=? order by c； a、b、c三列全覆盖索引，查询效率最高。 select * from test where a=? and b between ? and ? order by c； a、b列使用索引查找，因b列是范围查询，因此c列不能使用索引，会出现file sort。 5. 总结 联合索引的使用在写where调的顺序无关，mysql 查询分析会进行优化而使用索引，但是为了减轻查询分析器的压力，最好和索引的从左到右的顺序一致 使用等值查询，多列同时查询，索引会一直传递并生效。因此等值查询效率最好。 索引查找遵循最左侧原则。但是遇到范围查询列之后的列索引失效。 排序也能使用索引，合理使用索引排序，避免出现file sort。 "},"db/mysql/transaction/":{"url":"db/mysql/transaction/","title":"事务","keywords":"","body":"事务1. 什么是事务1.1 案例2. 事务的四大特性（ACID）2.1 数据库是如何保证ACID的3. 并发事务带来哪些问题3.1 不可重复读和幻读区别4. 事务隔离级别有哪些5. MySQL innoDB 的隔离级别5.1 InnoDB 的 REPEATABLE-READ为什么可以避免幻读事务 1. 什么是事务 事务就是逻辑上的一组操作，要么都执行，要么都不执行 1.1 案例 事务最经典例子转账：假设小明要给小红转账1000元，这个转账会涉及到两个关键操作 将小明的余额减少1000元 将小红的余额增加1000元 万一在这两个操作之间突然出现错误比如银行系统奔溃，导致小明余额减少而小红的余额没有增加，这就不对了。 事务就是保证这两个关键操作要么都成功，要么都失败 2. 事务的四大特性（ACID） 原子性（Atomicity）: 事务是最小的执行单位，不予许分割，事务的原子性保证动作么全部完成，要么完全不起作用 一致性（Consistency）: 执行事务前后，数据保持一致，多个事务对同一个数据读取结果是相同的 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的 持久性（Durability） 一个事务被提交之后，它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响 2.1 数据库是如何保证ACID的 四个特性，最重要的就是一致性。而一致性由原子性、隔离性、持久性来保证 原子性由Undo log保证： Undo Log 会保存每次变更之前的记录，从而在发生错误时进行回滚 隔离性由MVVC和Lock保证 持久性有Redo Log保证 每次真正修改数据之前，都会将记录写到Redo Log中，只有redo log 写入成功，才会真正写入到B+tree中。如果提交之前断电，就可以通过Redo log恢复记录 3. 并发事务带来哪些问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。 并发虽然是必须的，但是可能会导致以下问题 脏读（Dirty read） 当一个事务正在访问数据并且对数据进行了修改，这种修改还没有提交到数据库中 这时另外一个事务也访问了这个数据，然后使用了这个数据 因为这个数据是还没有提交的数据，那么另外一个事务读到这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的 丢失修改（Lost to modify） 在一个事务读取一个数据时，另外一个事务也访问了该数据 那么在第一个事务中修改了数据数据后，第二个事务也修改了这个数据 这样第一个事务内的修改就被丢失，因此称为丢失修改 例如：事务1 读取某表中的数据A=20，事务2也读取了A=20，事务1修改A=A-1,事务2也修改了A-1，最终结果A=19，事务1的修改被丢失 不可重复读（Unrepeatableread） 指在一个事务内多次读同一数据，在这个事务还没有结束时，另一个事务也访问该数据 那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样 这样就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重读读 幻读（Phantom read） 幻读与不可重复读类似。 他发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时 在随后的查询中，第一个事务（T1）就会发现多了一些根本不存在的记录 就好像发生了幻觉一样，所以称为幻读 3.1 不可重复读和幻读区别 不可重复读的重点是修改 比如：多次读取一条记录，发现其中某些列的值被修改 幻读的重点在与新增或者删除 比如：多次读取一条记录，发现记录增多或减少了 4. 事务隔离级别有哪些 SQL 标准定义了四个隔离级别 READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读，幻读，或不可重复读 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或者不可重复读仍然有可能发生 REPEATABLE-READ(可重复读)： 对同一个字段的多次读取结果结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间不可能产生干扰，也就是说，该级别可以防止脏读，不可重复读以及幻读 ​ 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × 5. MySQL innoDB 的隔离级别 MySQL innoDB 存储引擎的默认支持的隔离级别是REPEATABLE-READ（可重复读） 可以通过SELECT @@tx_isolation;命令来查看 mysql> SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 1 row in set, 1 warning (0.00 sec) 5.1 InnoDB 的 REPEATABLE-READ为什么可以避免幻读 Next-key Lock锁算法。因此可以避免幻读的产生 与SQL 标准不同的地方在于InnoDB 存储在 REPEATABLE-READ（可重读）事务隔离界别下使用的是Next-key Lock锁算法。因此可以避免幻读的产生 这与其他数据库系统（如：SQL Server）是不同的，所以说InnoDB 存在引擎的默认支持的隔离级别是REPEATABLE-READ（可重读），已经可以完全保证事务的隔离性要求，既达到了SQL 标准的SERIALIZABLE(可串行化) 隔离级别 因为隔离级别越低，事务请求的锁越少，但是大部分数据库的隔离级别都是READ-COMMITTED(读取提交内容) ，但是你要知道的是InnoDB 存储引擎默认使用 REPEAaTABLE-READ（可重读） 并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。 "},"db/mysql/lock/":{"url":"db/mysql/lock/","title":"锁","keywords":"","body":"锁机制锁1. MyISAM 和InnoDB 存储引擎使用的锁2.表级锁和行级锁对比3. InnoDB存储引擎的锁算法3.1 相关知识点锁机制锁 1. MyISAM 和InnoDB 存储引擎使用的锁 MyISAM 采用的是表级锁（table-level locking） InnoDB 支持行级锁（row-level locking）和表级锁，默认行为是行级锁 2.表级锁和行级锁对比 表级锁 MySQL 中锁定 粒度最大的一种锁，对当前操作的整张表加锁 优势 实现简单，资源消耗少，加锁快 不会出现死锁 缺点 其锁粒度最大，触发锁冲突的概率最高 并发度最低 行级锁 MySQL 中锁定 粒度最小的一种锁，只针对当前操作的行加锁 优势 大大减少数据库操作的冲突 加锁粒度小，并发度高 缺点 加锁的开销大 加锁慢 会出现死锁 3. InnoDB存储引擎的锁算法 Record lock：单个行记录上的锁 Gap lock: 间隙锁，锁定一个范围，不包括记录本身 Next-key lock: record+gap 锁定一个锁范围，包含记录本身 3.1 相关知识点 innodb 对于行的查询使用next-key lock next-locking keying 为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为 record key Gap 锁设计的目的是为了阻止多个事务将记录插入到同一范围内，这会导致幻读问题的产生 有两种方式显示关闭gap 锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） 将事务隔离级别设置为READ-COMMITTED 将参数innodb_locks_unsafe_for_binlog 设置为1 "},"db/mysql/lock/锁机制.html":{"url":"db/mysql/lock/锁机制.html","title":"锁机制","keywords":"","body":"锁机制1. 锁维度1.1 类型维度1.2 锁的粒度（粒度维度）1.3 锁的算法（算法维度）2. 默认的读操作，上锁吗？3 MySQL 的 SERIALIZABLE 有啥用呢？锁机制 1. 锁维度 锁有好几种维度 1.1 类型维度 共享锁（读锁/S锁） 排他锁（写锁/X 锁） 类型细分 意向共享锁 意向排他（互斥）锁 悲观锁（使用锁，既for update） 乐观锁（使用版本号字段，类似 CAS 机制，既用户自己控制。缺点：并发很高的时候，多了很多无用的重试） 1.2 锁的粒度（粒度维度） 表锁 页锁（Mysql BerkeleyDB 引擎） 行锁（InnoDB） 行锁的实现原理就是锁住聚集索引，如果你查询的时候，没有正确地击中索引，MySql 优化器将会抛弃行锁，使用表锁。 1.3 锁的算法（算法维度） Record Lock（单行记录） Gap Lock（间隙锁，锁定一个范围，但不包含锁定记录） Next-key Lock（Record Lock+Gap Lock，锁定一个范围，并且锁定记录本身。MySQL 防止幻读，就是使用此锁实现） 2. 默认的读操作，上锁吗？ 默认是MVCC机制（“一致性非锁定读”）保证RR 级别的隔离正确性。是不上锁的 可以选择手动上锁 排他锁：select xxxx for update 共享锁：select xxx lock in share mode 称为一致性锁定读 使用锁之后，就能在RR 级别下，避免幻读。当然默认的MVCC读，也能避免幻读 3 MySQL 的 SERIALIZABLE 有啥用呢？ MySQL RR能够防止幻读,那么，SERIALIZABLE 有啥用呢？ 他可以防止丢失更新 这个时候，我们必须使用SERIALIZABLE 级别进行串行读取。 "},"db/mysql/bigtable/":{"url":"db/mysql/bigtable/","title":"大表优化","keywords":"","body":"大表优化1. 限制数据的范围2. 读/写分离3. 垂直分区3.1 垂直拆分的优缺点4. 水平分区4.1 水平拆分总结4.2 数据库分片两种方案大表优化 当MySQL单表记录数据过大，数据库的CRUD性能会明显下降，一些常见的优化措施 1. 限制数据的范围 务必禁止不带任何限制数据范围的查询语句 例如：我们当用户在查询订单历史的时候，可以控制在一个范围内； 2. 读/写分离 经典的数据库拆分方案，主库负责写，从库负责读 3. 垂直分区 根据数据库里面的数据表的相关性进行拆分 例如：用户表中既有用户登录信息又有用户基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表 如下图 3.1 垂直拆分的优缺点 优点 可以使得列数据变小，在查询时减少读取的block数，减少I/O次数 简化表结构，易于维护 缺点 主键会出现冗余，需要管理冗余列 并会引起join操作，可以通过在应用层进行Join来解决 会让事务变得更加复杂 4. 水平分区 保持数据表结构不变，通过某种策略存储数据分片，这样每一片数据分散到不同的表或者库中，达到了分布式的目的。水平拆分可以支持非常大的数据量 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。 举个例子，我们可以将用户信息表拆分成多个用户信息表，这样可以避免单一表数据量过大对性能造成的影响 水平拆分可以支持非常大的数据量 需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升Mysql并发能力没有意义。所以水平拆分最好分库 4.1 水平拆分总结 水平拆分能够支持非常大的数据量存储，应用端改造也少，但分片事务难以解决，跨节点join性能较差，逻辑复杂。 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维等各种复杂度，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O 4.2 数据库分片两种方案 客户端代理 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现 当当网的Sharding-JDBC 阿里的TDDL是两种比较常用的实现 中间件代理 在应用和数据中间加了一层代理层，分片逻辑统一维护在中间件服务中。 Mycat 360的Atlas 网易的DDB "},"db/mysql/常用操作.html":{"url":"db/mysql/常用操作.html","title":"常用操作","keywords":"","body":"常用操作常用操作 启动mysql /etc/init.d/mysqld start 连接mysql mysql –u用户名 [–h主机名或者IP地址] –p密码 mysql -uroot -p //-p回车输入密码 mysql -uroot -pmypwd //root是用户名，mypwd 是密码 –u、-h、-p后无空格。 "},"db/mysql/MySQL配置文件.html":{"url":"db/mysql/MySQL配置文件.html","title":"MySQL配置文件","keywords":"","body":"MySQL配置文件1. 配置文件路径2. 配置文件MySQL配置文件 1. 配置文件路径 linux： 一般放在 /etc/my.cnf，/etc/mysql/my.cnf win： 一般会在安装目录的根目录 my.ini 2. 配置文件 # For advice on how to change settings please see # http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html # *** DO NOT EDIT THIS FILE. It's a template which will be copied to the # *** default location during install, and will be replaced if you # *** upgrade to a newer version of MySQL. [mysqld] #数据库表不区分大小写 lower_case_table_names=1 skip-name-resolve skip-grant-tables # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # Remove leading # to turn on a very important data integrity option: logging # changes to the binary log between backups. # log_bin # These are commonly set, remove the # and set as required. basedir = /usr/local/mysql datadir = /usr/local/mysql/data port = 3306 # server_id = ..... socket = /tmp/mysql.sock character-set-server = utf8 [mysql] auto-rehash "},"db/mysql/optimize/Explain使用分析.html":{"url":"db/mysql/optimize/Explain使用分析.html","title":"Explain使用分析","keywords":"","body":"1. 简介2. 准备3. EXPLAIN 输出格式3.1 select_type3.2 table3.3 type （重要）3.4 possible_keys3.5 key(重要)3.6 key_len3.7 rows（重要）3.8 Extra4. 总结一下参考文章Explain使用分析 1. 简介 MySQL 提供了一个EXPLAIN 命令，他可以对 SELECT语句进行分析，并输出SELECT 执行的详细信息，以供开发人员针对性优化 EXPLAIN 命令语法十分简单，在SELECT 语句前加上EXPLAIN EXPLAIN SELECT * FROM user_info WHERE id 2. 准备 为了方便演示EXPLAIN 的使用，首先我们需要建立两个测试用的表，并添加相应的数据 CREATE TABLE `user_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(50) NOT NULL DEFAULT '', `age` INT(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; INSERT INTO user_info (name, age) VALUES ('xys', 20); INSERT INTO user_info (name, age) VALUES ('a', 21); INSERT INTO user_info (name, age) VALUES ('b', 23); INSERT INTO user_info (name, age) VALUES ('c', 50); INSERT INTO user_info (name, age) VALUES ('d', 15); INSERT INTO user_info (name, age) VALUES ('e', 20); INSERT INTO user_info (name, age) VALUES ('f', 21); INSERT INTO user_info (name, age) VALUES ('g', 23); INSERT INTO user_info (name, age) VALUES ('h', 50); INSERT INTO user_info (name, age) VALUES ('i', 15); CREATE TABLE `order_info` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `user_id` BIGINT(20) DEFAULT NULL, `product_name` VARCHAR(50) NOT NULL DEFAULT '', `productor` VARCHAR(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p1', 'WHH'); INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p2', 'WL'); INSERT INTO order_info (user_id, product_name, productor) VALUES (1, 'p1', 'DX'); INSERT INTO order_info (user_id, product_name, productor) VALUES (2, 'p1', 'WHH'); INSERT INTO order_info (user_id, product_name, productor) VALUES (2, 'p5', 'WL'); INSERT INTO order_info (user_id, product_name, productor) VALUES (3, 'p3', 'MA'); INSERT INTO order_info (user_id, product_name, productor) VALUES (4, 'p1', 'WHH'); INSERT INTO order_info (user_id, product_name, productor) VALUES (6, 'p1', 'WHH'); INSERT INTO order_info (user_id, product_name, productor) VALUES (9, 'p8', 'TE'); 3. EXPLAIN 输出格式 EXPLAIN 命令的输出内容大致内容 mysql> explain select * from user_info where id = 2\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: const possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) 各列的含义 id: SELECT 查询的标识符，每个SELECT 都会自动分配一个唯一的标识符 select_type: SELECT 查询的类型 table: 查询的是哪个表 partitions: 匹配的分区 type： join类型 possible_keys: 此次查询可能选择的索引 key: 此次查询中确切使用到的索引 ref：哪个字段或常数与key字段一起被使用 rows：显示此查询一共扫描了多少航，这个是一个估计值 filtered: 表示此查询条件所过滤的数据的百分比 extra： 额外的信息 3.1 select_type select_type 表示查询的类型，他常用的取值有 SIMPLE: 表示此查询不包含UNION 查询或子查询 PRIMARY: 表示此查询是最外层的查询 UNION: 表示此查询是UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句，取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 3.1.1 SIMPLE 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型。例如： mysql> explain select * from user_info where id = 2\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: const possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) 3.1.2 UNION 如果我们使用了UNION 查询，那么EXPLAIN 输出的结果类似如下 EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) -> UNION -> (SELECT * FROM user_info WHERE id IN (3, 4, 5)); 3.2 table 表示查询设计的表或者衍生表 3.3 type （重要） type 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. 3.3.1 system system: 表中只有一条数据. 这个类型是特殊的 const 类型. 3.3.2 const const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可. 例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. mysql> explain select * from user_info where id = 2\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: const possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) 3.3.3 eq_ref eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: mysql> EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: index possible_keys: user_product_detail_index key: user_product_detail_index key_len: 254 ref: NULL rows: 8 filtered: 100.00 Extra: Using where; Using index *************************** 2. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: eq_ref possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: test.order_info.user_id rows: 1 filtered: 100.00 Extra: NULL 2 rows in set, 1 warning (0.00 sec) 3.3.4 ref ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询. 例如下面这个例子中, 就使用到了 ref 类型的查询: mysql> EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: const possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL *************************** 2. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: ref possible_keys: user_product_detail_index key: user_product_detail_index key_len: 9 ref: const rows: 1 filtered: 100.00 Extra: Using index 2 rows in set, 1 warning (0.00 sec) 3.3.5 range range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, <>, >, >=, , BETWEEN, IN() 操作中. 当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. mysql> EXPLAIN SELECT * -> FROM user_info -> WHERE id BETWEEN 2 AND 8 \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: range possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: NULL rows: 7 filtered: 100.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) 3.3.5 index index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据. index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. mysql> EXPLAIN SELECT name FROM user_info \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: index possible_keys: NULL key: name_index key_len: 152 ref: NULL rows: 10 filtered: 100.00 Extra: Using index 1 row in set, 1 warning (0.00 sec) 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. 3.3.7 ALL ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. 下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. mysql> EXPLAIN SELECT age FROM user_info WHERE age = 20 \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10 filtered: 10.00 Extra: Using where 1 row in set, 1 warning (0.00 sec) type类型性能比较 通常来说, 不同的 type 类型的性能关系如下: ALL ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的. 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快. 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. 3.4 possible_keys possible_keys 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. 3.5 key(重要) 此字段是 MySQL 在当前查询时所真正使用到的索引. 3.6 key_len 表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到. key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的栗子: mysql> EXPLAIN SELECT * FROM order_info WHERE user_id 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 不过此查询语句 WHERE user_id 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT '0', 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: mysql> EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = 'p1' \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: ref possible_keys: user_product_detail_index key: user_product_detail_index key_len: 161 ref: const,const rows: 1 filtered: 100.00 Extra: Using index 1 row in set, 1 warning (0.00 sec) 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = 'p1' 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 3.7 rows（重要） rows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数. 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. 3.8 Extra EXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: mysql> EXPLAIN SELECT * FROM order_info ORDER BY product_name \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: index possible_keys: NULL key: user_product_detail_index key_len: 254 ref: NULL rows: 8 filtered: 100.00 Extra: Using index; Using filesort 1 row in set, 1 warning (0.00 sec) 我们的索引是 KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort. 如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: mysql> EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: order_info partitions: NULL type: index possible_keys: NULL key: user_product_detail_index key_len: 254 ref: NULL rows: 8 filtered: 100.00 Extra: Using index 1 row in set, 1 warning (0.00 sec) Using index \"覆盖索引扫描\", 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary 查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. 4. 总结一下 我们可以通过Explain 语句来判断sql是否高效，是否用到了索引。 key：通过key的值为 PRIMARY 则使用到了索引 rows：rows扫描的行数，越小越好 type：通过type的值判断查询是否高效，判断此次是全表扫描还是索引扫描 ALL 参考文章 MySQL 性能优化神器 Explain 使用分析 "},"db/mysql/optimize/一条SQL语句在MySQL中如何执行的.html":{"url":"db/mysql/optimize/一条SQL语句在MySQL中如何执行的.html","title":"一条SQL语句在MySQL中如何执行的","keywords":"","body":"一条SQL语句在MySQL中如何执行的1. MySQL 基础架构分析1.1 MySQL 基本架构概览1.2 Server 层基本组件介绍1.2.1 连接器2. 语法分析2.1 查询语句2.2 更新语句3. 为什么要用两个日志模块4. 总结参考文章一条SQL语句在MySQL中如何执行的 本文会分析一个sql 语句在MySQL中的执行流程，包括 sql的查询在Mysql内部会怎么流转 sql语句的更新是怎么完成的 分析之前会先看Mysql的基础架构 知道Mysql由哪些组件组成 这些组件有什么作用 可以帮助我们解决什么问题 1. MySQL 基础架构分析 1.1 MySQL 基本架构概览 下图是Mysql 的一个简要架构图，从下面的可以清晰的看到用户的SQL语句在MySQL内部是如何执行的 先简单介绍一下下图涉及到的一些组件的基本作用，帮忙大家理解这幅图。在1.2 章会详细介绍到这些组件的作用 连接器：身份认证和权限相关（登录MySQL的时候） 查询缓存：执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用） 分析器：没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的SQL 语句要干嘛，再检查你的SQL 语句语法是否正确 优化器：按照MySQL 认为最优的方案去执行 执行器：执行语句，然后从存储引擎返回数据 简单来说Mysql 主要分为Server层和存储引擎层 Server层 主要包括连接器，查询缓存，分析器，优化器，执行器等 所有跨存储引擎的功能都在这一层实现，比如 存储过程 触发器 视图 函数等 还有一个通过的日志模块binglog日志模块 存储引擎层 主要负责数据的存取和读取，采用可以替换的插件式架构。 支持InnoDB、MyISAM、Memory等多个存储引擎 其中innoDB 引擎有自有的日志模块redolog 现在最常用的存储引擎是InnoDB,他从MySQL5.5.5 版本开始就被当做默认的存储引擎了 1.2 Server 层基本组件介绍 1.2.1 连接器 连接器主要和身份认证和权限相关的功能，就好比一个级别很高的门卫一样 主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。 1.2.2 查询缓存（MySQL 8.0 版本后移除） 查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。 查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。 连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。 MySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。 所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。 MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。 1.2.3 分析器 MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步： 第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。 第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。 完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。 1.2.4 优化器 优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。 可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。 1.2.5 执行器 当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。 2. 语法分析 2.1 查询语句 说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下： select * from tb_student A where A.age='18' and A.name=' 张三 '; 结合上面的说明，我们分析下这个语句的执行流程： 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student,需要查询所有的列，查询条件是这个表的 id='1'。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案： a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。 b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。 ​ 那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。 2.2 更新语句 以上就是一条查询 sql 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql 语句如下： update tb_student A set A.age='19' where A.name=' 张三 '; 我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块式 binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下： 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成。 3. 为什么要用两个日志模块 这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗? 这是因为最开始 MySQL 并没与 InnoDB 引擎( InnoDB 引擎是其他公司以插件形式插入 MySQL 的) ，MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。 并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？ 先写 redo log 直接提交，然后写 binlog，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 先写 binlog，然后写 redo log，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。 如果采用 redo log 两阶段提交的方式就不一样了，写完 binglog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下： 判断 redo log 是否完整，如果判断是完整的，就立即提交。 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。 这样就解决了数据一致性的问题。 4. 总结 MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。 SQL 等执行过程分为两类，一类对于查询等过程如下：权限校验---》查询缓存---》分析器---》优化器---》权限校验---》执行器---》引擎 对于更新等语句执行流程如下：分析器----》权限校验----》执行器---》引擎---redo log prepare---》binlog---》redo log commit 参考文章 一条SQL语句在MySQL中如何执行 "},"db/mysql/problem/无法连接远端Mysql.html":{"url":"db/mysql/problem/无法连接远端Mysql.html","title":"无法连接远端Mysql","keywords":"","body":"无法连接远端Mysql1. 错误提示3. 解决方案4. 具体解决参考无法连接远端Mysql 1. 错误提示 提示：Host 'xxx.xxx.x.xx' is not allowed to connect to this MySQL server 问题原因 客户端想直接用root账户从远端直接连接，Mysql从安全性考虑，限定了root账户只能有本机端localhost连接 可以执行 select Host,User from mysql.user; 结果如下 3. 解决方案 root 的權限開放，讓他也能從遠端連 新建一個帳號，讓他只有所要連接的資料庫的權限，只要是非 root 帳號都可以從遠端連。 4. 具体解决 创建一个远程用户 create user test identified by '123456'; 分配权限 grant all privileges on *.* to 'test'@'%'identified by '123456' with grant option; 刷新mysql用户权限 flush privileges ; 参考 無法遠端連接MySQL "},"db/mysql/interview/MySql面试提问.html":{"url":"db/mysql/interview/MySql面试提问.html","title":"MySql面试提问","keywords":"","body":"MySql面试提问1. 存储引擎篇2. 索引篇3.事务篇4. 锁篇5. 大表优化篇6. 性能优化MySql面试提问 1. 存储引擎篇 你了解MySQL存储引擎吗？说说MyISAM 和 InnoDB 的区别？ 2. 索引篇 什么是索引？ 说说Mysql索引的数据结构？ B+Tree 和B-Tree的不同点 说说MySQL对B+Tree做的最大优化是什么？ 使用B+Tree索引的优势是什么？怎么做到的？ MyISAM 和 InnoDB 都是B+Tree索引，有什么不同？ 为什么InnoDB 一定要有主键？（mysql可以没有主键吗？） 辅助索引搜索的流程？ 说说什么是联合索引？ 3.事务篇 什么是事务？ 事务的四大特性？ 数据库是如何保证ACID的？ 并发事务带来哪些问题？ 不可重复读和幻读的区别？ 事务的隔离级别有哪些？ InnoDB 的 REPEATABLE-READ为什么可以避免幻读 4. 锁篇 MyISAM 和 InnoDB 存储引擎支持的锁？对比一下优缺点？ InnoDB存储引擎的锁算法 读操作上锁吗？ 5. 大表优化篇 你平常工作中有对大表进行优化吗？都是怎么做的？ 什么是垂直分区？优缺点 什么是水平分区？优缺点 说说数据库分片的两种方案？ 6. 性能优化 如何判断一条SQL语句执行的是否高效？（我们建立好的索引在这条SQL中是否被用到了） 一条语句再MySql中的执行过程 为什么 redo log 要引入 prepare 预提交状态？ "},"db/Oracle/":{"url":"db/Oracle/","title":"Oracle","keywords":"","body":"OracleOracle "},"db/Oracle/序列.html":{"url":"db/Oracle/序列.html","title":"序列","keywords":"","body":"序列创建序列序列 create sequence SEQ_TEST minvalue 1 --最小值 nomaxvalue --不设置最大值 start with 1 --从1开始计数 increment by 1 --每次加1个 nocycle --一直累加，不循环 nocache; --不建缓冲区 创建序列 create sequence seq_user increment by 1 minvalue 1 nomaxvalue start with 1 nocycle cache 20; "},"sql/":{"url":"sql/","title":"SQL","keywords":"","body":"SQLSQL SELECT SELECT检索数据 ORDER BY排序检索数据 WHERE过滤数据 LIKE等通配符过滤 创建计算字段 使用函数处理数据 汇总数据（聚集函数） GROUP BY/HAVING分组数据 子查询 JOIN联结表 UNION组合查询 INSERT INSERT插入数据 UPDATE UPDATE更新数据 DELETE DELETE删除数据 创建和操纵表 CREATE TABLE 创建表 ALTER TABLE更新表 DROP TABLE 删除表 RENAME TABLE 重命名表 VIEW视图 存储过程 事务 [索引]( "},"sql/SELECT.html":{"url":"sql/SELECT.html","title":"SELECT检索数据","keywords":"","body":"SELECT检索数据1. SELECT 查询数据1.1 检索单个列1.2 检索多个列1.3 检索所有列1.4 检索不同的值1.5 限制结果SELECT检索数据 1. SELECT 查询数据 使用SELECT 检索数据，必须至少给出两条信息 想选择什么 从什么地方选择 1.1 检索单个列 SELECT prod_name FROM products; 1.2 检索多个列 多个列名用逗号隔开 SELECT prod_id,prod_name FROM products; 1.3 检索所有列 通配符（*），返回所有的列 通常不用*，因为会降低检索和应用程序性能 SELECT * FROM products; 1.4 检索不同的值 DISTINCT关键字，指示数据库只返回不同的值 SELECT DISTINCT vend_id FROM products; 1.5 限制结果 每个数据库都不一致，例如选取前5条 1.5.1 ORACLE下 SELECT prod_name FROM products WHERE ROWNUM 1.5.2 MYSQL SELECT prod_name FROM products LIMIT 5; 如果要得到后面5行数据 SELECT prod_name FROM products LIMIT 3 OFFSET 4; LIMIT 3 OFFSET 4; 表示从第4行起的3行数据， 第一个数字是检索的行数 第二个数字是指从哪儿开始 简化 LIMIT 4 OFFSET 3 简化成 LIMIT 3,4; SELECT prod_name FROM products LIMIT 3,4; "},"sql/orderby.html":{"url":"sql/orderby.html","title":"ORDER BY排序检索数据","keywords":"","body":"ORDER BY排序检索数据1. 为什么使用排序2. 概述2.1 单个列排序2.2 多个列排序2.3 按列位置排序2.4 指定排序的方案ORDER BY排序检索数据 1. 为什么使用排序 如果数据不排序，那么他检索出来的结果，一般以他在数据库底层中的出现顺讯显示，可能是添加的顺序。 但如果数据更新或删除，就有可讷讷个收到DBMS重用回收存储空间的方式影响 2. 概述 ORDER BY 子句取一个或多个列的名字，据此对输出结果进行排序 2.1 单个列排序 对prod_name 列以字母顺序排序 ORDER BY 要在SELECT 语句的最后一行 SELECT prod_name FROM Products ORDER BY prod_name; 2.2 多个列排序 多个列之间用“,”分隔 SELECT prod_id,prod_price,prod_name FROM products ORDER BY prod_price,prod_name; 仅在多个行具有相同的prod_price值时，才对产品按prod_name 进行排序 如果prod_price 列中的所有值都是唯一的，则不会按prod_name 排序 2.3 按列位置排序 除了按列名还可以按列位置 SELECT prod_id,prod_price,prod_name FROM products ORDER BY 2,3; 好处 不用重新输入列名 缺点 不明确给出列名有可能造成错用列名排序 对SELECT 清单进行更改时容易错误地对数据进行排序 2.4 指定排序的方案 ASC (ASCENDING):升序 数据排序不限定升序排序（从A-Z）这只是默认的排序顺序， DESC (DESCENDING):降序 还可以使用ORDER BY 子句进行降序（Z-A）排序。 例：价格降序来排序产品（最贵的排在最前面） SELECT prod_id,prod_price,prod_name FROM products ORDER BY prod_price DESC,prod_name; DESC 只应用到签名的列名 "},"sql/WHERE.html":{"url":"sql/WHERE.html","title":"WHERE过滤数据","keywords":"","body":"WHERE过滤数据2. SQL 过滤与 应用过滤3. WHERE 子句操作符4. 例子4.1 检查单个值4.2 不匹配检查4.3 范围值检查4.4 空值检查5.高级过滤5.1 AND操作符5.2 OR 操作符5.3 求值顺序5.5 NOT操作符WHERE过滤数据 数据库表一般包含大量的数据，很少需要检索所有表中的行。通常只会根据特定操作或者报告的需要提取表中的子集 在SELECT 语句中，数据根据WHERE 子句中指定的搜索条件进行过滤 SELECT prod_name,prod_price FROM products WHERE prod_price = 3.49; 这条语句并不返回所有行，只返回prod_price 的值为3.49的行 2. SQL 过滤与 应用过滤 数据也可以在应用层过滤，为此SQL 的SELECT 语句为客户端应用检索出超过实际所需的数据，然后客户端代码对返回的数据进行循环，提取出所需要的行 这种方式极不妥 优化数据库后可以更快速有效的对数据进行过滤，极大提升响应性能 服务器发送多余数据，导致带宽浪费 3. WHERE 子句操作符 操作符 说明 操作符 说明 = 等于 > 大于 <> 不等于 >= 大于等于 != 不等于 !> 不大于 小于 BETWEEN 在指定的两个值之间 小于等于 IS NULL 为NULL 值 ! 不小于 4. 例子 4.1 检查单个值 加个小于10美元的产品 SELECT prod_name,prod_price FROM products WHERE prod_price 4.2 不匹配检查 SELECT vend_id,prod_name FROM products WHERE vent_id != 'DLL01'; 4.3 范围值检查 检查某个范围的值可以使用BETWEEN操作符 SELECT prod_name,prod_price FROM products WHERE prod_price BETWEEN 5 AND 10; 4.4 空值检查 用来检查具有NULL值的列 SELECT cust_name FROM Customers WHERE cust_email IS NULL; 5.高级过滤 5.1 AND操作符 SELECT 语句包含两个过滤条件，用AND连接在一起 AND 操作符只给出满足所有条件的行 SELECT prod_id,prod_price FROM products WHERE vend_id=\"DELL01\" AND prod_price 5.2 OR 操作符 OR操作符 表示匹配任一条件的行 在满足第一个条件之后就不再执行第二个条件了 SELECT prod_name,prod_price FROM products WHERE vend_id ='DLL01' OR vend_id =\"BRS01\" 5.3 求值顺序 WHERE 子句可以包含任意的AND和OR 操作符，但会出现一个问题 在处理OR操作符之前，会先处理AND 操作符 为了解决此问题，需要使用圆括号对操作符进行明确分组 圆括号具有比AND或OR 更高的求值顺序 SELECT prod_name,prod_price FROM prodcts WHERE (vend_id='DLL01' OR vend_id='BRS01') AND prod_price >=10 5.4 IN 操作符 IN 操作符用来指定条件范围，范围中的每个条件都可以进行匹配 IN 取一组由逗号分隔，括在圆括号中的合法值 SELECT prod_name ,prod_price FROM products WHERE vend_id IN ('DLL01','BRS01') ORDER BY prod_name; 5.4.1 为什么选IN 操作符 在有很多合法选项时，IN 操作符更清楚，更直观 在与其他AND和OR 操作符组合使用IN 时，求值顺序更容易管理 IN 操作符比一组OR 操作符执行得更快 IN 最大的优点可以包含其他SELECT 语句，能够动态简历WHERE 子句 5.5 NOT操作符 NOT 操作可以否定其后所跟的任何条件 SELECT prod_name FROM products WHERE NOT vend_id='DLL01' ORDER BY prod_name; "},"sql/LIKE等通配符过滤.html":{"url":"sql/LIKE等通配符过滤.html","title":"LIKE等通配符过滤","keywords":"","body":"LIKE等通配符过滤1. LIKE 操作符1.1 百分号（%）通配符1.2 下划线（_）通配符2. 使用通配符技巧LIKE等通配符过滤 通配符是用来匹配值的一部分的特殊字段 通配符只能用于文本字段（字符串）。非文本数据类型字段不能使用通配符 1. LIKE 操作符 1.1 百分号（%）通配符 最常使用的通配符%，%表示任意字符串出现任意次数 #找出所有fish 开头的产品 SELECT prod_id,prod_name FROM products WHERE prod_name LIKE 'fish%' 通配符可以在任意位置使用，并且可以使用多个通配符 SELECT prod_id,pro_name FROM products WHERE prod_name LIKE '%bean bag%' 注：WHERE prod_name LIKE '%'不会匹配产品名称为NULL 的行 1.2 下划线（_）通配符 '_'通配符只能匹配单个字符，而不是多个字符 SELECT prod_id,prod_name FROM products WHERE prod_name LIKE '__ inch teddy' 2. 使用通配符技巧 使用通配符很有用，但也要付出代价的，既通配符搜索要耗费更长的处理时间 不要过度使用通配符，如果其他操作符能达到相同目的，应该使用其他操作符 在确定使用通配符时，也尽量不要把通配符写在开始处，开始处搜索起来是最慢的 "},"sql/创建计算字段.html":{"url":"sql/创建计算字段.html","title":"创建计算字段","keywords":"","body":"创建计算字段1.概述2. 拼接字段2.1 SQL Server 使用+2.2 在DB2,ORACLE ,中使用||2.3 在mysql或MariaDB时需要使用的语句3. TRIM 函数4. AS 使用别名5. 执行算术计算创建计算字段 1.概述 存储在数据库表中的数据一般不是应用程序所需要的格式，我们需要直接从数据库中检索出转换、计算或者格式化数据，而不是检索出数据在客户端转换 注：在数据库中完成格式化要比在客户端完成快得多 2. 拼接字段 拼接：将值联结在一起. 拼接在每种数据库中的关键字并不相同，有的使用+号有的使用双竖杆（||），在Mysql中需要使用特殊函数 注：左圆括号前面有个空格 2.1 SQL Server 使用+ SELECT vend_name +' ('+vend_country')' FROM Vendors ORDER BY vend_name; 2.2 在DB2,ORACLE ,中使用|| SELECT vend_name +' ('+vend_country')' FROM Vendors ORDER BY vend_name; 2.3 在mysql或MariaDB时需要使用的语句 SELECT Concat(vend_name,' (',vend_country,')') FROM Vendors ORDER BY vend_name; 3. TRIM 函数 TRIM() ：去掉字符串左右两个的字符 RTRIM(): 去掉字符串右边空格 LTRIM() : 去掉字符串左边空格 SELECT RTRIM(vend_name) FROM Vendors ORDER BY vend_name; 4. AS 使用别名 别名用AS关键字赋予 SELECT Concat(vend_name,' (',vend_country,')') AS vend_title FROM Vendors ORDER BY vend_name; 5. 执行算术计算 支持基本算术运算符，圆括号可以用来区分先后顺序 SELECT prod_id, quantity, item_price, quantity*item_price AS expanded_price FROM OrderItems WHERE order_num =20008; "},"sql/使用函数处理数据.html":{"url":"sql/使用函数处理数据.html","title":"使用函数处理数据","keywords":"","body":"使用函数处理数据2. 函数的使用2.2 日期与时间处理函数（不可移植/差）2.3 数值处理函数（）使用函数处理数据 函数带来的问题 只有少数几个函数被主要的DBMS 等同支持，但有些各个数据库中函数名称与语法可能及其不同 SQL 函数不可移植 为了代码的可移植性，许多SQL程序员不赞成使用特定于实现的功能 不使用函数，有时候不利于程序性能，编写某些应用程序代码会很难 2. 函数的使用 2.1 文本处理函数(可移植) 常用的文本处理函数 函数 说明 LEFT()(或使用子字符串函数) 返回字符串左边的字符 LENGTH() 返回字符串长度 LOWER() 将字符串转换为小写 LTRIM() 去掉字符串左边的空格 RIGHT()(或使用子字符串函数) 返回字符串右边的字符 RTRIM() 去掉字符串右边的函数 SOUNDEX() 返回字符串的SOUNDEX值 UPPER() 将字符串转换为答谢 2.1.1 SOUNDEX() SOUNDEX 是一个将任何文本串转换为描述其语音表示的字母数字模式的算法 SOUNDEX 考虑了类似的发音字符和音节，使得能够对字符串进行发音比较而不是字母比较 SELECT cust_name,cust_contact FROM Customers WHERE SOUNDEX(cust_contact)= SOUNDEX('michael Green') 2.2 日期与时间处理函数（不可移植/差） 日期和时间采用相同的数据类型存储在表中，每种DBMS 都有自己的特殊形式，日期和时间值以特殊格式存储，以便能快速和有效的排序或过滤，并且节省存储空间 用用程序一般不使用日期和时间的存储格式，因此日期和时间函数总是用来读取，统计和处理这些值 例：要检索2012年所有的订单 2.2.1 SQL Server SELECT order_num FROM Oders WHERE DATEPART(yy,order_date) =2012; 2.2.2 ORACEL ORACLE 没有DATEPART函数，要借助其他几个函数 SELECT order_num FROM Oders WHERE to_number (to_char(order_date,'YYYY'))=2012; to_char() 函数用来提取日期的成分 to_number() 用来提取的成分转换为数值 2.2.3 Mysql和MariaDB Mysql 具有各种日期处理函数 SELECT ordrr_num FROM Orders WHERE YEAR(order_date) = 2012; 2.3 数值处理函数（） 函数 说明 ABS() 返回一个数的绝对值 COS() 返回一个角度的余弦 EXP() 返回一个数的指数值 PI() 返回周期率 SIN() 返回一个角度的正弦 SQRT() 返回一个数的平方根 TAN() 返回一个数的正切 "},"sql/汇总数据.html":{"url":"sql/汇总数据.html","title":"汇总数据（聚集函数）","keywords":"","body":"汇总数据（聚集函数）1. SQL 聚集函数1.1 AVG() 平均值函数1.2 COUNT() 函数1.3 MAX() 函数1.4 MIN() 函数1.5 SUM() 函数2. SQL 聚集不同值3. 组合聚集函数汇总数据（聚集函数） 1. SQL 聚集函数 函数 说明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 1.1 AVG() 平均值函数 AVG() 对表中行数计算并计算其列值之和，求得该列的平均值。 # 所有产品的平均价格 SELECT AVG(prod_price) AS avg_price FROM Products; AVG函数将忽略null 的行 1.2 COUNT() 函数 COUNT()确定表中的行数目或符合特定条件的行的数目 ·两种使用方式 使用COUNT()对表中行的数目进行计算，*不管表列中包含的是空值（NULL）还是非空值 SELECT COUNT(*) AS num_cust FROM Customers; 使用COUNT(column)对特定列中具有值的行进行计数，忽略NULL SELECT COUNT(cust_email) AS num_cust FROM Customers; 1.3 MAX() 函数 MAX()返回指定列的最大值，MAX()要求指定列名 SELECT MAX(prod_price) AS max_price FROM Products; MAX（） 一般用来找出最大的数值或日期值，对于文本返回该列排序后的最后一行 MAX() 忽略值为NULL 的行 1.4 MIN() 函数 MIN() 返回指定列的最小值 SELECT MIN(prod_price) AS min_price FROM Products; 1.5 SUM() 函数 SUM() 返回指定列值的和（总和） SELECT SUM(quantity) AS items_ordered FROM OrderItems WHERE order_num = 20005; SUM 忽略NULL值 2. SQL 聚集不同值 对所有行执行计算，指定ALL参数或不指定参数（ALL 是默认行为） 只包含不同值，指定DISTINCT SELECT AVG(DISTINCT prod_price) AS avg_price FROM Products WHERE vend_id ='DLL01'; 3. 组合聚集函数 SELECT COUNT(*) AS num_items, MIN(prod_price) AS price_min, MAX(prod_price) AS price_max, AVG(prod_price) AS price_avg FROM Products; "},"sql/分组数据.html":{"url":"sql/分组数据.html","title":"GROUP BY/HAVING分组数据","keywords":"","body":"GROUP BY/HAVING分组数据2. 创建分组3. 过滤分组3.1 HAVING 和WHERE 的差别3.2 同时使用HAVING 和WHERE4. 分组和排序4.1 ORDER BY 与GROUP BY 的区别4.2 不要忘记使用ORDER BYGROUP BY/HAVING分组数据 分组数据主要涉及两个子句 GROUP BY GROUP BY 子句可以包含任意数目的列，因此可以对分组进行嵌套，更细致的进行分组 如果在GROUP BY 子句中嵌套了分组，数据将在最后指定的分组上进行汇总，换句话说，在建立分组时，指定的所有列都一起计算（所以不能从个别列取回数据） GROUP BY 子句中列出的每一列都必须是检索列或有效表达式 GROUP BY 子句必须出现在WHERE 子句之后，GROUP BY 子句之前 HAVING 2. 创建分组 分组是使用GROUP BY 子句建立的 # 查询出相同供应商的产品数量 SELECT vend_id,COUNT(*) AS num_prods FROM Products GROUP BY vend_id; 3. 过滤分组 除了能用GROUP BY 分组数据外，SQL 还允许过滤分组，规定包括哪些分组，排除哪些分组。 HAVING 类似于WHERE ,只不过WHERE 过滤行，而HAVING 过滤分组 SELECT cust_id,COUNT(*) AS orders FROM Orders GROUP BY cust_id HAVING COUNT(*) >= 2; 3.1 HAVING 和WHERE 的差别 WHERE 在数据分组前进行过滤 HAVING 在数据分组之后进行过滤 3.2 同时使用HAVING 和WHERE # where 过滤所有prod_price 至少为4的行，然后按vend_id 分组数据，HAVING 子句过滤计数为2或2以上的分组 SELECT vend_id,COUNT(*) AS num_prods FROM Products WHERE prod_price >= 4 GROUP BY vend_id HAVING COUNT(*) >=2; 4. 分组和排序 4.1 ORDER BY 与GROUP BY 的区别 ORDER BY GROUP BY 对产生的输出排序 对行分组，但输出可能不是分组顺序 任意列都可以使用（设置非选择的列也可以使用） 只可能使用选择列或表达式列，而且必须使用每个选择列表达式 不一定需要 如果与聚集函数一起使用列，则必须使用 4.2 不要忘记使用ORDER BY 一般在使用GROUP BY 子句时，应该也给出ORDER BY 子句，这是保证数据正确排序的唯一方法，千万不要依赖GROUP BY 排序数据 "},"sql/子查询.html":{"url":"sql/子查询.html","title":"子查询","keywords":"","body":"子查询2. 为什么要创建子查询？2.1 利用子查询进行过滤2.2 作为计算字段使用子查询子查询 SQL 还允许创建子查询，既嵌套在其他查询中的查询 2. 为什么要创建子查询？ 2.1 利用子查询进行过滤 可以把一条SELECT 语句的返回结果用于另一条SELECT 语句的WHERE 语句中 SELECT cust_id FROM Orders WHERE order_num in (SELECT order_num FROM OrderItems WHERE prod_id = 'RGAN01'); SELECT 子查询总是从内向外处理 只能返回单列 作为子查询的SELECT 语句只能查询单个列，企图检索多个列将返回错误 2.2 作为计算字段使用子查询 SELECT cust_name, cust_state, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_iid = Customers.cust_id) AS orders FROM Customers ORDER BY cust_name; "},"sql/联结表.html":{"url":"sql/联结表.html","title":"JOIN联结表","keywords":"","body":"JOIN联结表1. 联结1.2 反例2.联结方式2.1 内联结2.2 自联结（self-join）2.3 自然联结2.4 外联结JOIN联结表 1. 联结 SQL 最强大的功能之一就是能在数据查询的执行中联结（join）表 联结是一种机制，用一条SELECT 语句中关联表 1.2 反例 结果为笛卡尔积 由没有联结条件的表关系返回的结果为笛卡尔积，检索出的行的数目将是第一个表的行数乘以第二个表中函数 SELECT vend_name,prod_name.prod_price FROM Vendors,Products; 这里的返回结果并不是我们想要的，返回的数据用每个供应商匹配了每个产品，包括了供应商不正确的产品（即使供应商根本就没有的产品） 2.联结方式 2.1 内联结 内联结：两个表之间的相等测试 联结条件用特定的ON 子句而不是WHERE 子句给出 SELECT vend_name,prod_name,prod_price FRM Vendors INNER JOIN Products ON Vendors.vend_id = Products.vend_id; 2.2 自联结（self-join） SELECT c1.cust_id,c1.cust_name.c1.cust_contact FROM Customers AS c1,Customers AS c2 WHERE c1.cust_name=c2.cust_name AND c2.cust_contact='Jim Jones' 为什么用自联结而不用子查询 因为处理联结远比处理子查询快得多 2.3 自然联结 SELECT C.*,O.order_num,O.order_date,OI.prod_id FROM Customers AS C,Orders AS O,OrderItems AS OI WHERE C.cust_id = O.cust_id AND OI.order_num = O.Order_num AND prod_id = 'RGAN01'; 2.4 外联结 内联结和外联结不同的是，外连接不包含没有关联的行 左外连接 右外连接 全外连接 全外连接包含两个不关联的行 SELECT Customers.cust_id,Orders,order_num FROM Customers INNER JOIN Orders ON Customers.cust_id = Orders.cust_id; "},"sql/UNION组合查询.html":{"url":"sql/UNION组合查询.html","title":"UNION组合查询","keywords":"","body":"UNION组合查询UNION组合查询 大多数SQL查询只包含一个或多个表中返回数据的单条SELECT 语句。但是，SQL也允许执行多个查询（多条SQL语句），并将结果作为一个查询结果集返回。这些组合查询通常称为并（union）或复合查询（compound query） 两种情况需要组合查询 在一个查询中从不同的表返回结构数据 对一个表执行多个查询，按一个查询返回数据 SELECT cust_name,cust_contact,cust_email FROM Customers WHERE cust_state IN ('IL','IN') UNION SELECT cust_name,cust_contact,cust_email FROM Customers WHERE cust_name='Fun4AL;'; "},"sql/insert/INSERT插入数据.html":{"url":"sql/insert/INSERT插入数据.html","title":"INSERT插入数据","keywords":"","body":"INSERT插入数据2. 插入的几种方式2.1 插入完整的行2.2 插入某些查询的结果3.从一个表复制到另一个表3.1 SELECT INTO 与 INSERT SELECT区别INSERT插入数据 INSERT 用来将行插入（或添加）到数据库表 2. 插入的几种方式 2.1 插入完整的行 要求指定表名和插入到新行中的值 INSERT INTO Customers VALUES ('1000006', 'tom', '1234', 'NYU', NULL ) 存储到表中每一列的数据在VALUES子句中给出，必须给每一列提供一个值， 如果列没有值，应该使用NULL值（假定表允许控制） 各列必须以表定义中出现的次序填充 这种方式面临的问题 虽然语法简单，但并不安全，应该尽量避免使用· 2.1.1 更安全的写法 INSERT INTO Customers(cust_id, cust_name, cust_address, cust_city) VALUES('1000006', 'TOM', '123 ADY', 'SHANG HAI' ) 因为提供了列名，VALUES 必须以其指定的次序匹配指定的列名，不一定按各列出现在表中实际次序 优点：即使表结构改变，这条INSERT 语句仍然能正常工作 2.2 插入某些查询的结果 INSERT SELECT: 利用SELECT 的结果插入表中，他是由INSERT 和一条SELECT语句组成 # 把顾客列合并到Customers中 INSERT INTO Customers(cust_id, cust_contact, cust_email, cust_name) SELECT cust_id, cust_contact, cust_email, cust_name FROM CustNew; 2.2.1 插入多行 INSERT 通常只插入一行，要插入多行，必须执行多个INSERT 语句， 但是INSERT SELECT 是个例外，他可由用一条INSERT 插入多行，不管select 返回多少行，都将被insert 插入 3.从一个表复制到另一个表 有一种数据插入不使用 INSERT 语句。要将一个表的内容复制到一个新的表（运行中创建的表），可以使用SELECT INTO 语句 SELECT * INTO CustCopy FROM Customers; 在MYSQL,ORACLE,SQLITE 中的语法稍有不同 CREATE TABLE CustCopy AS SELECT * FROM Customersl 3.1 SELECT INTO 与 INSERT SELECT区别 INSET SELECT 将数据添加到一个已经存在的表（插入数据） SELECT INTO 将数据复制到一个新表（导出数据） "},"sql/insert/UPDATE更新数据.html":{"url":"sql/insert/UPDATE更新数据.html","title":"UPDATE更新数据","keywords":"","body":"UPDATE更新数据UPDATE更新数据 更新（修改）表中的数据，使用UPDATE语句 UPDATE 语法分为 要更新的表 列名和他们的新值 确定要更新哪些行的过滤条件 UPDATE Customers SET cust_email ='zsz@gmail.com', cust_contact = 'zsz' WHERE cust_id = '100001'; "},"sql/insert/DELETE删除数据.html":{"url":"sql/insert/DELETE删除数据.html","title":"DELETE删除数据","keywords":"","body":"DELETE删除数据DELETE删除数据 DELETE FROM Customers WHERE cust_id = '1100001'; "},"sql/table/创建表.html":{"url":"sql/table/创建表.html","title":"CREATE TABLE 创建表","keywords":"","body":"创建表1.创建基础表2. 使用NULL 值2.1 NULL 和空字符3.指定默认值3.1 当前时间创建表 1.创建基础表 创建表的语法 新表的名字，在关键字CREATE TABLE 之后给出 表列的名字和定义用逗号分隔 CREATE TABLE Products( prod_id CHAR(10) NOT NULL, vend_id CHAR(10) NOT NULL, prod_name CHAR(254) NOT NULL, prod_price DECIMAL(8,2) NOT NULL, prod_desc VARCHAR(1000) NULL ); 不同数据库对应的语法略微不同 2. 使用NULL 值 NULL 值就是没有值或缺值， 允许NULL值的列也允许在插入行时不给出该列的值， 不允许NULL值的列不接受没有列值的行（该列必须有值） 2.1 NULL 和空字符 NULL 是没有值，不是空字符串，如果指定''，这在NOT NULL列中是允许的，空字符串是一个有效的值 3.指定默认值 在插入行时如果不给出值，DBMS 将自动采用默认值，默认值使用DEFAULT 指定 CREATE TABLE OrderItems( order_num INTEGER NOT NULL, quantity INTEGER NOT NULL DEFAULT 1 ); 3.1 当前时间 默认值还经常用于日期或者时间，但是哥哥DBMS 是不同的 DBMS 函数/变量 MySQL CURRENT_DATE() ORACLE SYSDATE "},"sql/table/更新表.html":{"url":"sql/table/更新表.html","title":"ALTER TABLE更新表","keywords":"","body":"ALTER TABLE更新表1. DBMS 限制不同2.更改表2.1 新增列2.2 删除列2.2.1 复杂表结构删除步骤ALTER TABLE更新表 1. DBMS 限制不同 使用ALTER TABLE 是需要考虑的事 理想情况下，不要再表中包含数据时对其进行更新，应该在表设计过程中充分考虑未来可能的需求，避免今后对表的结构做大改动 所有的DBMS 都允许给现有的表增加列，不过对所增加的列数据类型（以及NULL和DEFAULT）有所限制 许多DBMS 不允许删除或更改表中的列 许多DBMS 允许重命名表中的列 许多DBMS 限制对已经天佑数据的表进行更改，对未填充数据的列几乎没有限制 2.更改表 2.1 新增列 ALTER TABLE Vendors ADD vend_phone CHAR(20); 2.2 删除列 并非对所有的DBMS 有效 ALTER TABLE Vendors DROP COLUMN vend_phone; 2.2.1 复杂表结构删除步骤 用新的列布局创建一个新表 使用INSERT SELECT 语句从旧表复制数据到新表。有必要的话，可以使用转换行数和计算字段 检验包含所需数据的新表 重命名旧表（如果确定，可以删除） 将旧表原来的名字重命名新表 根据需要，重新创建触发器，存储过程，索引和外键 "},"sql/table/删除表.html":{"url":"sql/table/删除表.html","title":"DROP TABLE 删除表","keywords":"","body":"DROP TABLE 删除表DROP TABLE 删除表 删除表是删除整个表，而不是内容 DROP TABLE CustCopy; 删除表没有确定，也不能撤销，执行后将永久删除该表 "},"sql/table/重命名表.html":{"url":"sql/table/重命名表.html","title":"RENAME TABLE 重命名表","keywords":"","body":"RENAME TABLE 重命名表RENAME TABLE 重命名表 各个DBMS 不一致 MySQL RENAME TABLE user11 TO user10; "},"sql/view/":{"url":"sql/view/","title":"VIEW视图","keywords":"","body":"VIEW视图1. 是什么1.1 使用背景1.2 使用视图包装成虚拟表2. 为什么使用视图2.1 性能问题3. 创建视图3.1 利用视图简化复杂的联结3.2 用视图重新格式化检索出的数据3.3 使用视图过滤不想要的数据3.4 使用视图与计算字段VIEW视图 1. 是什么 视图是虚拟的表，与包含数据的表不一样，视图值包含使用时动态检索数据的查询 MySQL 从5版本开始支持视图 SQLite 只支持读视图，不支持更改 视图提供了一种封装SELECT 语句的层次，可以用来简化数据处理，重新格式化或保护基础数据 1.1 使用背景 我们先看一个SQL SELECT cust_name,cust_contact FROM Customers,Orders,OrderItems WHERE Customers.cust_id = Orders.cust_id AND OrderItem.order_num = Orders.order_num AND prod_id = 'RANG01'； 这里查看用来检索订购了某种商品的顾客。 任何需要这个数据的人都必须理解相关的表结构，知道如何创建查询和对表进行联结检索其他产品的相同数据，必须修改最后的where子句 1.2 使用视图包装成虚拟表 我们可以吧整个查询包装成一个名为ProductCustomers 的虚拟表，则可以轻松的检索出相同的数据 SELECT cust_name,cust_contact FROM ProductCustomers WHERE prod_id = 'RANG01'; ProductCustomers 是一个视图，他不包含任何列或数据，包含的是一个查询 2. 为什么使用视图 重用SQL语句 简化复杂的SQL操作，在编写查询后，可以方便得重用他而不必知道其基本查询细节 使用表的一部分而不是整个表 保护数据。可以授予用户访问表的特定部分权限，而不是整个表的访问权限 更改数据格式和表示。视图可以返回与底层表的表示和格式不同的数据 创建视图之后，可以用与表的基本相同的方式使用他们。可以对视图执行SELECT 操作，过滤，排序数据。将视图联结到其他视图或表，甚至天天加班和更新数据 视图本身不包含数据，返回的数据来自其他表 2.1 性能问题 因为视图不包含数据，所以每次使用视图时，都必须处理查询执行时需要的所有检索。 如果你用多个联结和过滤创建了复杂的视图或者嵌套了视图，性能可能就会下降的厉害。 PS:部署前要进行测试 3. 创建视图 视图用CREATE VIEW 来创建 删除视图：DROP VIEW viewname; 重命名视图：必须先删除在重新创建 3.1 利用视图简化复杂的联结 一个最常见的视图应用是隐藏复杂的SQL,这通常涉及联结 CREATE VIEW ProductCustomers AS SELECT cust_name,cust_contact,prod_id FROM Customer,Orders,OrderItems WHERE Customer.cust-id= Orders.cust_id AND OrderItems.order_num = Orders.order_num; 这条语句创建了一个名为ProductCustomers 的视图，联结了三个表，返回已订购了任意产品的所有顾客列表 检索订购了产品RANG01 的顾客 SELECT cust_name,cust_contact FROM ProductCustomers WHERE prod_id = 'RANG01'; 3.2 用视图重新格式化检索出的数据 CREATE VIEW VendorLocations AS SELECT RTRIM(vend_name)+'('+RTRIM(vend_country)+')' AS vend_id FROM Vendors; 3.3 使用视图过滤不想要的数据 CREATE VIEW CustomerEMailList AS SELECT cust_id,cust_name,cust_email FROM Customers WHERE cust_email IS NOT NULL; 3.4 使用视图与计算字段 CREATE VIEW OrderItemsExpanded AS SELECT order_num, prod_id, qquantity*item_price AS expanded_price FROM OrderItems; "},"sql/存储过程/":{"url":"sql/存储过程/","title":"存储过程","keywords":"","body":"存储过程1. 背景1.1 解决方案2. 是什么3. 为什么使用存储过程3.1 缺点4. 创建存储过程4.1 ORACLE5.执行存储过程存储过程 1. 背景 我们大多数SQL语句都是针对一个或多个表的单条语句，但是并非所有表都这么简单。 经常会有一些复杂的操作需要多条语句才能完成 执行这个处理需要针对许多表的多条sql语句 需要执行的具体sql语句及其次序也不是固定的(例如;可能会根据物品的库存而变化) 1.1 解决方案 单独编写每条SQL语句，并根据结果又条件得执行其他语句 2. 是什么 存储过程就是为以后使用而保存的一条或多条SQL,可以将他视为批文件，虽然作用不仅仅是批文件 MySQL 5 之后才支持存储过程 SQLite不支持存储过程 3. 为什么使用存储过程 通过把处理封装在一个易用的单元中，可以简化复杂操作 由于不要求反复建立一系列的处理不走，因而保证了数据的一致性。如果所有开发人员和应用程序都使用同一存储过后才能，则所使用的代码都是相同的，这一点的延伸就是防止错误。需要执行的步骤越多，出错的可能性就越大 简化对变动的管理 因为存储过程通常以编译过的形式存储，所以工作量少，提高了性能 简化来说：简单，安全、高性能 3.1 缺点 不同DBMS 中存储过程语法不同 编写存储过程比编写基本SQL语句复杂，需要更高的技能，更丰富的经验 4. 创建存储过程 对邮件发送清单中具有邮件地址的顾客进行计算 4.1 ORACLE CREATE PROCEDURE MailinglistCount( ListCount OUT INTEGER) IS v_rows INTEGER BEGIN SELECT COUNT(*) INTO v_rows FROM CUSTOMERS wHERE NOT cust_email IS NOLL ListCount := v_rows; END; 这个存储过程有一个ListCount 的参数。 此参数从存储过程返回一个值，而不是传递一个值给存储过程 关键字OUT用来表示行为 IN 传递值给存储过程 OUT 从存储过程返回值 INOUT 既传值给存储过程，也从存储过程返回值 存储过程的代码在BEGIN 和END 语句中 4.1.1 调用方式 var ReturnValue NUMBER EXEC MaillingListCount(:ReturnValue); SELECT returnValue; 这段代码声明一个变量来保存存储过程返回的任何值，然后执行存储过程，在使用SELECT 语句显示返回的值 5.执行存储过程 执行存储过程，既EXECUTE EXECUTE 接受存储过程名和需要传递给他的任何参数 EXECUTE AddNewProduct('jts01', 'stuffed eiffel', 6.49); 这里执行一个名为AddNewProduct的存储过程，将一个新产品添加到Product表中 "},"sql/transaction/":{"url":"sql/transaction/","title":"事务","keywords":"","body":"事务1.1 术语2. 事务操作实例2.1 SQL Server2.2 MySQL2.3 ORACLE3. 事务结束3.1 ROLLBACK3.2 COMMIT4.保留点4.1 是什么4.2 MySQL,ORACLE事务 事务处理（transaction processing）：通过确保成批的SQL 操作要么完全执行，要么完全不执行，来维护数据库的完整性 1.1 术语 事务(transaction)： 指一组SQL语句 回退（rollback） 撤销指定SQL语句的过程 提交（commit） 将未存储的SQL语句结果写入数据库表 保留点（savepoint） 事务处理中设置的临时占位符（placeholder），可以对他发布回退（与回退整个事务不同） 2. 事务操作实例 2.1 SQL Server BEGIN TRANSATION ... COMMIT TRANSACTION BEGIN TRANSATION 和 COMMIT TRANSACTION 语句之间的SQL 必须完全执行或者完全不执行 2.2 MySQL START TRANSACTION ... 2.3 ORACLE SET TRANSACTION ... 3. 事务结束 多数实现没有明确表示事务处理在何时结束，事务一直存在，知道被中断。 通常COMMIT 用于保存更改，ROLLBACK用于撤销 3.1 ROLLBACK SQL 的 ROLLBACK 命令用例回退（撤销）SQL 语句 DELETE FROM ORDER； ROLLBACK; 3.2 COMMIT 一般的SQL语句都是针对数据库表直接执行和编写的，这就是隐式提交（implicit commit）,既提交操作是自动进行的 3.2.1 ORDER SET TRANSCTION DELETE OrderItems WHERE order_num =12345; DELETE Orders WHERE order_num = 1234; COMMIT; 4.保留点 使用简单的ROLLBACK 和 COMMIT 语句，可以写入或者撤销整个事务。 但是，复杂的事务可能需要部分提交或者回退 4.1 是什么 要支持回退部分事务，必须在事务处理快中的合适位置放置占位符，这样如果需要回退，可以回退到某个占位符。这些占位符就是保留点 4.2 MySQL,ORACLE 创建占位符 SAVEPOINT delete1； 回退到占位符 ROLLBACK TO delete1; "},"sql/索引.html":{"url":"sql/索引.html","title":"索引","keywords":"","body":"索引定义索引前注意内容索引 定义索引前注意内容 索引改善检索操作的性能，但是降低了数据插入、修改和删除的性能，在执行这些操作时，DBMS 必须动态更新索引 索引数据可能要占用大量的存储空间 并非所有的数据都适合做索引，取值不多的数据（如州，性别），不如取值更多可能性的数据（如姓或名），能通过索引得到那么多好处 索引用于数据过滤和排序，如果你经常以某种特定的顺序排序数据，则该数据可能合适做索引 "},"mq/RabbitMQ/RabbitMQ基础.html":{"url":"mq/RabbitMQ/RabbitMQ基础.html","title":"RabbitMQ基础","keywords":"","body":"RabbitMQ基础1. 简介2. 核心概念2.1 整体架构2.3 Banding 绑定2.4 Queue（消息队列）2.5 Broker（消息中间件的服务节点）3. Exchange Types(交换器类型)RabbitMQ基础 1. 简介 RabbitMQ 是采用Erlang 语言实现 AMQP(Advanced Message Queuing Protocol，高级消息队列协议) 的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。 具有如下特点 可靠性：RabbitMQ 使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等 灵活的路由：在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ 已经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。 扩展性：多个RabbitMQ 节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中的节点 高可用性：队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用 支持多种协议：RabbitMQ 除了原生支持 AMQP 协议，还支持 STOMP、MQTT等多种消息中间件协议 多语言客户端：RabbitMQ 几乎支持所有常用语言，比如Java、Python、Ruby、PHP等 易用的管理界面：RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。 插件机制：RabbitMQ 提供了许多插件，以实现从多个方面进行扩展，当然也可以编写自己的插件 2. 核心概念 RabbitMQ 整体上是一个生产者与消费者模型，主要负责接收、存储和转发消息。 可以把消息传递的过程想象成：当你将一个包裹送到邮局，邮局会暂存并最终将邮件通过邮递员送到收件人的手上，RabbitMQ 就好比邮局、邮箱和邮递员组成的一个系统（从计算机术语层面来说，RabbitMQ 模型更像是一种交换机模型） 2.1 整体架构 2.1 Producer(生成者)和 Consumer(消费者) Producer(生产者)：生产消息的一方（邮件投递者） Consumer(消费者)：消费消息的一方（邮件收件人） 消息一般由2部分组成：消息头（或者说是标签Label）和消息体。 消息体: 消息体也可以称为payLoad，消息体是不透明的, 消息头则由一系列可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 生产者把消息交由RabbitMQ后，RabbitMQ会根据消息头把消息发送给感兴趣的Consumer(消费者)。 2.2 Exchange(交换器) 在RabbitMQ 中，消息并不是直接被投递到 Queue（消息队列）中的，中间还需要经过 Exchange(交换器)这一层。Exchange(交换器)会把我们的消息分配到对应的Queue（消息队列）中 Exchange(交换器)用来接收生产者发送的消息并将这些消息路由给服务器中队列中，如果路由不到，或者会返回给Producer（生产者），或许会被直接丢弃掉。（这里可以将RabbitMQ）中的交换器看做一个简单的实体。 2.2.1 Exchange 的四种策略 RabbitMQ 的Exchange（交换器）有四种类型，不同的类型对应着不同的路由策略 direct（默认的） fanout topic headers 不同类型的Exchange转发消息的策略有所区别。 2.2.2 Exchange(交换器)示意图 2.3 Banding 绑定 生产者将消息发给交换器的时候，一般会指定一个 RoutingKey(路由键)，用来指定这个消息的路由规则，而这个RoutingKey需要与交换器类型和绑定键（Bindingkey）联合使用才能最终生效 RabbitMQ 中通过 Binding(绑定) 将 Exchange(交换器)与 Queue（消息队列）关联起来，在绑定的时候，一般会指定一个BindingKey(绑定键)，这样RabbitMQ 就知道如何正确的将消息路由到队列了 如下图所示，一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange和 Queue 的绑定可以是多对多的关系 2.3.1 Binding（绑定）示意图 生产者将消息发送给交换器时，需要一个RoutingKey，当BindingKey 和 RoutingKey 相匹配时，消息会被路由到对应的队列中。在绑定多个队列到同一个交换器的时候，这些绑定允许使用相同的BindKey。BindKey并不是在所有的情况下都生效，它依赖于交换器类型，比如fanout 类型的交换器就会无视，而是将消息路由到所有绑定到该交换器的队列中 2.4 Queue（消息队列） Queue(消息队列)用来保存消息直到发送给消费者。他是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走 RabbitMQ 中消息只能存储在 队列 中，这一点和kafka 这种中间件相反。kafka 将消息存储在topic（主题）这个逻辑层面，而相对应的队列逻辑知识topic实际存储文件中的位移标识。RabbitMQ 的生产者生产消息并最终投递到队列中，消费者可以从队列中获取消息并消费。 多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊（Round-Robin，即轮询）给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理，这样避免的消息被重复消费 RabbitMQ 不支持队列层面的广播消费，如果有广播消费的需求，需要在其上进行二次开发，这样会很麻烦，不建议这样做 2.5 Broker（消息中间件的服务节点） 对于 RabbitMQ 来说，一个RabbitMQ Broker 可以简单地看作是一个RabbitMQ 服务节点，或者RabbitMQ服务实例。大多数情况下也可以讲一个RabbitMQ Broker 看作一台 RabbitMQ 服务器。 下图展示了生成者将消息存入 RabbitMQ Broker,以及消费者从Broker 中消费数据的整个流程 3. Exchange Types(交换器类型) RabbitMQ常用的 Exchange Type 有 fanout、direct、topic、headers 这四种（AMQP规范里还提到两种 Exchange Type，分别为 system 与 自定义，这里不予以描述） fanout fanout 类型的Exchange 路由规则非常简单，他会把所有发送到该Exchange的消息路由到所有与他绑定的Queue 中，不需要做任何判断操作，所以fanout 类型是所有的交换机类型里面速度最快的，fanout 类型常用来广播消息 direct direct类型的Exchange 路由规则也很简单，他会把消息路由到那些Bindingkey 与 RoutingKey 完全匹配的 Queue中。 以上图呀为例，如果发送消息的时候设置路由键为“wraning”，那么消息会路由到Queue1 和 Queue2 。如果在发送消息的时候，设置路由键为info或者“debug”，那么消息只会路由到Queue2.如果以其他的路由键发送消息，则消息不会路由到这两个队列中。 direct 类型常用在处理有优先级的任务，根据任务的优先级吧消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列 topic 前面讲到direct类型的交换器路由规则是完全匹配 BindingKey 和 RoutingKey ，但是这种严格的匹配方式在很多情况下不能满足实际业务的需求。topic类型的交换器在匹配规则上进行了扩展，它与 direct 类型的交换器相似，也是将消息路由到 BindingKey 和 RoutingKey 相匹配的队列中，但这里的匹配规则有些不同，它约定： RoutingKey 为一个点号“．”分隔的字符串（被点号“．”分隔开的每一段独立的字符串称为一个单词），如 “com.rabbitmq.client”、“java.util.concurrent”、“com.hidden.client”; BindingKey 和 RoutingKey 一样也是点号“．”分隔的字符串； BindingKey 中可以存在两种特殊字符串“”和“#”，用于做模糊匹配，其中“”用于匹配一个单词，“#”用于匹配多个单词(可以是零个)。 ​ 以上图为例： 路由键为 “com.rabbitmq.client” 的消息会同时路由到 Queuel 和 Queue2; 路由键为 “com.hidden.client” 的消息只会路由到 Queue2 中； 路由键为 “com.hidden.demo” 的消息只会路由到 Queue2 中； 路由键为 “java.rabbitmq.demo” 的消息只会路由到Queuel中； 路由键为 “java.util.concurrent” 的消息将会被丢弃或者返回给生产者（需要设置 mandatory 参数），因为它没有匹配任何路由键。 headers（不推荐） headers 类型的交换器不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。在绑定队列和交换器时制定一组键值对，当发送消息到交换器时，RabbitMQ会获取到该消息的 headers（也是一个键值对的形式)'对比其中的键值对是否完全匹配队列和交换器绑定时指定的键值对，如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers 类型的交换器性能会很差，而且也不实用，基本上不会看到它的存在。 "},"mq/RabbitMQ/action/RabbitMQ安装.html":{"url":"mq/RabbitMQ/action/RabbitMQ安装.html","title":"RabbitMQ安装","keywords":"","body":"RabbitMQ安装1. 安装erlang1.1 下载erlang 安装包1.2 解压erlang 安装包1.3 删除erlang 安装包1.4 安装erlang 的依赖工具1.5 进入 erlang 安装包解压文件对erlang 进行安装环境的配置1.6 编译安装1.7 验证erlang 是否安装成功1.8 配置 erlang 环境变量2. 安装 RabbitMQ2.1 下载 rpm2.2 安装rpm2.3 开启 web 管理插件2.4 设置开机启动2.5 启动服务2.6 查看服务状态2.7 访问RabbitMQ 控制台RabbitMQ安装 1. 安装erlang 1.1 下载erlang 安装包 在官网下载然后上传到 Linux 上或者直接使用下面的命令下载对应的版本。 wget http://erlang.org/download/otp_src_19.3.tar.gz 1.2 解压erlang 安装包 tar -xvzf otp_src_19.3.tar.gz 1.3 删除erlang 安装包 rm -rf otp_src_19.3.tar.gz 1.4 安装erlang 的依赖工具 yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel 1.5 进入 erlang 安装包解压文件对erlang 进行安装环境的配置 在/usr/local新建一个文件夹 mkdir erlang 对erlang 进行安装环境配置 ./configure --prefix=/usr/local/erlang --without-javac 1.6 编译安装 make && make install 1.7 验证erlang 是否安装成功 ./bin/erl 运行下面的语句输出“hello world” io:format(\"hello world~n\", []). 到此就安装完毕 1.8 配置 erlang 环境变量 vim /etc/profile 追加下列环境变量到文件末尾 #erlang ERL_HOME=/usr/local/erlang PATH=$ERL_HOME/bin:$PATH export ERL_HOME PATH 运行下列命令使配置文件profile生效 source /etc/profile 输入 erl 查看 erlang 环境变量是否配置正确 2. 安装 RabbitMQ 2.1 下载 rpm wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.8/rabbitmq-server-3.6.8-1.el7.noarch.rpm 2.2 安装rpm rpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc 紧接着执行 yum install rabbitmq-server-3.6.8-1.el7.noarch.rpm 2.3 开启 web 管理插件 rabbitmq-plugins enable rabbitmq_management 2.4 设置开机启动 chkconfig rabbitmq-server on 2.5 启动服务 service rabbitmq-server start 2.6 查看服务状态 service rabbitmq-server status 2.7 访问RabbitMQ 控制台 浏览器访问：http://你的ip地址:15672/ 默认用户名和密码： guest/guest;但是需要注意的是：guestuest用户只是被容许从localhost访问。官网文档描述如下： “guest” user can only connect via localhost 解决远程访问 RabbitMQ 远程访问密码错误 新建用户并授权 rabbitmqctl add_user root root rabbitmqctl set_user_tags root administrator rabbitmqctl set_permissions -p / root \".*\" \".*\" \".*\" 再次访问:http://你的ip地址:15672/ ,输入用户名和密码：root root "},"mq/RabbitMQ/action/SpringBoot整合RabbitMQ.html":{"url":"mq/RabbitMQ/action/SpringBoot整合RabbitMQ.html","title":"SpringBoot整合RabbitMQ","keywords":"","body":"SpringBoot整合RabbitMQ1. 简介2. Spring Boot 集成 RabbitMQ2.1 简单使用参考文章SpringBoot整合RabbitMQ 1. 简介 主要作用：解耦 最标准的用法： 生产者生产消息队列 消费者从队列中拿取消息并处理 生产者不用关系是谁来消费，消费者不用关心谁在生产消息，从而达到解耦的目的 分布式系统中的应用：分布式事务的支持，RPC的调用等等 2. Spring Boot 集成 RabbitMQ Spring Boot 集成 RabbitMQ 非常简单，如果只是简单的使用配置非常少，Spring Boot 提供了spring-boot-starter-amqp 项目对消息各种支持。 2.1 简单使用 1、配置 Pom 包，主要是添加 spring-boot-starter-amqp 的支持 org.springframework.boot spring-boot-starter-amqp 2、配置文件 配置 RabbitMQ 的安装地址、端口以及账户信息 spring.application.name=Spring-boot-rabbitmq spring.rabbitmq.host=120.79.200.111 spring.rabbitmq.port=5672 spring.rabbitmq.username=febs spring.rabbitmq.password=123456 3、队列配置 @Configuration public class RabbitConfig { @Bean public Queue Queue() { return new Queue(\"hello\"); } } 4、发送者 rabbitTemplate 是 Spring Boot 提供的默认实现 @component public class HelloSender { @Autowired private AmqpTemplate rabbitTemplate; public void send() { String context = \"hello \" + new Date(); System.out.println(\"Sender : \" + context); this.rabbitTemplate.convertAndSend(\"hello\", context); } } 5、接收者 @Component @RabbitListener(queues = \"hello\") public class HelloReceiver { @RabbitHandler public void process(String hello) { System.out.println(\"Receiver : \" + hello); } } 6、测试 @RunWith(SpringRunner.class) @SpringBootTest public class RabbitMqHelloTest { @Autowired private HelloSender helloSender; @Test public void hello() throws Exception { helloSender.send(); } } 注意，发送者和接收者的 queue name 必须一致，不然不能接收 多对多参考以下文章 参考文章 Spring Boot(八)：RabbitMQ 详解 "},"microservice/ServiceDiscovery/eureka/eureka基础.html":{"url":"microservice/ServiceDiscovery/eureka/eureka基础.html","title":"Eureka基础","keywords":"","body":"eureka基础1. 简介2. 背景2.1 服务中心2.2 引入服务中心的变化3. Eureka3.1 官方介绍3.2 介绍3.3 Eureka服务器和Eureka客户端3.4 Eureka 基本架构参考文章eureka基础 1. 简介 Eureka 是 Netfix 开源的一款提供服务注册和发现的产品，他提供了完整的 Service Register 和Service Discovery实现。是SpringCloud 体系中最重要最核心的组件之一 2. 背景 2.1 服务中心 服务中心又称注册中心，管理各种服务功能包括服务的注册、发现、熔断、负载、降级等。 2.2 引入服务中心的变化 情况1：项目A 调用项目B 正常调用项目A 请求项目B 有了服务中心之后，任何一个服务都不能直接去调用，都需要服务中心来调用 情况2：项目A 调用项目B,项目B 再调用项目C 这时候调用的步骤就会分为两步 第一步：项目A首先从服务中心请求项目B服务器 第二步：项目B 再从服务中心请求项目C 服务 上面的项目只是两三个相互之间的简单调用，但是如果项目超过20个30个呢，画一张图来描述几十个项目之间的相互调用关系全是线条。任何其中一个项目改动，就会牵连好几个项目跟着重启，麻烦且容易出错 通过服务中心来获取服务，你不需要关系调用的项目的IP 地址，由几台服务器组成，每次直接去服务中心获取可以使用的服务调用就可以 3. Eureka 3.1 官方介绍 Eureka 是一个基于 REST 的服务，主要在AWS 云中使用，定位服务来进行中间层服务器的负载均衡和故障转移 3.2 介绍 Spring Cloud 封装了Netfix 公司开发的 Eureka 模块来实现服务注册与发现。 Eureka 采用了C-S 的设计架构， Eureka Server 作为服务注册功能的服务器，他是服务注册中心。 而系统中的其他微服务，使用Eureka 的客户端连接到Eureka Server ，并维持心跳连接 优势： 系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行 Spring Cloud 的一些其他模块（Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关逻辑 3.3 Eureka服务器和Eureka客户端 Eureka 服务器：服务注册服务器 Eureka 客户端：是一个java 客户端，用来简化与服务器的交互，作为轮询负载均衡器，并提供服务的故障切换支持。 3.4 Eureka 基本架构 上图简要描述了Eureka 的基本架构，由3个角色组成 Eureka Server 提供服务注册和发现 Service Provider 服务提供方 将自身服务注册到Eureka，从而使服务消费放能够找到 Service coNSUMER 服务消费放 从Eureka获取注册服务列表，从而能够消费服务 参考文章 springcloud(二)：注册中心Eureka "},"microservice/ServiceDiscovery/eureka/SpringBoot整合Eureka.html":{"url":"microservice/ServiceDiscovery/eureka/SpringBoot整合Eureka.html","title":"SpringBoot整合Eureka","keywords":"","body":"SpringBoot整合Eureka1. 创建一个SpringBoot 项目2. 添加POM 依赖3. 在启动类上使用@EnableEurekaServer 注解4. 编写配置文件SpringBoot整合Eureka 微服务注册中心的作用就是用于统一管理微服务实例，微服务间的调用只需要知道对方的服务名，而无需关注具体的IP 和端口，便于微服务架构的拓展和维护。 1. 创建一个SpringBoot 项目 2. 添加POM 依赖 org.springframework.cloud spring-cloud-starter-netflix-eureka-server 3. 在启动类上使用@EnableEurekaServer 注解 在启动类上使用@EnableEurekaServer 注解，用以开启Eureka服务端功能： @EnableEurekaServer @SpringBootApplication public class RegisterApplication { public static void main(String[] args) { SpringApplication.run(FebsRegisterApplication.class, args); } } 4. 编写配置文件 server: port: 8001 servlet: context-path: /register spring: application: name: Register eureka: instance: hostname: localhost client: register-with-eureka: false fetch-registry: false instance-info-replication-interval-seconds: 30 serviceUrl: defaultZone: http://${eureka.instance.hostname}:${server.port}${server.servlet.context-path}/eureka/ spring.application.name，定义服务名称为Register; eureka.instance.hostname，指定了Eureka服务端的地址，因为我们是在本地搭建的，所以填写为localhost即可； eureka.client.register-with-eureka，表示是否将服务注册到Eureka服务端，由于我们这里是单节点的Eureka服务端，所以这里指定false； eureka.client.fetch-registry，表示是否从Eureka服务端获取服务信息，因为这里是单节点的Eureka服务端，并不需要从别的Eureka服务端同步服务信息，所以这里设置为false； eureka.client.instance-info-replication-interval-seconds，微服务更新实例信息的变化到Eureka服务端的间隔时间，单位为秒，这里指定为30秒（这就是微服务启动后，要过一会才能注册到Eureka服务端的原因）。 eureka.client.serviceUrl.defaultZone，指定Eureka服务端的地址，这里为当前项目地址，即 http://localhost:8001/register/eureka/ "},"microservice/ServiceDiscovery/nacos/nacos概念.html":{"url":"microservice/ServiceDiscovery/nacos/nacos概念.html","title":"nacos基础","keywords":"","body":"nacos概念1. 是什么2. 可以做什么3. 特性参考文章nacos概念 1. 是什么 Nacos 是构建以“服务” 为中心的现代应用架构（例如微服务范式，云原生范式）的服务基础设施。 2. 可以做什么 动态配置服务 支持以中心化，外部化和动态化的方式管理所有环境的配置。 动态配置消除了配置变更时重新部署和服务的需要 配置中心化管理让实现无状态服务更简单，也让按需弹性扩展服务更容易 服务发现及管理 支持DNS-Based和RPC-Based(Dubbo、gRPC)模式的服务发现 同时提供实时健康检查，以防止请求发往不健康的主机或服务实例 借助Nacos，可以更容易地为服务实现断路器 动态DNS服务 通过支持权重路由，轻松实现中间层负载均衡、更灵活的路由策略、流量控制及简单数据中心内网的简单DNS解析服务 更加容易地实现以DNS 协议为基础的服务发现，以消除耦合到厂商私有服务发现API 上的风险 3. 特性 易于使用 更适应云架构 生成等级 丰富的应用场景 参考文章 Nacos系列：欢迎来到Nacos的世界！ "},"microservice/ServiceDiscovery/nacos/nacos安装与使用.html":{"url":"microservice/ServiceDiscovery/nacos/nacos安装与使用.html","title":"nacos安装与使用","keywords":"","body":"nacos安装与使用1. 下载最新版的Nacos2.解压3. 修改配置（可选）4. 新建数据库5. 启动6.浏览7. 关闭服务nacos安装与使用 1. 下载最新版的Nacos 官方下载地址 2.解压 解压后的bin 目录下就是启动和关闭脚本 conf下为nacos的配置文件 target目录下为nacos的fat jar 3. 修改配置（可选） 修改conf/application.properties配置文件 例如：将应用端口改为8001 server.port=8001 4. 新建数据库 使用Navicat新建一个MySQL数据库，名字为febs_nacos： 然后导入febs_nacos.sql SQL脚本，导入后，数据库表如下图所示: 接着修改nacos解压包目录下conf/application.properties配置文件，添加如下配置： spring.datasource.platform=mysql db.num=1 db.url.0=jdbc:mysql://localhost:3306/febs_nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true db.user=root db.password=123456 spring.datasource.platform指定数据库的类型，这里为mysql，db.num指定数据库的个数（nacos支持多数据源，比如MySQL主从），剩下的配置为数据库配置。 5. 启动 Linux 单机启动 sh startup.sh -m standalone Windowns 修改好application.properties配置后，双击bin/startup.cmd启动nacos服务端 6.浏览 启动后使用浏览器访问：http://localhost:8001/nacos 默认账号:nacos 默认密码:nacos 7. 关闭服务 sh shutdown.sh "},"microservice/gateway/zuul/zuul基础.html":{"url":"microservice/gateway/zuul/zuul基础.html","title":"zuul基础","keywords":"","body":"zuul基础1. 为什么需要API Gateway2. Zuul参考文章zuul基础 在微服务中，外部的应用如何来访问内部各种各样的微服务的呢？在微服务架构中，后端服务往往不直接开放给调用端，而是通过一个API 网关根据请求的url，路由到相应的服务。当添加API网关后，在第三方调用端和服务提供方之间就创建了一面墙，这面墙直接与调用方通信进行权限控制，后将请求均衡分发给后台服务端 1. 为什么需要API Gateway 简化客户端调用复杂度 在微服务架构模式下后端服务的实例数一般是动态的，对于客户端而言很难发现动态改变的服务的访问地址信息。因此在基于微服务的项目中为了简化前端的调用逻辑，通常会引入API Gateway 作为轻量级网关，同时API Gateway 中也会实现相关的认证逻辑从而简化内部服务之间相互调用的复杂度 数据裁剪以及聚合 通常而言不同的客户端对于显示时对于数据的需求是不一致的，比如手机端或者Web端又或者在低延迟的网络环境或者高延迟的网络环境 因此为了优化客户端的使用体验，API Gateway 可以对通用性的响应数据进行裁剪以适应不同客户端的使用需求。同时还将可以将API 调用逻辑进行聚合，从而减少客户端的请求数，优化客户端的用户体验 多渠道的支持 当然我们还可以针对不同的渠道和客户端提供不同的API Gateway，对于该模式的使用由另外一个大家熟知的方式叫Backend for front-end, 在Backend for front-end模式当中，我们可以针对不同的客户端分别创建其BFF 遗留系统的微服务化改造 对于系统而言进行微服务改造通常是由于原有的系统存在或多或少的问题，比如技术债务，代码质量，可维护性，可扩展性等等，API Gateway 的模式同样适用于这一类遗留系统的改造，通过微服务化的改造逐步实现对原有系统中的问题修复，从而提升对原有业务响应力的提升。通过引入抽象层，逐步使用新的实现替换旧的实现 2. Zuul zuul 提供负载均衡，反向代理，权限认证的一个API Gateway。 同时还提供动态路由，监控，弹性、安全等服务 参考文章 springcloud(十)：服务网关zuul初级篇 "},"microservice/gateway/zuul/zuul实战.html":{"url":"microservice/gateway/zuul/zuul实战.html","title":"zuul实战","keywords":"","body":"zuul实战1. 简单集成使用1.1 添加依赖1.2. 配置文件1.3 启动类1.4 测试2. zuul配置zuul实战 1. 简单集成使用 1.1 添加依赖 ​ 引入zuul org.springframework.cloud spring-cloud-starter-netflix-zuul 1.2. 配置文件 spring.application.name=gateway-service-zuul server.port=8888 #这里的配置表示，访问/it/** 直接重定向到http://java.isture.com/ zuul.routes.baidu.path=/it/** zuul.routes.baidu.url=http://java.isture.com/ 1.3 启动类 启动类添加@EnableZuulProxy，支持网关路由。 @EnableZuulProxy @SpringBootApplication public class ZuuldemoApplication { public static void main(String[] args) { SpringApplication.run(ZuuldemoApplication.class, args); } } 1.4 测试 启动项目，在浏览器中访问http://localhost:8888/it/spring-cloud，看到页面返回http://java.isture.com/ 2. zuul配置 zuul: routes: auth: path: /auth/** serviceId: ylzap-Auth sensitiveHeaders: \"*\" retryable: true ignored-services: \"*\" ribbon: eager-load: enabled: true ribbon: ReadTimeout: 3000 其中 ... auth: path: /auth/** serviceId: ylzap-Auth sensitiveHeaders: \"*\" 这一段的意思是所有以/auth 开头的请求都会被转发到名称为ylzap-auth 的服务器，由于我们需要在请求头中携带令牌，所以sensitiveHeaders设置为*，表示不过滤请求头信息，既请求的请求头信息将原封不动的转发出去，此外，因为zuul已经包含了ribbon和hystrix依赖，所以我们在使用zuul 的同时，可以添加ribbon和hystrix相关配置 上述配置剩下的含义如下： zuul.retryable: 设置true，表示开启重试机制 zuul.ignored-services: zull 配合Eureka 后会有一套默认的配置规则，这里我们只想请求根据我们显示配置的路由规则走，所以设置为*，表示关闭所有默认路由配置规则 zuul.ribbon.eager-load.enabled，Zuul内部通过Ribbon按照一定的负载均衡算法来获取服务，Ribbon进行客户端负载均衡的Client并不是在服务启动的时候就初始化好的，而是在调用的时候才会去创建相应的Client，所以第一次调用的耗时不仅仅包含发送HTTP请求的时间，还包含了创建RibbonClient的时间，这样一来如果创建时间速度较慢，同时设置的超时时间又比较短的话，第一次请求很容易出现超时的情况。设置为true的时候表示开启Ribbon的饥饿加载模式，即在应用启动的时候就去获取相应的Client备用。 ribbon.ReadTimeout，设置请求超时时间，单位为毫秒； "},"microservice/gateway/zuul/微服务保护.html":{"url":"microservice/gateway/zuul/微服务保护.html","title":"微服务保护","keywords":"","body":"微服务保护1. 微服务保护设计思路2. zuul的4种核心过滤器3. 设计微服务保护3.1 自定义zuul过滤器3.2 校验Zuul Token微服务保护 我们设计所有客户端请求都是通过微服务网关转发完成的，虽然我们约定如此，但是还是可以直接通过访问微服务地址的方式来获取服务 为了解决这个问题，我们可以自定义Zuul过滤器 1. 微服务保护设计思路 在网关转发请求前，请求头部加入网关信息，然后在处理请求的微服务模块里定义全局拦截器，校验请求头部的网关信息，这样就能避免客户端直接访问微服务 2. zuul的4种核心过滤器 这4种过滤器处于不同的生命周期，所以其职责也各不相同 PRE: PRE过滤器用于将请求路径与配置的路由规则进行匹配，以找到需要转发的目标地址，并做一些前置加工，比如请求的校验等 ROUTING: ROUTING 过滤器用于将外部请求转发到具体服务实例上去 POST: POST 过滤器用于将微服务的响应信息返回到客户端，这个过程可以对返回数据进行加工处理 ERROR: 上述的过程发生异常后将调用ERROR 过滤器。ERROR 过滤器捕获到异常后需要将异常信息返回给客户端，所以最终还是会调用POST过滤器 Spring Cloud Zuul为各个生命周期阶段实现了一批过滤器，如下所示： 这些过滤器的优先级和作用如下表所示： 生命周期 优先级 过滤器 描述 pre -3 ServletDetectionFilter 标记处理Servlet的类型 pre -2 Servlet30WrapperFilter 包装HttpServletRequest请求 pre -1 FormBodyWrapperFilter 包装请求体 route 1 DebugFilter 标记调试标志 route 5 PreDecorationFilter 处理请求上下文供后续使用 route 10 RibbonRoutingFilte serviceId请求转发 route 10 SimpleHostRoutingFilter url请求转发 route 50 SendForwardFilter forward请求转发 post 0 SendErrorFilter 处理有错误的请求响应 post 10 SendResponseFilter 处理正常的请求响应 从上面的表格可以看到，PreDecorationFilter用于处理请求上下文，优先级为5，所以我们可以定义一个优先级在PreDecorationFilter之后的过滤器，这样便可以拿到请求上下文。 3. 设计微服务保护 3.1 自定义zuul过滤器 @Slf4j @Component public class GatewayRequestFilter extends ZuulFilter { @Override public String filterType() { return FilterConstants.PRE_TYPE; } @Override public int filterOrder() { return 6; } @Override public boolean shouldFilter() { return true; } @Override public Object run() { RequestContext ctx = RequestContext.getCurrentContext(); String serviceId = (String) ctx.get(FilterConstants.SERVICE_ID_KEY); HttpServletRequest request = ctx.getRequest(); String host = request.getRemoteHost(); String method = request.getMethod(); String uri = request.getRequestURI(); log.info(\"请求URI：{}，HTTP Method：{}，请求IP：{}，ServerId：{}\", uri, method, host, serviceId); byte[] token = Base64Utils.encode((\"febs:zuul:123456\").getBytes()); ctx.addZuulRequestHeader(\"ZuulToken\", new String(token)); return null; } } 自定义Zuul过滤器需要继承ZuulFilter，并实现它的四个抽象方法： filterType，对应Zuul生命周期的四个阶段：pre、post、route和error，我们要在请求转发出去前添加请求头，所以这里指定为pre； filterOrder，过滤器的优先级，数字越小，优先级越高。PreDecorationFilter过滤器的优先级为5，所以我们可以指定为6让我们的过滤器优先级比它低； shouldFilter，方法返回boolean类型，true时表示是否执行该过滤器的run方法，false则表示不执行； run，定义过滤器的主要逻辑。这里我们通过请求上下文RequestContext获取了转发的服务名称serviceId和请求对象HttpServletRequest，并打印请求日志。随后往请求上下文的头部添加了Key为ZuulToken，Value为febs:zuul:123456的信息。这两个值可以抽取到常量类中。 定义好Zuul过滤器后，我们需要在各个微服务里定义一个全局拦截器拦截请求，并校验Zuul Token。 3.2 校验Zuul Token public class FebsServerProtectInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws IOException { // 从请求头中获取 Zuul Token String token = request.getHeader(FebsConstant.ZUUL_TOKEN_HEADER); String zuulToken = new String(Base64Utils.encode(FebsConstant.ZUUL_TOKEN_VALUE.getBytes())); // 校验 Zuul Token的正确性 if (StringUtils.equals(zuulToken, token)) { return true; } else { FebsResponse febsResponse = new FebsResponse(); response.setContentType(MediaType.APPLICATION_JSON_UTF8_VALUE); response.setStatus(HttpServletResponse.SC_FORBIDDEN); response.getWriter().write(JSONObject.toJSONString(febsResponse.message(\"请通过网关获取资源\"))); return false; } } FebsServerProtectInterceptor实现了HandlerInterceptor的preHandle方法，该拦截器可以拦截所有Web请求。在preHandle方法中，我们通过HttpServletRequest获取请求头中的Zuul Token，并校验其正确性，当校验不通过的时候返回403错误。 "},"microservice/gateway/SpringCloudGateWay/服务网关SpringCloudGateWay基础.html":{"url":"microservice/gateway/SpringCloudGateWay/服务网关SpringCloudGateWay基础.html","title":"服务网关Spring Cloud GateWay基础","keywords":"","body":"服务网关Spring Cloud GateWay基础1. 与zuul（1.x）优势2. 背景3.相关概念4. 工作流程5. Spring Cloud Gateway 的特征参考文章服务网关Spring Cloud GateWay基础 1. 与zuul（1.x）优势 zuul（1.0）基于Servlet,使用阻塞API，且不支持长连接，如WebSockets 优势： 使用非阻塞 API 支持 WebSockets 支持限流等新特性 2. 背景 Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 Spring Cloud Gateway 作为Spring Cloud 生态系统中的网关，目标是替代Netflix Zuul。其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。 3.相关概念 Route（路由）：这是网关的基本构建模块。它由一个ID，一个目标URL，一组断言和一组过滤器定义组成。如果断言为真，则路由匹配 Predicate（断言）：这是一个 Java8 的 Predicate。输入类型是ServerWebExchange。我们可以使用他来匹配来自Http请求的任何内容。例如headers或参数 Filter(过滤器)：这是org.springframework.cloud.gateway.filter.GatewayFilter的实例，我们可以使用它修改请求和响应。 4. 工作流程 客户端向 Spring Cloud Gateway 发出请求。 如果Gateway handler Mapping 中找到与请求相匹配的路由， 将其发送到Gateway Web Handler。 Handler 再通过指定的过滤器链来将请求发送到我们实际的服务执行业务逻辑 过滤器之间用虚线分开是因为过滤器可能会发送代理请求之前（“pre”）或之后（“post”）执行业务逻辑。 5. Spring Cloud Gateway 的特征 基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0 动态路由 Predicates 和 Filters 作用于特定路由 集成 Hystrix 断路器 集成 Spring Cloud DiscoveryClient 易于编写的 Predicates 和 Filters 限流 路径重写 参考文章 springcloud(十五)：服务网关 Spring Cloud GateWay 入门 "},"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay网关功能.html":{"url":"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay网关功能.html","title":"Spring Cloud GateWay网关功能","keywords":"","body":"Spring Cloud GateWay网关功能动态路由负载均衡认证授权跨域设置日志失败重试参考文章Spring Cloud GateWay网关功能 微服务网关最重要的几个功能分别是路由, 负载均衡,认证授权, 跨域配置, 日志, 失败重试, 下来一一来配置. 动态路由 通常微服务都是注册中心, 服务都是自动发现的,spring cloud gateway可以基于注册小心动态配置路由, 转发的规则是: /service_id/path->转发到service_id对应的服务 只需要添加配置: spring.cloud.gateway.discovery.locator.enabled=true spring.cloud.gateway.discovery.locator.lowerCaseServiceId=true 就能动态路由. 负载均衡 以lb://开头的请求, 会被全局过滤器RetryLoadBalancerClientFilter拦截并进行负载均衡处理, 所有的动态路由都会自动负载均衡. 认证授权 有一个专门用来做认证授权的服务, 网关需要做的就是自定义一个全局过滤器, 将每一个请求发到授权服务进行认证授权. @Component public class AuthFilter implements GlobalFilter, Ordered { @Override public int getOrder() { return Constants.PRE_FILTER_ORDER_AUTH; } @Override public Mono filter(ServerWebExchange exchange, GatewayFilterChain chain) { ServerHttpRequest request = exchange.getRequest(); try { //TODO 调用授权服务进行认证和授权 return chain.filter(exchange.mutate().request(request).build()); } catch (Exception e) { log.error(\"auth failed: \" + e.getMessage()); return exchange.getResponse() .writeWith(Flux.just(exchange.getResponse().bufferFactory().wrap(\"授权失败\".getBytes()))); } } 跨域设置 有时候前端需要支持跨域访问, 这里简单配置允许所有域名访问. spring.cloud.gateway.globalcors.cors-configurations.[/**].allowed-origins=* spring.cloud.gateway.globalcors.cors-configurations.[/**].allowed-methods=* spring.cloud.gateway.globalcors.cors-configurations.[/**].allowed-headers=* spring.cloud.gateway.globalcors.cors-configurations.[/**].allow-credentials=true 日志 日志功能同样是自定义全局过滤器实现, 在请求进入时打印输入日志,返回时打印输出日志, 唯一的问题是在打印请求或返回的body时要处理下,一般情况下请求和响应的body都不能多次读, 需要自定义装饰器封装实现多次读的功能. 失败重试 这个没有全局过滤器, 需要自己实现. 参考文章 微服务网关spring cloud gateway "},"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay实战.html":{"url":"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay实战.html","title":"Spring Cloud GateWay实战","keywords":"","body":"Spring Cloud GateWay实战1. 网关路由有两种配置方式2. POM依赖3. 测试转发请求Spring Cloud GateWay实战 1. 网关路由有两种配置方式 在配置文件 yml 中配置 通过 @Bean 自定义 RouteLocator，在启动主类 Application 中配置 这两种方式是等价的，建议使用 yml 方式进配置。 2. POM依赖 org.springframework.cloud spring-cloud-starter-gateway Spring Cloud Gateway 是使用 netty+webflux 实现因此不需要再引入 web 模块。 3. 测试转发请求 server: port: 8080 spring: cloud: gateway: routes: - id: neo_route uri: http://www.ityouknow.com predicates: - Path=/spring-cloud 各字段含义如下： id：我们自定义路由ID，保持唯一 uri：目标服务地址 predicates：路由条件，接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将predicates 组合成其他复杂的逻辑（比如：与，或，非）。 filters：过滤规则，本示例暂时没用 上面这段配置的意思是，配置了一个id为 neo_route 的路由规则，当访问地址 http://localhost:8080/spring-cloud时会自动转发到地址：http://www.ityouknow.com/spring-cloud。配置完成启动项目即可在浏览器访问进行测试，当我们访问地址http://localhost:8080/spring-cloud 时会展示页面展示如下 证明页面转发成功。 转发功能同样可以通过代码来实现，我们可以在启动类 GateWayApplication 中添加方法 customRouteLocator() 来定制转发规则。 @SpringBootApplication public class GateWayApplication { public static void main(String[] args) { SpringApplication.run(GateWayApplication.class, args); } @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder builder) { return builder.routes() .route(\"path_route\", r -> r.path(\"/about\") .uri(\"http://ityouknow.com\")) .build(); } } 以上只是一个简单实现，更多路由规则 "},"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay之Filter.html":{"url":"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay之Filter.html","title":"Spring Cloud GateWay之Filter","keywords":"","body":"Spring Cloud GateWay之Filter1. Filter的作用1.1 Filter的作用实例2. 生命周期3. 作用范围4. 自定义GlobalFilter参考文章Spring Cloud GateWay之Filter 1. Filter的作用 “pre”类型的过滤器：参数校验、权限校验、流量监控、日志输出、协议转换等 “post”类型的过滤器：响应内容、响应头的修改，日志的输出，流量监控等 1.1 Filter的作用实例 当我们有很多服务时，比如下图中的user-service、goods-service、sales-service等服务，客户端请求各个服务的Api时，每个服务都需要做相同的事情，比如鉴权、限流、日志输出等。 对于这样重复工作，有没有办法做得更好？我们可以在微服务的上一层加一个全局的权限控制，限流、日志输出的Api Gateway服务。然后再将请求转发到具体的业务服务层。这个Api Gateway服务就是起到一个服务边界的作用，外界的请求访问系统，必须先通过网关层。 2. 生命周期 Spring Cloud Gateway同zuul类似，有“pre”和“post”两种方式的filter。客户端的请求先经过“pre”类型的filter，然后将请求转发到具体的业务服务，比如上图中的user-service，收到业务服务的响应之后，再经过“post”类型的filter处理，最后返回响应到客户端。 3. 作用范围 gateway filter： 针对单个路由 需要通过spring.cloud.routes.filters 配置在具体路由下，只作用在当前路由上或通过spring.cloud.default-filters配置在全局，作用在所有路由上 global gateway filer 针对所有路由 全局过滤器，不需要在配置文件中配置，作用在所有的路由上，最终通过GatewayFilterAdapter包装成GatewayFilterChain可识别的过滤器，它为请求业务以及路由的URI转换为真实业务服务的请求地址的核心过滤器，不需要配置，系统初始化时加载，并作用在每个路由上。 4. 自定义GlobalFilter 自定义filter 需要实现GlobalFilter, Ordered 例如：该GlobalFilter会校验请求中是否包含了请求参数“token”，如何不包含请求参数“token”则不转发路由，否则执行正常的逻辑。代码如下： public class TokenFilter implements GlobalFilter, Ordered { Logger logger=LoggerFactory.getLogger( TokenFilter.class ); @Override public Mono filter(ServerWebExchange exchange, GatewayFilterChain chain) { String token = exchange.getRequest().getQueryParams().getFirst(\"token\"); if (token == null || token.isEmpty()) { logger.info( \"token is empty...\" ); exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); return exchange.getResponse().setComplete(); } return chain.filter(exchange); } @Override public int getOrder() { return -100; } } 参考文章 spring cloud gateway之filter篇 "},"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay负载均衡.html":{"url":"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay负载均衡.html","title":"Spring Cloud GateWay负载均衡","keywords":"","body":"Spring Cloud GateWay负载均衡Spring Cloud GateWay负载均衡 以lb://开头的请求, 会被全局过滤器RetryLoadBalancerClientFilter拦截并进行负载均衡处理, 所有的动态路由都会自动负载均衡. "},"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay之Hystrix断路器.html":{"url":"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay之Hystrix断路器.html","title":"Spring Cloud GateWay之Hystrix断路器","keywords":"","body":"Spring Cloud GateWay之Hystrix断路器1. POM依赖2. application.yml 配置3. 服务降级处理Spring Cloud GateWay之Hystrix断路器 Hystrix 是Netflix实现的断路器模式工具包，The Hystrix GatewayFilter就是将断路器使用在gateway的路由上，目的是保护你的服务避免级联故障，以及在下游失败时可以降级返回。 1. POM依赖 org.springframework.cloud spring-cloud-starter-netflix-hystrix 2. application.yml 配置 spring: application: name: Chinahrss-Gateway gateway: routes: - id: Chinahrss-Auth uri: lb://Chinahrss-Auth predicates: - Path=/auth/** filters: - name: Hystrix args: name: authfallback fallbackUri: forward:/fallback/Chinahrss-Auth - id: Chinahrss-Server-System uri: lb://Chinahrss-Server-System predicates: - Path=/system/** filters: - name: Hystrix args: name: systemfallback fallbackUri: forward:/fallback/FEBS-Server-System 服务降级后，请求会跳转fallbackUri配置的路径，目前只支持forward:的URI协议。 3. 服务降级处理 此时服务降级后会跳转我们制定的页面 @RestController public class FallbackController { @RequestMapping(\"fallback/{name}\") @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) public Mono systemFallback(@PathVariable String name) { String response = String.format(\"访问%s超时或者服务不可用\", name); return Mono.just(new FebsResponse().message(response)); } } "},"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay跨域设置.html":{"url":"microservice/gateway/SpringCloudGateWay/SpringCloudGateWay跨域设置.html","title":"Spring Cloud GateWay跨域设置","keywords":"","body":"Spring Cloud GateWay跨域设置Spring Cloud GateWay跨域设置 有时候前端需要支持跨域访问, 这里简单配置允许所有域名访问. spring: application: name: Chinahrss-Gateway cloud: gateway: globalcors: corsConfigurations: '[/**]': allowedOrigins: \"*\" allowedMethods: \"*\" allowedHeaders: \"*\" allowCredentials: true "},"microservice/auth/spring-cloud-starter-oauth2/使用spring-cloud-starter-oauth2搭建授权服务.html":{"url":"microservice/auth/spring-cloud-starter-oauth2/使用spring-cloud-starter-oauth2搭建授权服务.html","title":"使用spring-cloud-starter-oauth2搭建授权服务","keywords":"","body":"使用spring-cloud-starter-oauth2搭建授权服务1.添加pom依赖2. 添加注解和配置3. 授权流程使用spring-cloud-starter-oauth2搭建授权服务 spring-cloud-starter-oauth2 是 Spring 对OAuth 的开源实现，可以无缝和Spirng cloud 集成。 1.添加pom依赖 授权服务是基于Spring Security的，因此需要在项目引入两个依赖 org.springframework.cloud spring-cloud-starter-security org.springframework.cloud spring-cloud-starter-oauth2 前者为 Security，后者为Security的OAuth2扩展。 2. 添加注解和配置 在启动类中添加在启动类或配置类中添加@EnableAuthorizationServer注解： @SpringBootApplication @EnableAuthorizationServer public class AlanOAuthApplication { public static void main(String[] args) { SpringApplication.run(AlanOAuthApplication.class, args); } } 完成这些我们的授权服务最基本的骨架就已经搭建完成了，但是要想跑通整个流程，我们必须分配client_id, client_secret才行。Spring Security Oauth2 的配置方法是编写@Configuration类继承AuthorizationServerConfigurerAdapter，然后重写void configure(ClientDetailsServiceConfigurer clients)方法，如： @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception { clients.inMemory() // 使用in-memory存储 .withClient(\"client\") // client_id .secret(\"secret\") // client_secret .authorizedGrantTypes(\"authorization_code\") // 该client允许的授权类型 .scopes(\"app\"); // 允许的授权范围 } 当然这里除了使用内存，还能使用数据库存储之类的，演示方便使用了内存 3. 授权流程 访问授权页面： localhost:8080/oauth/authorize?client_id=client&response_type=code&redirect_uri=http://www.baidu.com 此时浏览器会让你输入用户名密码，这是因为 Spring Security 在默认情况下会对所有URL添加Basic Auth认证。默认的用户名为user, 密码是随机生成的，在控制台日志中可以看到。 "},"microservice/auth/spring-cloud-starter-oauth2/SpringsecurityOAuth2深入解析.html":{"url":"microservice/auth/spring-cloud-starter-oauth2/SpringsecurityOAuth2深入解析.html","title":"Spring security OAuth2 深入解析","keywords":"","body":"Spring security OAuth2 深入解析1. OAuth概要1.1 OAuth 基本流程1.2 服务类型1.3 授权认证服务1.4 资源获取服务2. Spring Security OAuth2 的使用2.1 授权认证服务2.1.4 自定义错误处理（Error Handling）2.2 资源获取服务2.2.2 ResourceServerSecurityConfigurer的相关属性2.2.3 ResourceServerTokenServices参考文章Spring security OAuth2 深入解析 1. OAuth概要 1.1 OAuth 基本流程 基本组件 client: 第三方应用（既App或向外提供接口） Resource Owner: 资源所有者（既用户） Authentication Server: 授权认证服务（发配Access Token） Resource Server： 资源服务器（存储用户资源信息等组员） 微信和QQ 都是使用这种OAuth2的基本流程 第三方应用请求用户授权 用户同意授权，并发挥一个授权码（code） 第三方应用根据授权码（code）向授权认证服务进行授权； 授权服务器根据授权码（code）。校验通过，并返回给第三方应用令牌（Access Token）; 第三方应用根据令牌（Access Token）向资源服务请求相关资源 资源服务器验证令牌（Access Token），校验通过，并返回第三方所请求的资源 1.2 服务类型 OAuth 在服务提供者上可分为两类： 授权认证服务： AuthenticationServer @Configuration @EnableAuthorizationServer public class CustomAuthenticationServerConfig extends AuthorizationServerConfigurerAdapter 资源获取服务：ResourceServer @Configuration @EnableResourceServer public class CustomResourceServerConfig extends ResourceServerConfigurerAdapter 注：这两者有时候可能存在同一个应用程序中（既SOA架构）。在Spring OAuth 中可以简便的将其分配到两个应用中（既微服务），而且可多个资源获取共享一个授权认证服务器 1.3 授权认证服务 主要的操作： 获取第三方应用发送的授权码（code）以及第三方应用标识 根据授权码及标识进行校验 校验通过，发送令牌（Access Token） 分析： 1）第一步： 授权码（code）：第三方应用进行第一步“Authorization Request”时，请求参数redirect_uri 中的回调链接，服务会生成相关用户凭证，并在其回调链接上附带的code 第三方用户标识： client_id: 第三方用户的id（可理解为账号） client_secret: 第三方应用和授权服务器之间的安全凭证（可理解为密码） 注：其中client_id 和 client_secret 都是授权服务器发送给第三方应用的，如：微信等一系列授权，在其平台上注册。获取其appid和secret同样道理 既然是账号密码，总不能以get请求，不安全，因此，OAuth2要求请求必须是POST请求，同时，还必须是HTTPS服务，以此保证获取到的安全凭证（Access Token）的安全性 2）第二步： 授权认证服务器根据标识校验第三方应用的真实性 授权认证服务器根据授权码（code）进行校验用户凭证 3）第三步操作 生成Access Token （MD5类型，uuid类型，jwt类型） 1.4 资源获取服务 主要的操作 校验Access Token 发放资源信息 2. Spring Security OAuth2 的使用 2.1 授权认证服务 Spirng OAuth2中，我们配置一个授权认证服务，我们最主要有以下三点： 第三方用户客户端详情 -> Client 令牌的生成管理 -> Access Token 端点接入 -> endpoints Spring 中有三个配置与这三点一一对应： ClientDetailsServiceConfigurer: 用来配置客户端详情服务 AuthorizationServerSecurityConfigurer:用来配置令牌端点（Token EndPoint）的安全约束 AuthorizationServerEndpointsConfigurer: 配置授权（authorization）以及令牌（token）的访问端点和令牌服务（token services） 2.1.1 第三方用户客户端详情 除了上面说道的client_id 和 client_client_secret,还需要一些服务附带一些授权认证参数 1）Grant Type 其实OAuth2 不仅仅提供授权码（code）这种格式授权方式，还提供几个其他类型，其中用Grant Type 代表当前授权的类型。Grant Type包括 authorization_code:传统的授权码模式 Implicit:隐式授权模式 password：资源所有者（既用户）密码模式 client_credentials: 客户端凭证（客户端ID以及Key）模式 refresh_token:获取access token时附带的用于刷新新的token模式 2）scope 其实授权赋予第三方用户可以在资源服务器获取资源，经常就是调用Api请求附带令牌，然后调用api有增删改查等功能，而scopes的值就是all（全部权限），read，write等权限。就是第三方访问资源的一个权限，访问范围。 3）accessTokenValiditySeconds 还可以设置accessTokenValiditySeconds属性来设置Access Token 的存活时间 @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception { clients.inMemory() .withClient(\"catalpaFlat\") .secret(\"catalpaFlat-secret\") .accessTokenValiditySeconds(7200) .authorizedGrantTypes(\"refresh_token\",\"password\") .scopes(\"all\"); ｝ 2.1.2 令牌的生成和管理 AccessToken的存在意义： 创建AccessToken，并保存，以备后续请求访问都可以认证成功并获取到资源 AccessToken还有一个潜在功能，就是使用jwt生成token时候，可以用来加载一些信息，把一些相关权限等包含在AccessToken中 1） AuthorizationServerTokenServices AuthorizationServerTokenServices 提供了对AccessToken 的相关创建、刷新、获取 public interface AuthorizationServerTokenServices { OAuth2AccessToken createAccessToken(OAuth2Authentication authentication) throws AuthenticationException; OAuth2AccessToken refreshAccessToken(String refreshToken, TokenRequest tokenRequest) throws AuthenticationException; OAuth2AccessToken getAccessToken(OAuth2Authentication authentication); } 2) DefaultTokenServices AuthorizationServerTokenServices 竟然可以操作AccessToken，那么OAuth2就默认为我们提供了一个默认的DefaultTokenServices。包含了一些有用实现，可以使用他来修改令牌的格式和令牌的存储等，但是生成的token是随机数 3）TokenStore 创建AccessToken完之后，除了发放给第三方，肯定还得保存起来，才可以使用。因此TokenStore为我们完成这一操作，将令牌（AccessToken）保存和持久化。 okenStore也有一个默认的实现类InMemoryTokenStore，从名字就知道是通过保存到内存进而实现保存Access Token。 TokenStore的实现有多种类型，可以根据业务需求更改Access Token的保存类型： InMemoryTokenStore：这个是OAuth2默认采用的实现方式。在单服务上可以体现出很好特效（即并发量不大，并且它在失败的时候不会进行备份），大多项目都可以采用此方法。毕竟存在内存，而不是磁盘中，调试简易。 JdbcTokenStore：这个是基于JDBC的实现，令牌（Access Token）会保存到数据库。这个方式，可以在多个服务之间实现令牌共享。 JwtTokenStore：jwt全称 JSON Web Token。这个实现方式不用管如何进行存储（内存或磁盘），因为它可以把相关信息数据编码存放在令牌里。JwtTokenStore 不会保存任何数据，但是它在转换令牌值以及授权信息方面与 DefaultTokenServices 所扮演的角色是一样的。但有两个缺点： 撤销一个已经授权的令牌会很困难，因此只适用于处理一个生命周期较短的以及撤销刷新令牌。 令牌占用空间大，如果加入太多用户凭证信息，会存在传输冗余 4）JWT Token 想使用jwt令牌，需要在授权服务中配置JwtTokenStore。之前说了，jwt将一些信息数据编码后存放在令牌,那么其实在传输的时候是很不安全的，所以Spring OAuth2提供了JwtAccessTokenConverter来怼令牌进行编码和解码。适用JwtAccessTokenConverter可以自定义秘签（SigningKey）。SigningKey用处就是在授权认证服务器生成进行签名编码，在资源获取服务器根据SigningKey解码校验。 JwtAccessTokenConverter jwtAccessTokenConverter = new JwtAccessTokenConverter(); jwtAccessTokenConverter.setSigningKey(\"CatalpaFlat\") 2.1.3 端点接入-endpoints 授权认证是使用AuthorizationEndpoint这个端点来进行控制，一般使用AuthorizationServerEndpointsConfigurer 来进行配置。 @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception {} 1) 端点（endpoints）的相关配置 authenticationManager: 认证管理器。若我们上面的Grant Type设置为password，则需设置一个AuthenticationManager对象 userDetailsService：若是我们实现了UserDetailsService来管理用户信息，那么得设我们的userDetailsService对象 authorizationCodeServices：授权码服务。若我们上面的Grant Type设置为authorization_code，那么得设一个AuthorizationCodeServices对象 tokenStore：这个就是我们上面说到，把我们想要是实现的Access Token类型设置 accessTokenConverter：Access Token的编码器。也就是JwtAccessTokenConverter tokenEnhancer:token的拓展。当使用jwt时候，可以实现TokenEnhancer来进行jwt对包含信息的拓展 tokenGranter：当默认的Grant Type已经不够我们业务逻辑，实现TokenGranter 接口，授权将会由我们控制，并且忽略Grant Type的几个属性。 2) 端点（endpoints）的授权url：要授权认证，肯定得由url请求，才可以传输，因此OAuth2提供了配置授权端点的URL AuthorizationServerEndpointsConfigurer ，还是这个配置对象进行配置，其中由一个pathMapping()方法进行配置授权端点URL路径，默认提供了两个参数defaultPath和customPath： public AuthorizationServerEndpointsConfigurer pathMapping(String defaultPath, String customPath) { this.patternMap.put(defaultPath, customPath); return this; } pathMapping的defaultPath有： /oauth/authorize：授权端点 /oauth/token：获取授权token令牌 /oauth/confirm_access：用户确认授权提交端点 /oauth/error：授权服务错误信息端点 /oauth/check_token：用于资源服务访问的令牌解析端点 /oauth/token_key：提供公有密匙的端点，如果使用JWT令牌的话 注：pathMapping的两个参数都将以 \"/\" 字符为开始的字符串 2.1.4 自定义错误处理（Error Handling） 实际上我们上面说到的端点,其实可以看成Controller，用于返回不同端点的响应内容 授权服务的错误信息是使用标准的Spring MVC来进行处理的，也就是 @ExceptionHandler 注解的端点方法，我们可以提供一个 WebResponseExceptionTranslator 对象。最好的方式是改变响应的内容而不是直接进行渲染。 假如说在呈现令牌端点的时候发生了异常，那么异常委托了 HttpMessageConverters 对象（它能够被添加到MVC配置中）来进行输出。 假如说在呈现授权端点的时候未通过验证，则会被重定向到 /oauth/error 即错误信息端点中。whitelabel error （即Spring框架提供的一个默认错误页面）错误端点提供了HTML的响应，但是我们大概可能需要实现一个自定义错误页面（例如只是简单的增加一个 @Controller 映射到请求路径上 @RequestMapping(\"/oauth/error\")）。 2.2 资源获取服务 资源服务器，其实就是存放一些受令牌保护的资源，只有令牌并且有效正确才能获取到资源。内部是通过Spring OAuth2的Spring Security Authentication filter 的过滤链来进行保护。 2.2.1 ResourceServerConfigurerAdapter 我们可以继承ResourceServerConfigurerAdapter，来使用 ResourceServerSecurityConfigurer进行相关配置。 public class ResourceServerConfigurerAdapter implements ResourceServerConfigurer { @Override public void configure(ResourceServerSecurityConfigurer resources) throws Exception { } @Override public void configure(HttpSecurity http) throws Exception { http.authorizeRequests().anyRequest().authenticated(); } ｝ 2.2.2 ResourceServerSecurityConfigurer的相关属性 tokenServices：ResourceServerTokenServices 类的实例，用来实现令牌服务。 resourceId：这个资源服务的ID，这个属性是可选的，但是推荐设置并在授权服务中进行验证。 tokenExtractor 令牌提取器用来提取请求中的令牌。 请求匹配器，用来设置需要进行保护的资源路径，默认的情况下是受保护资源服务的全部路径。 受保护资源的访问规则，默认的规则是简单的身份验证（plain authenticated）。 其他的自定义权限保护规则通过 HttpSecurity 来进行配置。 2.2.3 ResourceServerTokenServices ResourceServerTokenServices 是组成授权服务的另一半。 1）.若是资源服务器和授权服务在同一个应用，可以使用DefaultTokenServices 2）.若是分离的。ResourceServerTokenServices必须知道令牌的如何解码。 ResourceServerTokenServices解析令牌的方法： 使用RemoteTokenServices，资源服务器通过HTTP请求来解码令牌。每次都请求授权服务器的端点-/oauth/check_toke，以此来解码令牌 若是访问量大，则通过http获取之后，换成令牌的结果 若是jwt令牌，需请求授权服务的/oauth/token_key，来获取key进行解码 注：授权认证服务需要把/oauth/check_toke暴露出来，并且附带上权限访问。 @Override public void configure(AuthorizationServerSecurityConfigurer oauthServer) throws Exception { oauthServer.tokenKeyAccess(\"isAnonymous() || hasAuthority('ROLE_TRUSTED_CLIENT')\") .checkTokenAccess(\"hasAuthority('ROLE_TRUSTED_CLIENT')\"); } 参考文章 Spring security OAuth2 深入解析 "},"microservice/auth/spring-cloud-starter-oauth2/SpringsecurityOAuth2授权url].html":{"url":"microservice/auth/spring-cloud-starter-oauth2/SpringsecurityOAuth2授权url].html","title":"Spring security OAuth2  端点（endpoints）授权url","keywords":"","body":"Spring security OAuth2 端点（endpoints）授权url1. 获取token令牌Spring security OAuth2 端点（endpoints）授权url 1. 获取token令牌 url： http://localhost:8101/oauth/token 请求参数：form-data key value username zsz password 1234qwer grant_type password key 0E2249E188468991EBD195351579954841236 code 9982 请求Headers： key value Authorization Basic ZmViczoxMjM0NTY= 返回结果： 访问资源服务器受保护的资源，附上令牌在请求头 { \"access_token\": \"940ff657-9454-4a32-95b2-9820f5d6c3f9\", \"token_type\": \"bearer\", \"refresh_token\": \"2cbfa791-0b92-4b46-b45e-b5c264c5e1f1\", \"expires_in\": 86399, \"scope\": \"all\" } "},"microservice/track/zipkin/ZinKin基础.html":{"url":"microservice/track/zipkin/ZinKin基础.html","title":"ZinKin基础","keywords":"","body":"ZinKin基础1. 是什么？2. 为什么使用ZipkinZinKin基础 1. 是什么？ Zipkin 分布式跟踪系统 可以帮助收集时间数据，解决在microservice 架构下的延迟问题 管理这些数据的收集和查找 每个应用程序向Zipkin 报告定时数据，Zipkin UI 呈现了一个依赖图表来展示多少跟踪请求经过了每个应用程序；如果想解决延迟问题，可以过滤或者排序所有的跟踪请求，并且可以查看跟踪请求占总跟踪时间的百分比 2. 为什么使用Zipkin 随着业务越来越复杂，系统也随之进行各种拆分。特别是随着微服务架构和容器技术的兴起，看似简单的一个应用，后台可能有几十个甚至几百个服务在支撑；一个前端的请求可能需要多次的服务调用最后才能完成；当请求变慢或者不可用时，我们无法得知是哪个后台服务引起的，这时就需要解决如何快速定位服务故障点，Zipkin 分布式跟踪系统就能很好的解决这样的问题。 "},"microservice/track/zipkin/Sleuth Zipkin链路追踪.html":{"url":"microservice/track/zipkin/Sleuth Zipkin链路追踪.html","title":"Sleuth Zipkin链路追踪","keywords":"","body":"Sleuth Zipkin链路追踪2. 整合Spring Cloud Sleuth2.1 引入sleuth依赖2.2 打印日志2.2.1 客户端ModuleA打印日志2.2.2 服务端ModuleB打印日志2.3 观察打印的日志3. 整合Zipkin3.1 安装RabbitMQ3.2 安装Zipkin3.3 新建zipkin数据库表3.4 启动zipkin.jar3.5 引入相关依赖3.6 application.yml里添加如下配置:3.7 浏览器访问3.8 查看数据表，看是否存储了信息：Sleuth Zipkin链路追踪 一个复杂的业务流程通常会被拆分多个微服务系统来完成，微服务间通过Feign来通信。当业务系统足够复杂时，一个完整的HTTP请求调用链一般会经过多个微服务系统，要通过日志来跟踪一整个调用链变得不再那么简单。 我们通过Spring Cloud Feign来远程访问受保护的资源，通过Spring Cloud Sleuth来跟踪这个过程，并借助Zipkin以图形化界面的方式展示 2. 整合Spring Cloud Sleuth 2.1 引入sleuth依赖 org.springframework.cloud spring-cloud-starter-sleuth 2.2 打印日志 2.2.1 客户端ModuleA打印日志 @Slf4j @RestController public class TestController { @Autowired private IHelloService helloService; @GetMapping(\"hello\") public String hello(String name) { log.info(\"Feign调用febs-server-system的/hello服务\"); return this.helloService.hello(name); } ...... } 2.2.2 服务端ModuleB打印日志 @Slf4j @RestController public class TestController { ...... @GetMapping(\"hello\") public String hello(String name) { log.info(\"/hello服务被调用\"); return \"hello\" + name; } } 2.3 观察打印的日志 观察客户端ModuleA 打印的日志 2019-08-23 14:22:51.774 INFO [ModuleA,72bb0469bee07104,72bb0469bee07104,false] 22728 --- [nio-8202-exec-1] c.m.f.s.test.controller.TestController : Feign调用febs-server-system的/hello服务 观察服务端ModuleB打印的日志 2019-08-23 14:22:52.469 INFO [ModuleB-System,72bb0469bee07104,43597a6edded6f2e,false] 812 --- [nio-8201-exec-2] c.m.f.s.s.controller.TestController : /hello服务被调用 可以看到，日志里出现了[ModuleB,72bb0469bee07104,72bb0469bee07104,false]信息，这些信息由Spring Cloud Sleuth生成，用于跟踪微服务请求链路。这些信息包含了4个部分的值，它们的含义如下： ModuleB微服务的名称，与spring.application.name对应； 72bb0469bee07104称为Trace ID，在一条完整的请求链路中，这个值是固定的。观察上面的日志即可证实这一点； 43597a6edded6f2e称为Span ID，它表示一个基本的工作单元； false表示是否要将该信息输出到Zipkin等服务中来收集和展示，这里我们还没有集成Zipkin，所以为false。 现在我们要跟踪整条请求链路，就可以通过traceId来完成，但是，从海量日志里捞取traceID并追踪也不是一件轻松的事。下面我们介绍借助zipkin实现使用图形化界面的方式追踪请求链路 3. 整合Zipkin 在整合Zipkin之前，我们需要先搭建RabbitMQ。RabbitMQ用于收集Sleuth提供的追踪信息，然后zipkin Server 从RabbitMQ里获取，这样可以提升性能 3.1 安装RabbitMQ RabbitMQ安装 3.2 安装Zipkin curl -sSL https://zipkin.io/quickstart.sh | bash -s 网络原因可能比较慢 3.3 新建zipkin数据库表 官方数据库表地址 3.4 启动zipkin.jar java -jar zipkin.jar --server.port=8402 --zipkin.storage.type=mysql --zipkin.storage.mysql.db=febs_cloud_base --zipkin.storage.mysql.username=root --zipkin.storage.mysql.password=123456 --zipkin.storage.mysql.host=localhost --zipkin.storage.mysql.port=3306 --zipkin.collector.rabbitmq.addresses=localhost:5672 --zipkin.collector.rabbitmq.username=febs --zipkin.collector.rabbitmq.password=123456 上面命令指定了数据库链接和RabbitMQ链接信息。更多可选配置可以解压zipkin.jar，查看zipkin\\BOOT-INF\\classes路径下的zipkin-server-shared.yml配置类源码。 3.5 引入相关依赖 org.springframework.cloud spring-cloud-starter-zipkin org.springframework.amqp spring-rabbit 3.6 application.yml里添加如下配置: spring: zipkin: sender: type: rabbit sleuth: sampler: probability: 1 rabbitmq: host: localhost port: 5672 username: febs password: 123456 spring.zipkin.sender.type指定了使用RabbitMQ收集追踪信息； spring.sleuth.sampler.probability默认值为0.1，即采样率才1/10，发送10笔请求只有一笔会被采集。为了测试方便，我们可以将它设置为1，即100%采样； spring.rabbitmq用于配置RabbitMQ连接信息，你可能会问，为什么刚刚RabbitMQ端口是15672，这里却配置为5672，是不是写错了呢？其实不是，15672是RabbitMQ的管理页面端口，5672是AMPQ端口。 3.7 浏览器访问 http://localhost:8402/zipkin/链接 查看依赖关系 3.8 查看数据表，看是否存储了信息： "},"microservice/track/zipkin/Zipkin下载.html":{"url":"microservice/track/zipkin/Zipkin下载.html","title":"Zipkin下载","keywords":"","body":"Zipkin下载方式一：方式二：Zipkin下载 方式一： 命令行下载 curl -sSL https://zipkin.io/quickstart.sh | bash -s 方式二： 从百度网盘上下载 链接: https://pan.baidu.com/s/1sIzmF1JlROvE3ELWAoZUug 提取码: 3ef4 复制这段内容后打开百度网盘手机App，操作更方便哦 "},"microservice/track/zipkin/SpringBoot整合Zipkin.html":{"url":"microservice/track/zipkin/SpringBoot整合Zipkin.html","title":"SpringBoot整合Zipkin","keywords":"","body":"SpringBoot整合Zipkin1. 项目依赖2. 启动类SpringBoot整合Zipkin 1. 项目依赖 org.springframework.cloud spring-cloud-starter-eureka io.zipkin.java zipkin-server io.zipkin.java zipkin-autoconfigure-ui 2. 启动类 "},"microservice/track/skywalking/skywalking分布式追踪.html":{"url":"microservice/track/skywalking/skywalking分布式追踪.html","title":"skywalking分布式追踪","keywords":"","body":"skywalking分布式追踪1. 简介2. 架构3. 搭建skywalking3.1 docker-compose.yml3.2 创建挂载目录和配置文件3.3 编辑 elasticsearch.yml3.4 授权3.5 创建skywalking-oap的挂载目录和配置文件3.6 设置内存权限3.7 启动容器3.9 访问 Skywalking的UIskywalking分布式追踪 1. 简介 skywalking 主要包括了分布式追踪、性能指标分析、应用和服务依赖分析等功能，使用体验后感觉比zipkin更为直观，是替代zipkin的一个不错的选择 2. 架构 从上图可以看出skywalking主要分为四个模块：agent、collector、webapp-ui和storage。 agent： Skywalking agent探针无侵入地接入Spring Cloud应用 collector 通过HTTP或者GRPC将应用数据采集到collector收集器 storage collector中的数据存储与storage，支持MySQL、H2、Elasticsearch等存储 webapp-ui 最终这些数据集中在webapp-ui以图形化的方式呈现 3. 搭建skywalking 我们使用Docker Compose 搭建Skywalking，数据存储我们选择性能更高的Elasticsearch。 3.1 docker-compose.yml 完整的docker-compose.yml如下 version: '3' services: elasticsearch: image: elasticsearch:6.4.1 container_name: elasticsearch restart: always environment: - cluster.name=elasticsearch - xpack.security.enabled=false - \"ES_JAVA_OPTS=-Xms216m -Xmx216m\" - node.name=elasticsearch_node_1 - \"TZ=Asia/Shanghai\" volumes: - /elk/elasticsearch/data:/usr/share/elasticsearch/data - /elk/elasticsearch/logs:/usr/share/elasticsearch/logs - /elk/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml ports: - 9200:9200 - 9300:9300 skywalking-oap: image: apache/skywalking-oap-server:6.4.0 container_name: skywalking-oap depends_on: - elasticsearch links: - elasticsearch restart: always ports: - 11800:11800 - 12800:12800 environment: - \"TZ=Asia/Shanghai\" volumes: - /elk/skywalking/config:/apache-skywalking-apm-bin/config:ro skywalking-ui: image: apache/skywalking-ui:6.4.0 container_name: skywalking-ui depends_on: - skywalking-oap links: - skywalking-oap restart: always ports: - 8090:8080 environment: - \"collector.ribbon.listOfServers=skywalking-oap:12800\" - \"TZ=Asia/Shanghai\" 上面部署了三个容器：Elasticsearch、skywalking-oap（即collector）和skywalking-ui（即webapp-ui）。 3.2 创建挂载目录和配置文件 # 创建Elasticsearch的挂载目录 mkdir -p /elk/elasticsearch/data /elk/elasticsearch/logs # 创建Elasticsearch的配置文件elasticsearch.yml vim /elk/elasticsearch/elasticsearch.yml 3.3 编辑 elasticsearch.yml 配置文件elasticsearch.yml内容如下所示： http.host: 0.0.0.0 http.cors.enabled: true http.cors.allow-origin: \"*\" transport.host: 0.0.0.0 3.4 授权 对Elasticsearch的挂载目录授予最高权限： chmod 777 -R /elk/elasticsearch 3.5 创建skywalking-oap的挂载目录和配置文件 # 创建skywalking-oap的挂载目录 mkdir -p /elk/skywalking/config # 创建skywalking-oap的配置文件skywalking.yml touch /elk/skywalking/config/skywalking.yml skywalking.yml内容如下所示： # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \"License\"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. cluster: standalone: # Please check your ZooKeeper is 3.5+, However, it is also compatible with ZooKeeper 3.4.x. Replace the ZooKeeper 3.5+ # library the oap-libs folder with your ZooKeeper 3.4.x library. # zookeeper: # nameSpace: ${SW_NAMESPACE:\"\"} # hostPort: ${SW_CLUSTER_ZK_HOST_PORT:localhost:2181} # #Retry Policy # baseSleepTimeMs: ${SW_CLUSTER_ZK_SLEEP_TIME:1000} # initial amount of time to wait between retries # maxRetries: ${SW_CLUSTER_ZK_MAX_RETRIES:3} # max number of times to retry # kubernetes: # watchTimeoutSeconds: ${SW_CLUSTER_K8S_WATCH_TIMEOUT:60} # namespace: ${SW_CLUSTER_K8S_NAMESPACE:default} # labelSelector: ${SW_CLUSTER_K8S_LABEL:app=collector,release=skywalking} # uidEnvName: ${SW_CLUSTER_K8S_UID:SKYWALKING_COLLECTOR_UID} # consul: # serviceName: ${SW_SERVICE_NAME:\"SkyWalking_OAP_Cluster\"} # Consul cluster nodes, example: 10.0.0.1:8500,10.0.0.2:8500,10.0.0.3:8500 # hostPort: ${SW_CLUSTER_CONSUL_HOST_PORT:localhost:8500} core: default: # Mixed: Receive agent data, Level 1 aggregate, Level 2 aggregate # Aggregator: Level 2 aggregate role: ${SW_CORE_ROLE:Mixed} # Mixed/Receiver/Aggregator restHost: ${SW_CORE_REST_HOST:0.0.0.0} restPort: ${SW_CORE_REST_PORT:12800} restContextPath: ${SW_CORE_REST_CONTEXT_PATH:/} gRPCHost: ${SW_CORE_GRPC_HOST:0.0.0.0} gRPCPort: ${SW_CORE_GRPC_PORT:11800} downsampling: - Hour - Day - Month # Set a timeout on metric data. After the timeout has expired, the metric data will automatically be deleted. recordDataTTL: ${SW_CORE_RECORD_DATA_TTL:90} # Unit is minute minuteMetricsDataTTL: ${SW_CORE_MINUTE_METRIC_DATA_TTL:90} # Unit is minute hourMetricsDataTTL: ${SW_CORE_HOUR_METRIC_DATA_TTL:36} # Unit is hour dayMetricsDataTTL: ${SW_CORE_DAY_METRIC_DATA_TTL:45} # Unit is day monthMetricsDataTTL: ${SW_CORE_MONTH_METRIC_DATA_TTL:18} # Unit is month storage: elasticsearch: # set the namespace in elasticsearch clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:elasticsearch:9200} indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2} indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0} # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests metadataQueryMaxSize: ${SW_STORAGE_ES_QUERY_MAX_SIZE:5000} segmentQueryMaxSize: ${SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200} # h2: # driver: ${SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource} # url: ${SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db} # user: ${SW_STORAGE_H2_USER:sa} # metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} # mysql: # metadataQueryMaxSize: ${SW_STORAGE_H2_QUERY_MAX_SIZE:5000} receiver-sharing-server: default: receiver-register: default: receiver-trace: default: bufferPath: ${SW_RECEIVER_BUFFER_PATH:../trace-buffer/} # Path to trace buffer files, suggest to use absolute path bufferOffsetMaxFileSize: ${SW_RECEIVER_BUFFER_OFFSET_MAX_FILE_SIZE:100} # Unit is MB bufferDataMaxFileSize: ${SW_RECEIVER_BUFFER_DATA_MAX_FILE_SIZE:500} # Unit is MB bufferFileCleanWhenRestart: ${SW_RECEIVER_BUFFER_FILE_CLEAN_WHEN_RESTART:false} sampleRate: ${SW_TRACE_SAMPLE_RATE:10000} # The sample rate precision is 1/10000. 10000 means 100% sample in default. slowDBAccessThreshold: ${SW_SLOW_DB_THRESHOLD:default:200,mongodb:100} # The slow database access thresholds. Unit ms. receiver-jvm: default: service-mesh: default: bufferPath: ${SW_SERVICE_MESH_BUFFER_PATH:../mesh-buffer/} # Path to trace buffer files, suggest to use absolute path bufferOffsetMaxFileSize: ${SW_SERVICE_MESH_OFFSET_MAX_FILE_SIZE:100} # Unit is MB bufferDataMaxFileSize: ${SW_SERVICE_MESH_BUFFER_DATA_MAX_FILE_SIZE:500} # Unit is MB bufferFileCleanWhenRestart: ${SW_SERVICE_MESH_BUFFER_FILE_CLEAN_WHEN_RESTART:false} istio-telemetry: default: envoy-metric: default: # receiver_zipkin: # default: # host: ${SW_RECEIVER_ZIPKIN_HOST:0.0.0.0} # port: ${SW_RECEIVER_ZIPKIN_PORT:9411} # contextPath: ${SW_RECEIVER_ZIPKIN_CONTEXT_PATH:/} query: graphql: path: ${SW_QUERY_GRAPHQL_PATH:/graphql} alarm: default: telemetry: none: 上面配置中， 存储选择了Elasticsearch代替默认的H2，其他配置均为skywalking-oap的默认配置。 Skywalking的UI。默认是8080端口。和我应用端口冲突了，我改成8090 上面docker-compose.yml中我们通过TZ=Asia/Shanghai设置了时区，如果不指定时区的话默认是UTC时区，你会看到收集到的数据会比实际早8个小时。 3.6 设置内存权限 在启动之前我们还需要通过sysctl -w vm.max_map_count=262144命令设置内存权限，262144是构建Elasticsearch的最小内存。 3.7 启动容器 在目录/elk/DockerCompos， 使用docker-compose up -d启动这几个容器： 3.8 查看 Elasticsearch 稍等片刻后，在浏览器访问 http://120.79.200.111:9200/ 地址，如下所示表示Elasticsearch已经启动成功： 3.9 访问 Skywalking的UI 访问Skywalking的UI界面：http://120.79.200.111:8090/ 因为还没有整合agent所以现在还没有数据，界面是空的。 "},"microservice/track/skywalking/SpringCloud应用整合Skywalking.html":{"url":"microservice/track/skywalking/SpringCloud应用整合Skywalking.html","title":"Spring Cloud应用整合Skywalking","keywords":"","body":"Spring Cloud应用整合Skywalking1. 下载Skywalking的压缩包2. 配置启动环境3. 查看效果3.1 主页跟踪效果3.2 拓扑图3.3 请求链路跟踪3.4 仪表盘的Service3.5 仪表盘的Endpoint3.6 仪表盘的InstanceSpring Cloud应用整合Skywalking 1. 下载Skywalking的压缩包 我们到 http://skywalking.apache.org/downloads/ 地址下载Skywalking的压缩包 下载解压后将agent文件。agent文件夹内容如下所示 2. 配置启动环境 在IDEA对应项目的启动配置中添加Skywalking agent探针配置 我们以chinahrss-auth为例 点击IDEA的Edit Configurations...，然后选择Environment -> VM Options，添加如下脚本： -javaagent:/Users/zsz/Project/software/skywalking/apache-skywalking-apm-bin/agent/skywalking-agent.jar -Dskywalking.agent.service_name=chinahrss-auth -Dskywalking.collector.backend_service=120.79.200.111:11800 -javaagent:/Users/zsz/Project/software/skywalking/apache-skywalking-apm-bin/agent/skywalking-agent.jar 指定了探针应用agent的地址 -Dskywalking.agent.service_name=chinahrss-auth 指定了被手机的应用名称为chinahrss-auth -Dskywalking.collector.backend_service=120.79.200.111:11800 指定了收集器的地址，即刚刚我们使用Docker Compose构建的skywalking-oap。 3. 查看效果 3.1 主页跟踪效果 3.2 拓扑图 3.3 请求链路跟踪 可以清晰的看到完整的请求调用链。查看一笔失败的调用，可以看到失败的具体异常堆栈： 3.4 仪表盘的Service 点击仪表盘页面的Service，可以看到一些服务相关的信息，如平均响应时间、平均吞吐量、平均时延统计，如下图所示： 3.5 仪表盘的Endpoint 点击仪表盘页面的Endpoint，可以看到一些端点相关的信息，如下图所示： 3.6 仪表盘的Instance 点击仪表盘页面的Instance，可以看到一些JVM相关的信息，如下图所示： "},"microservice/callservice/Feign/Feign基础.html":{"url":"microservice/callservice/Feign/Feign基础.html","title":"Feign基础","keywords":"","body":"Feign基础1. 集成使用1.1 依赖 feign1.2 服务端1.3 客户端Feign基础 微服务之间服务的调用可以借助Spring Cloud Feign 来完成，Spring Cloud Feign 内部整合了Spring Cloud Ribbon 和 Spring Cloud Hystrix,所以它具有客户端负载均衡和服务容错的功能。 1. 集成使用 1.1 依赖 feign org.springframework.cloud spring-cloud-starter-openfeign 1.2 服务端 1.2.1在服务提供方提供接口 @RestController public class TestController { @GetMapping(\"hello\") public String hello(String name) { return \"hello\" + name; } ...... } 1.3 客户端 1.3.1 开启Feign Client @EnableFeignClients @EnableDiscoveryClient @SpringBootApplication public class YlzapApplication { public static void main(String[] args) { SpringApplication.run(YlzapServerTestApplication.class, args); } } 1.3.2 在客户端定义Feign Client接口 @FeignClient(value = ServerConstant.FEBS_SERVER_SYSTEM, contextId = \"helloServiceClient\", fallbackFactory = HelloServiceFallback .class) public interface IHelloService { @GetMapping(\"hello\") String hello(@RequestParam String name); } 1.3.3 定义回退方法Fallback @Slf4j @Component public class HelloServiceFallback implements FallbackFactory { @Override public IHelloService create(Throwable throwable) { return new IHelloService() { @Override public String hello(String name) { log.error(\"调用febs-server-system服务出错\", throwable); return \"调用出错\"; } }; } } 1.3.4 修改application.yml让配置生效 feign: hystrix: enabled: true 因为Feign的回退功能是基于Hystrix实现的，所以需要开启它。 1.3.5 调用服务端方法 @RestController public class TestController { @Autowired private IHelloService helloService; @GetMapping(\"hello\") public String hello(String name){ return this.helloService.hello(name); } ...... } "},"microservice/log/logback/logback日志打印.html":{"url":"microservice/log/logback/logback日志打印.html","title":"logback日志打印","keywords":"","body":"logback日志打印2. 集成logback2.1 新建logback配置文件logback-spring.xml2.2 logback配置文件结构2.3 标签的作用2.4 查看日志logback日志打印 目前系统细致输出在控制台中，重启项目后，之前的日志就丢失了，我们可以借助logback将系统日志保存到日志文件中。 Spring Boot项目在引用了 spring-boot-starter-logging依赖后，默认使用了logback来记录日志。因为我们之前搭建的微服务系统都引用了spring-boot-starter=依赖，该依赖包含了spring-boot-starter-logging,所以无需再次引入 2. 集成logback 2.1 新建logback配置文件logback-spring.xml febs ${log.colorPattern} ${log.path}/info/info.%d{yyyy-MM-dd}.log ${log.maxHistory} ${log.pattern} INFO ACCEPT DENY ${log.path}/error/error.%d{yyyy-MM-dd}.log ${log.pattern} ERROR ACCEPT DENY 2.2 logback配置文件结构 2.3 标签的作用 2.3.1 configuration 标签 是logback配置文件的跟标签 scan: 当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒，当scan为true时，此属性生效。默认的时间间隔为1分钟 debug：当此属性为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false 2.3.2 propert 标签 用来定义变量值标签，有两个属性，name和value，其中name的值是变量的名称，value的值是变量定义的值。通过定义的值会被插入到logger上下文中。定义变量后，可以使${}来使用变量 这段配置用于引用Spring上下文变量。通过这段配置，我们可以在logback配置文件中使用用${springAppName}来引用配置文件application.yml里的spring.application.name配置值，在server-system模块中，该值为Server-System。 上面这段配置定义了log.path变量，用于指定日志文件存储路径。 上面这段配置定义了log.maxHistory变量，用于指定日志文件存储的天数，这里指定为15天。 这段配置定义了彩色日志打印的格式。在logback配置文件中，我们可以使用%magenta()、%boldCyan()等标识指定日志的颜色；%d{yyyy-MM-dd HH:mm:ss}用于格式化日志打印时间；%highlight(%-5level)配置了不同日志级别使用不同的颜色高亮显示；%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-Span-Export:-}用于打印Spring Cloud Sleuth提供的TraceId和SpanId等信息，如果不配置这些信息，我们在上一章搭建的Zipkin Server就无法追踪我们的请求链了。 。 这段配置定义了普通日志打印格式，大体上和上面彩色日志配置差不多，却别就是去掉了颜色配置。 如果微服务项目没有使用Spring Cloud Sleuth进行请求追踪，那么TraceId和SpanId打印出来都是空的，可以用下面这段配置来替代： appender用来格式化日志输出节点，有俩个属性name和class，class用来指定哪种输出策略，常用就是控制台输出策略和文件输出策略。 ${log.colorPattern} 上面这段配置用于指定日志输出到控制台，日志打印格式采用上面定义的彩色日志打印（IDEA控制台支持彩色日志输出），这样在开发的时候，控制台输出的日志会更为美观，易于分析问题。 ${log.path}/info/info.%d{yyyy-MM-dd}.log ${log.maxHistory} ${log.pattern} INFO ACCEPT DENY ${log.path}/error/error.%d{yyyy-MM-dd}.log ${log.pattern} ERROR ACCEPT DENY 这两段配置用于指定日志输出到日志文件。其中，名称为file_info的appender指定了INFO级别的日志输出到log/febs-server-system/info目录下，文件名称为info.日期.log，并且日志格式为普通格式，因为文件一般不支持彩色显示；名称为file_error的appender指定了ERROR级别的日志输出到log/febs-server-system/error目录下，文件名称为error.日期.log，日志格式也为普通格式。 2.3.3 root标签 root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性，用来设置打印级别。如果在appender里制定了日志打印的级别，那么root指定的级别将会被覆盖。 剩下的模块照着febs-server-system模块配置即可。编写好日志配置文件后，在启动各个微服务系统的时候，控制台输出如下所示： 2.4 查看日志 "},"microservice/log/elk/ELK日志收集.html":{"url":"microservice/log/elk/ELK日志收集.html","title":"ELK日志收集","keywords":"","body":"ELK日志收集1. 背景2. 简介3. Docker Compose搭建ELK3.1 增加内存3.2 创建Elasticsearch数据挂载路径：3.3 对该路径授予777权限3.4 创建Elasticsearch插件挂载路径：3.5 创建Logstash配置文件存储路径：3.6 创建配置文件3.7 创建ELK Docker Compose文件存储路径3.8 启动3.9 查看启动状态4. Logstash中安装json_lines插件4.1 进入Logstash容器4.2 安装步骤5. 效果ELK日志收集 1. 背景 各个微服务系统的日志都保存在各自指定的目录中，如果这些微服务部署在不同的服务器上，那么日志文件也是分散在各自的服务器上。分散的日志不利于我们快速通过日志定位问题，我们可以借助ELK来收集各个微服务系统的日志并集中展示。 2. 简介 ELK即Elasticsearch、Logstash和Kibana首字母缩写。Elasticsearch用于存储日志信息，Logstash用于收集日志，Kibana用于图形化展示。 3. Docker Compose搭建ELK 3.1 增加内存 Elasticsearch默认使用mmapfs目录来存储索引。操作系统默认的mmap计数太低可能导致内存不足，我们可以使用下面这条命令来增加内存： sysctl -w vm.max_map_count=262144 官方文档 3.2 创建Elasticsearch数据挂载路径： mkdir -p /elk/elasticsearch/data 3.3 对该路径授予777权限 chmod 777 /elk/elasticsearch/data 3.4 创建Elasticsearch插件挂载路径： mkdir -p /elk/elasticsearch/plugins 3.5 创建Logstash配置文件存储路径： mkdir -p /elk/logstash 3.6 创建配置文件 在/elk/logstash路径下创建logstash-elk.conf配置文件 vim /elk/logstash/logstash-elk.conf 内容如下所示： input { tcp { mode => \"server\" host => \"0.0.0.0\" port => 4560 codec => json_lines } } output { elasticsearch { hosts => \"es:9200\" index => \"febs-logstash-%{+YYYY.MM.dd}\" } } 3.7 创建ELK Docker Compose文件存储路径 mkdir -p /elk/DockerCompos 在该目录下创建docker-compose.yml文件： vim /elk/DockerCompos/docker-compose.yml 内容如下所示： version: '3' services: elasticsearch: image: elasticsearch:6.4.1 container_name: elasticsearch environment: - \"cluster.name=elasticsearch\" #集群名称为elasticsearch - \"discovery.type=single-node\" #单节点启动 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" #jvm内存分配为512MB volumes: - /elk/elasticsearch/plugins:/usr/share/elasticsearch/plugins - /elk/elasticsearch/data:/usr/share/elasticsearch/data ports: - 9200:9200 kibana: image: kibana:6.4.1 container_name: kibana links: - elasticsearch:es #配置elasticsearch域名为es depends_on: - elasticsearch environment: - \"elasticsearch.hosts=http://es:9200\" #因为上面配置了域名，所以这里可以简 写为http://es:9200 ports: - 5601:5601 logstash: image: logstash:6.4.1 container_name: logstash volumes: - /elk/logstash/logstash-elk.conf:/usr/share/logstash/pipeline/logstash.conf depends_on: - elasticsearch links: - elasticsearch:es ports: - 4560:4560 切换到 /elk/DockerCompos 目录下，使用如下命令 3.8 启动 docker-compose up -d 第一次启动的时候，Docker需要拉取ELK镜像，过程可能稍慢，耐心等待即可。成功启动后，观察容器运行情况： 3.9 查看启动状态 docker ps -a 三个状态都为up就证明启动成功了 如遇到状态为exit的，请查看当前内存是否足够 4. Logstash中安装json_lines插件 4.1 进入Logstash容器 使用如下命令进入到Logstash容器中： docker exec -it logstash /bin/bash 4.2 安装步骤 切换到/bin目录 安装json_lines插件 退出 5. 效果 使用浏览器访问 http://120.79.200.111:5601/ 可以看到Kibana管理界面： "},"microservice/log/elk/微服务ELK日志配置.html":{"url":"microservice/log/elk/微服务ELK日志配置.html","title":"微服务ELK日志配置","keywords":"","body":"微服务ELK日志配置2. 演示效果在Kibana里搜索日志：微服务ELK日志配置 我们在微服务模块中引入 Logstash 依赖 net.logstash.logback logstash-logback-encoder 6.1 在微服务的配置文件logback-spring.xml里添加如下配置： 120.79.200.111:4560 ...... ...... 120.79.200.111:4560对应我们刚刚搭建的Logstash地址。 2. 演示效果 Kibana UI地址 http://120.79.200.111:5601/ 依次启动chinahrss-gateway、chinahrss-auth、chinahrss-server-system和chinahrss-server-test模块，然后回到Kibana管理界面创建Kinaba Index Patterns： 在Index pattern里输入我们在logstash配置文件logstash-elk.conf里output.index指定的值chinahrss-logstash-*： 点击Next Step，在下拉框里选择@timestamp： 最后点击Create Index Pattern按钮完成创建。 在Kibana里搜索日志： "},"microservice/log/p6spy/P6Spy记录数据库日志.html":{"url":"microservice/log/p6spy/P6Spy记录数据库日志.html","title":"P6Spy记录数据库日志","keywords":"","body":"P6Spy记录数据库日志1. 简介1.1 优势2. 集成使用2.1 导入Pom依赖2.2 配置spy.properties2.3 自定义日志输出格式3. 完整的配置参考参考文章P6Spy记录数据库日志 1. 简介 P6Spy 是一个框架，可以无缝得拦截和记录数据库活动，无需更改现有程序代码。 一般使用p6spy打印我们最后执行的sql语句。 1.1 优势 常用的数据框架也会自带打印sql的功能，比如jpa,mybatis等，但是一般都会有缺陷，比如打印的sql是不带执行参数拼接的sql，这种sql不完整，不具有直接可执行性 2. 集成使用 2.1 导入Pom依赖 p6spy p6spy 3.8.5 2.2 配置spy.properties # 指定 Log 的 appender，取值： appender=com.p6spy.engine.spy.appender.Slf4JLogger # 指定日志输出样式 logMessageFormat=com.zszdevelop.chinahrss.server.system.configure.P6spySqlFormatConfigure #P6Outage 模块是否记录较长时间运行的语句 默认false outagedetection=true # P6Outage 模块执行时间设置，整数值 （以秒为单位)），只有当超过这个时间才进行记录 Log。 默认30s outagedetectioninterval=2 #是否开启日志过滤 默认false， 这项配置是否生效前提是配置了 include/exclude/sqlexpression filter=true # 过滤 Log 时所排除的表名列表，以逗号分隔 默认为空 exclude=select 1 2.3 自定义日志输出格式 public class P6spySqlFormatConfigure implements MessageFormattingStrategy { @Override public String formatMessage(int connectionId, String now, long elapsed, String category, String prepared, String sql, String url) { return StringUtils.isNotBlank(sql) ? DateUtil.formatFullTime(LocalDateTime.now(), DateUtil.FULL_TIME_SPLIT_PATTERN) + \" | 耗时 \" + elapsed + \" ms | SQL 语句：\" + StringUtils.LF + sql.replaceAll(\"[\\\\s]+\", StringUtils.SPACE) + \";\" : StringUtils.EMPTY; } } 3. 完整的配置参考 # 指定应用的日志拦截模块,默认为com.p6spy.engine.spy.P6SpyFactory #modulelist=com.p6spy.engine.spy.P6SpyFactory,com.p6spy.engine.logging.P6LogFactory,com.p6spy.engine.outage.P6OutageFactory # 真实JDBC driver , 多个以 逗号 分割 默认为空 #driverlist= # 是否自动刷新 默认 flase #autoflush=false # 配置SimpleDateFormat日期格式 默认为空 #dateformat= # 打印堆栈跟踪信息 默认flase #stacktrace=false # 如果 stacktrace=true，则可以指定具体的类名来进行过滤。 #stacktraceclass= # 监测属性配置文件是否进行重新加载 #reloadproperties=false # 属性配置文件重新加载的时间间隔，单位:秒 默认60s #reloadpropertiesinterval=60 # 指定 Log 的 appender，取值： #appender=com.p6spy.engine.spy.appender.Slf4JLogger #appender=com.p6spy.engine.spy.appender.StdoutLogger #appender=com.p6spy.engine.spy.appender.FileLogger # 指定 Log 的文件名 默认 spy.log #logfile=spy.log # 指定是否每次是增加 Log，设置为 false 则每次都会先进行清空 默认true #append=true # 指定日志输出样式 默认为com.p6spy.engine.spy.appender.SingleLineFormat , 单行输出 不格式化语句 #logMessageFormat=com.p6spy.engine.spy.appender.SingleLineFormat # 也可以采用 com.p6spy.engine.spy.appender.CustomLineFormat 来自定义输出样式, 默认值是%(currentTime)|%(executionTime)|%(category)|connection%(connectionId)|%(sqlSingleLine) # 可用的变量为: # %(connectionId) connection id # %(currentTime) 当前时间 # %(executionTime) 执行耗时 # %(category) 执行分组 # %(effectiveSql) 提交的SQL 换行 # %(effectiveSqlSingleLine) 提交的SQL 不换行显示 # %(sql) 执行的真实SQL语句，已替换占位 # %(sqlSingleLine) 执行的真实SQL语句，已替换占位 不换行显示 #customLogMessageFormat=%(currentTime)|%(executionTime)|%(category)|connection%(connectionId)|%(sqlSingleLine) # date类型字段记录日志时使用的日期格式 默认dd-MMM-yy #databaseDialectDateFormat=dd-MMM-yy # boolean类型字段记录日志时使用的日期格式 默认boolean 可选值numeric #databaseDialectBooleanFormat=boolean # 是否通过jmx暴露属性 默认true #jmx=true # 如果jmx设置为true 指定通过jmx暴露属性时的前缀 默认为空 # com.p6spy(.)?:name=#jmxPrefix= # 是否显示纳秒 默认false #useNanoTime=false # 实际数据源 JNDI #realdatasource=/RealMySqlDS # 实际数据源 datasource class #realdatasourceclass=com.mysql.jdbc.jdbc2.optional.MysqlDataSource # 实际数据源所携带的配置参数 以 k=v 方式指定 以 分号 分割 #realdatasourceproperties=port;3306,serverName;myhost,databaseName;jbossdb,foo;bar # jndi数据源配置 # 设置 JNDI 数据源的 NamingContextFactory。 #jndicontextfactory=org.jnp.interfaces.NamingContextFactory # 设置 JNDI 数据源的提供者的 URL。 #jndicontextproviderurl=localhost:1099 # 设置 JNDI 数据源的一些定制信息，以分号分隔。 #jndicontextcustom=java.naming.factory.url.pkgs;org.jboss.naming:org.jnp.interfaces # 是否开启日志过滤 默认false， 这项配置是否生效前提是配置了 include/exclude/sqlexpression #filter=false # 过滤 Log 时所包含的表名列表，以逗号分隔 默认为空 #include= # 过滤 Log 时所排除的表名列表，以逗号分隔 默认为空 #exclude= # 过滤 Log 时的 SQL 正则表达式名称 默认为空 #sqlexpression= #显示指定过滤 Log 时排队的分类列表，取值: error, info, batch, debug, statement, #commit, rollback, result and resultset are valid values # (默认 info,debug,result,resultset,batch) #excludecategories=info,debug,result,resultset,batch # 是否过滤二进制字段 # (default is false) #excludebinary=false # P6Log 模块执行时间设置，整数值 (以毫秒为单位)，只有当超过这个时间才进行记录 Log。 默认为0 #executionThreshold= # P6Outage 模块是否记录较长时间运行的语句 默认false # outagedetection=true|false # P6Outage 模块执行时间设置，整数值 （以秒为单位)），只有当超过这个时间才进行记录 Log。 默认30s # outagedetectioninterval=integer time (seconds) 参考文章 spring boot集成p6spy的最佳实践-p6spy-spring-boot-starter "},"microservice/deploy/docker/Centos安装Docker.html":{"url":"microservice/deploy/docker/Centos安装Docker.html","title":"Centos安装Docker","keywords":"","body":"Centos安装Docker1. 安装步骤1.1 卸载旧版本Docker（如果系统之前没安装过Docker，可以跳过）1.2 安装Docker所需要的包1.3 设置稳定的仓库1.4 安装最新版的Docker引擎1.5 启动Docker2 安装Docker Compose2.1 获取Docker Compose的最新稳定版本：2.2 对二进制文件授予可执行权限2.3 创建link：2.4 查看是否安装成功：Centos安装Docker Docker官方文档安装教程 Docker Compose官方文档安装教程 1. 安装步骤 1.1 卸载旧版本Docker（如果系统之前没安装过Docker，可以跳过） yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 1.2 安装Docker所需要的包 yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 1.3 设置稳定的仓库 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1.4 安装最新版的Docker引擎 yum install docker-ce docker-ce-cli containerd.io 1.5 启动Docker systemctl start docker 查看是否安装成功： 2 安装Docker Compose 2.1 获取Docker Compose的最新稳定版本： curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 2.2 对二进制文件授予可执行权限 chmod +x /usr/local/bin/docker-compose 2.3 创建link： ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 2.4 查看是否安装成功： "},"microservice/deploy/docker/Docker镜像与容器.html":{"url":"microservice/deploy/docker/Docker镜像与容器.html","title":"Docker 镜像与容器","keywords":"","body":"Docker 镜像与容器1. 简介2. 什么是Dockerfile3. 什么是Docker容器Docker 镜像与容器 1. 简介 Docker Image 俗称Docker镜像，它是由一系列图层（Layer）构成的，每个图层代表Dockerfile（通过Dockerfile我们可以创建镜像）中的一行指令，镜像是只读的 2. 什么是Dockerfile 我们举个简单的例子 FROM centos RUN yum install -y vim 上面Dockerfile包含两行命令（所以它对应两个图层），第一行表示从centos这个镜像中创建一个图层，然后第二行表示接着运行yum install -y vim来安装vim。通过这个Dockerfile我们可以构建一个镜像，通过镜像我们可以创建一个容器（Container） 3. 什么是Docker容器 容器是通过镜像构建的一个隔离的应用平台，它包含了运行应用程序所需要的一切。和镜像相比，他在顶部多了一个可读写的图层，如下图所示 镜像和容器的关系就像是Java的类和对象的关系那样，竟然是构建容器的模板，容器是镜像构建出来的实例 "},"microservice/deploy/docker/使用Dockerfile定制镜像.html":{"url":"microservice/deploy/docker/使用Dockerfile定制镜像.html","title":"使用Dockerfile定制镜像","keywords":"","body":"使用Dockerfile定制镜像1.1 FROM 指定基础镜像1.2 RUN 执行命令1.3 构建镜像使用Dockerfile定制镜像 镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令写入一个脚本，用这个脚本来构建，定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是Dockerfile Dokcerfile 是一个文本文件，其内包含了一条条的 指令(Instruction),每一条指定构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 还以之前定制nginx 镜像为例，这次我们使用Dockerfile来定制 在一个空白目录中，建立一个文本文件，并命名为Dockerfile： mkdir mynginx cd mynginx touch Dockerfile 其内容为： FROM nginx RUN echo 'Hello, Docker!' > /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 1.1 FROM 指定基础镜像 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个nginx镜像的容器，再进行修改一样，基础镜像是必须指定的。而FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 FROM scratch ... 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 1.2 RUN 执行命令 RUN 指令是用来执行命令行命令的。由于命令行的强大功能。RUN 指令在定制镜像时是最常用的功能之一。 shell格式：RUN, 就像直接在命令行中输入命令一样。刚才写的Dockerfile 中的RUN 指令就是这种格式 RUN echo 'Hello, Docker!' > /usr/share/nginx/html/index.html exec格式：RUN [\"可执行文件\", \"参数1\", \"参数2\"]，这更像是函数调用中的格式。 1.2.1 RUN 错误使用方法 FROM debian:stretch RUN apt-get update RUN apt-get install -y gcc libc6-dev make wget RUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" RUN mkdir -p /usr/src/redis RUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 RUN make -C /usr/src/redis RUN make -C /usr/src/redis install Dockerfile中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样。新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了7层镜像。这完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 1.2.2 正确写法 FROM debian:stretch RUN buildDeps='gcc libc6-dev make wget' \\ && apt-get update \\ && apt-get install -y $buildDeps \\ && wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ && mkdir -p /usr/src/redis \\ && tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ && make -C /usr/src/redis \\ && make -C /usr/src/redis install \\ && rm -rf /var/lib/apt/lists/* \\ && rm redis.tar.gz \\ && rm -r /usr/src/redis \\ && apt-get purge -y --auto-remove $buildDeps Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 && 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 1.2.3 良好的格式 \\ : 支持 Shell 类的行尾添加 \\ 的命令换行方式 # : 行首 # 进行注释的格式 apt : 清理工作命令 良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 1.3 构建镜像 我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在Dockerfile 文件所在目录执行 $ docker build -t nginx:v3 . Sending build context to Docker daemon 2.048 kB Step 1 : FROM nginx ---> e43d811ce2f4 Step 2 : RUN echo 'Hello, Docker!' > /usr/share/nginx/html/index.html ---> Running in 9cdc27646c7b ---> 44aa4490ce2c Removing intermediate container 9cdc27646c7b Successfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。 在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build 命令进行镜像构建。其格式为： docker build [选项] 在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。 "},"microservice/deploy/docker/删除所有的docker容器和镜像.html":{"url":"microservice/deploy/docker/删除所有的docker容器和镜像.html","title":"Docker停止、删除所有的docker容器和镜像","keywords":"","body":"停止、删除所有的docker容器和镜像1. 列出所有的容器ID2. 停止所有的容器3. 删除所有的容器4. 删除所有的镜像停止、删除所有的docker容器和镜像 1. 列出所有的容器ID docker ps -aq 2. 停止所有的容器 docker stop $(docker ps -aq) 3. 删除所有的容器 docker rm $(docker ps -aq) 4. 删除所有的镜像 docker rmi $(docker images -q) "},"microservice/deploy/docker/Instruction/Dockerfile之COPY复制文件.html":{"url":"microservice/deploy/docker/Instruction/Dockerfile之COPY复制文件.html","title":"Dockerfile之COPY复制文件","keywords":"","body":"Dockerfile之COPY复制文件Dockerfile之COPY复制文件 格式： COPY [--chown=:] ... COPY [--chown=:] [\"\",... \"\"] 和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。 示例 COPY ./target/chinahrss-auth.jar /chinahrss/chinahrss-auth.jar COPY 指令将从构建上下文目录中 的文件/目录（./target/chinahrss-auth.jar） 复制到 新的一层的镜像内的 位置（/chinahrss/chinahrss-auth.jar） "},"microservice/deploy/dockerCompose/DockerCompose入门.html":{"url":"microservice/deploy/dockerCompose/DockerCompose入门.html","title":"Docker Compose入门","keywords":"","body":"Docker Compose入门1. 简介1.1 Compose 定位:1.2 解决的问题与场景2. Compose 两个重要概念Docker Compose入门 1. 简介 Docker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速的部署分布式应用。 1.1 Compose 定位: 定义和运行多个 Docker 容器应用（Defining and running multi-container Docker applications），其前身是开源项目Fig 1.2 解决的问题与场景 我们知道使用一个Dockerfile模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个Web项目，除了Web服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足这样的需求，它允许用户通过一个单独的Docker-compose.yml 模板文件（YAML格式）来定义一组关联的应用容器为一个项目（project） 2. Compose 两个重要概念 服务（service）：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目（peoject）：由一组关联的应用容器组成的一个完整业务单元，在docker-compose.yml "},"microservice/deploy/Kubernetes/Kubeadm安装Kubernetes.html":{"url":"microservice/deploy/Kubernetes/Kubeadm安装Kubernetes.html","title":"Kubeadm安装Kubernetes","keywords":"","body":"Kubeadm安装Kubernetes1. 安装1.1 配置国内的kubernetes源1.2 安装kubelet、kubeadm和kubectl工具：1.3 启动kubelet并设置开机自启1.4 启动kubernetes 的master节点Kubeadm安装Kubernetes Kubernetes 从1.4 版本开始后引入了kubeadm 用于简化集群搭建的过程 1. 安装 1.1 配置国内的kubernetes源 在/etc/yum.repos.d/ 目录下新建kubernetes.repo 文件，内容如下 [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 1.2 安装kubelet、kubeadm和kubectl工具： yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 1.3 启动kubelet并设置开机自启 systemctl enable kubelet && systemctl start kubelet 1.4 启动kubernetes 的master节点 kubeadm init --kubernetes-version=v1.17.2 \\ --pod-network-cidr=10.244.0.0/16 \\ --service-cidr=10.1.0.0/16 \\ --apiserver-advertise-address=120.79.200.111 \\ --image-repository registry.aliyuncs.com/google_containers 配置含义如下： kubernetes-version: 用于指定k8s版本，这里指定为最新的1.16.2版本； apiserver-advertise-address：用于指定kube-apiserver监听的ip地址，就是master本机IP地址。 pod-network-cidr：因为后面我们选择flannel作为Pod的网络插件，所以这里需要指定Pod的网络范围为10.244.0.0/16 service-cidr：用于指定SVC的网络范围； image-repository: 其中默认的镜像仓库k8s.gcr.io没有科学上网的话无法访问，我们可以将它修改为国内的阿里镜像仓库registry.aliyuncs.com/google_containers 启动时，需要拉取镜像，过程比较缓慢耐心等待即可。如果你想先拉好镜像再启动，你可以使用kubeadm config images list命令列出需要拉取的镜像。 启动成功后，你会看到类似如下提示: "},"microservice/deploy/rancher/安装Rancher.html":{"url":"microservice/deploy/rancher/安装Rancher.html","title":"安装Rancher","keywords":"","body":"安装Rancher Server1. 启动 RANCHER SERVER - 单容器部署 (NON-HA)2. Rancher UI3. 添加集群4. 集群安装成功5. 切换Default项目视图6. 部署工作负载7. 部署完成参考文章安装Rancher Server 1. 启动 RANCHER SERVER - 单容器部署 (NON-HA) 在安装了Docker的Linux服务器上，使用一个简单的命令就可以启动一个单实例的Rancher。 安装 2.* 的版本 sudo docker run -d --restart=unless-stopped -p 8080:80 -p 443:443 rancher/rancher 本机8080端口映射到rancher 的80端口 2. Rancher UI 访问443 端口https://120.79.200.111/ 默认账号是admin，第一次需要设置密码 3. 添加集群 添加自定义集群 集群设置 集群设置二 由于只有一个节点，所有3个角色都要勾选 输入主机的外网和内网地址 ssh终端输入复制的命令 4. 集群安装成功 点击集群也能够看到集群的仪表盘信息了： 5. 切换Default项目视图 集群创建完成后，默认会生成Default项目，点击Default可以切换到项目视图： 6. 部署工作负载 在rancher里工作负载是一个对象，包括pod以及部署应用程序所需的其他文件和信息。我们这里以nginx作为demo示例，在Default视图下，点击工作负载—部署服务 在部署工作负载页面，设置工作负载名称、副本数量、镜像名称、命名空间、端口映射，其他参数保持默认，不设置端口映射的话，默认是随机映射端口，我这里选择随机，最后点击启动： 7. 部署完成 参考文章 安装 Rancher2.x 并部署工作负载 "},"microservice/deploy/repository/harbor/Docker镜像仓库Harbor.html":{"url":"microservice/deploy/repository/harbor/Docker镜像仓库Harbor.html","title":"Docker镜像仓库Harbor","keywords":"","body":"Docker镜像仓库Harbor1. 背景2. 简介3. 安装Harbor3.1 下载Harbor离线安装包3.2 解压Harbor3.3 修改Harbor 配置文件3.4 执行install.sh 脚本进行安装3.5 安装成功的提示信息3.6 浏览器访问查看4. 创建用户和项目4.1 在管理界面新增一个用户4.2 新增项目4.3 在该项目下添加用户5. 服务器上登录harbor5.1 添加私有仓库5.2 重启docker5.3 重启Harbor5.4 登录6. 测试镜像推拉6.1 从官方Docker Hub中拉取busybox镜像6.2 然后给该镜像打上标签：6.3 镜像推送到Harbor仓库：7 . 开启远程仓库7.1 修改Docker配置7.2 重启Docker服务7.3 验证2375端口是否通8. 构建Chinahrss服务Dokcer镜像8.1 新建远程Docker服务8.2 构建chinahrss-auth镜像8.3 点击构建9. 镜像推送Docker镜像仓库Harbor 1. 背景 我本地环境下（或者公司局域网），将Docker镜像推送到Docker Hub速度比较慢，推荐的做法就是安装一个第三方的Docker镜像仓库。这里推荐使用Harbor 2. 简介 Harbor是一框开源的Docker镜像存储仓库，其扩展了Docker Distribution，在此基础上添加了我们常用的功能，比如安全认证，RBAC用户权限管理，可视化页面操作等功能 3. 安装Harbor 3.1 下载Harbor离线安装包 wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.4.tgz 3.2 解压Harbor tar -xzvf harbor-offline-installer-v1.8.4.tgz 3.3 修改Harbor 配置文件 vim /software/harbor/harbor.yml 将hostname修改为宿主机IP和端口 端口8601 ip：120.79.200.111 3.4 执行install.sh 脚本进行安装 sh harbor/install.sh 如出现该异常 ERROR: for redis Cannot create container for service redis: Conflict. The container name \"/redis\" is already in use by container \"63ac1b31b56907c8e027ed847160edf6655380f34c3b944c31e745693e436764\". You have to remove (or rename) that container to be able to reuse that name. ERROR: Encountered errors while bringing up the project. 这里提示容器名冲突 解决步骤 查找redis容器 docker ps -a --filter name=redis 将他改名 docker rename redis micro-service-redis 3.5 安装成功的提示信息 3.6 浏览器访问查看 http://120.79.200.111:8601/ 为了方便日后管理，配置了域名解析 http://harbor.isture.com 默认的用户名：admin 密码：Harbor12345 登录后 4. 创建用户和项目 4.1 在管理界面新增一个用户 密码需要大小写混用Zs16 4.2 新增项目 4.3 在该项目下添加用户 5. 服务器上登录harbor 5.1 添加私有仓库 vi /etc/docker/daemon.json 添加如下内容 \"insecure-registries\": [\"harbor.isture.com\"] 5.2 重启docker service docker restart 因为Harbor的install.sh脚本实际上是基于Docker Compose的，所以重启Docker，Harbor也需要重启 5.3 重启Harbor sh /software/harbor/install.sh 5.4 登录 6. 测试镜像推拉 接着测试下，是否能够顺利的将Docker镜像推送到Harbor仓库中 6.1 从官方Docker Hub中拉取busybox镜像 docker pull busybox 6.2 然后给该镜像打上标签： docker tag busybox:latest harbor.isture.com/chinahrss/busybox:latest 标签格式为[docker仓库域名]/[项目名称]/[镜像:版本]。 6.3 镜像推送到Harbor仓库： 打好标签后，将 harbor.isture.com/chinahrss/busybox:latest 镜像推送到Harbor仓库： docker push harbor.isture.com/chinahrss/busybox:latest docker push harbor.isture.com/chinahrss/busybox:latest 7 . 开启远程仓库 我们约定的做法是，通过IDEA Docker插件在master远程构建Docker镜像，再将这些镜像推送到Harbor仓库 7.1 修改Docker配置 要开始Docker服务的远程访问，需要修改Docker配置 vi /lib/systemd/system/docker.service 修改的地方如下图所示 7.2 重启Docker服务 systemctl daemon-reload systemctl restart docker.service 7.3 验证2375端口是否通 curl 120.79.200.111:2375/info 如果返回如下JSON说明开启Docker远程服务成功，端口为2735： {\"ID\":\"W73X:KQSA:FEKB:5OT5:X4A5:AQCD:MKNX:VES4:EWPK:V3GJ:UIYC:35D3\",\"Containers\":69,\"ContainersRunning\":30,\"ContainersPaused\":0,\"ContainersStopped\":39,\"Images\":43,\"Driver\":\"overlay2\",\"DriverStatus\":[[\"Backing Filesystem\",\"extfs\"],[\"Supports d_type\",\"true\"],[\"Native Overlay Diff\",\"true\"]],\"SystemStatus\":null,\"Plugins\":{\"Volume\":[\"local\"],\"Network\":[\"bridge\",\"host\",\"ipvlan\",\"macvlan\",\"null\",\"overlay\"],\"Authorization\":null,\"Log\":[\"awslogs\",\"fluentd\",\"gcplogs\",\"gelf\",\"journald\",\"json-file\",\"local\",\"logentries\",\"splunk\",\"syslog\"]},\"MemoryLimit\":true,\"SwapLimit\":true,\"KernelMemory\":true,\"KernelMemoryTCP\":true,\"CpuCfsPeriod\":true,\"CpuCfsQuota\":true,\"CPUShares\":true,\"CPUSet\":true,\"PidsLimit\":true,\"IPv4Forwarding\":true,\"BridgeNfIptables\":true,\"BridgeNfIp6tables\":true,\"Debug\":false,\"NFd\":207,\"OomKillDisable\":true,\"NGoroutines\":186,\"SystemTime\":\"2020-01-30T14:31:37.954825827+08:00\",\"LoggingDriver\":\"json-file\",\"CgroupDriver\":\"cgroupfs\",\"NEventsListener\":0,\"KernelVersion\":\"3.10.0-957.21.3.el7.x86_64\",\"OperatingSystem\":\"CentOS Linux 7 (Core)\",\"OSType\":\"linux\",\"Architecture\":\"x86_64\",\"IndexServerAddress\":\"https://index.docker.io/v1/\",\"RegistryConfig\":{\"AllowNondistributableArtifactsCIDRs\":[],\"AllowNondistributableArtifactsHostnames\":[],\"InsecureRegistryCIDRs\":[\"127.0.0.0/8\"],\"IndexConfigs\":{\"docker.io\":{\"Name\":\"docker.io\",\"Mirrors\":[\"https://dockerhub.azk8s.cn/\",\"https://hub-mirror.c.163.com/\"],\"Secure\":true,\"Official\":true},\"harbor.isture.com\":{\"Name\":\"harbor.isture.com\",\"Mirrors\":[],\"Secure\":false,\"Official\":false}},\"Mirrors\":[\"https://dockerhub.azk8s.cn/\",\"https://hub-mirror.c.163.com/\"]},\"NCPU\":2,\"MemTotal\":8201396224,\"GenericResources\":null,\"DockerRootDir\":\"/var/lib/docker\",\"HttpProxy\":\"\",\"HttpsProxy\":\"\",\"NoProxy\":\"\",\"Name\":\"iZwz97t3ru69kye3l7uj70Z\",\"Labels\":[],\"ExperimentalBuild\":false,\"ServerVersion\":\"19.03.5\",\"ClusterStore\":\"\",\"ClusterAdvertise\":\"\",\"Runtimes\":{\"runc\":{\"path\":\"runc\"}},\"DefaultRuntime\":\"runc\",\"Swarm\":{\"NodeID\":\"\",\"NodeAddr\":\"\",\"LocalNodeState\":\"inactive\",\"ControlAvailable\":false,\"Error\":\"\",\"RemoteManagers\":null},\"LiveRestoreEnabled\":false,\"Isolation\":\"\",\"InitBinary\":\"docker-init\",\"ContainerdCommit\":{\"ID\":\"b34a5c8af56e510852c35414db4c1f4fa6172339\",\"Expected\":\"b34a5c8af56e510852c35414db4c1f4fa6172339\"},\"RuncCommit\":{\"ID\":\"3e425f80a8c931f88e6d94a8c831b9d5aa481657\",\"Expected\":\"3e425f80a8c931f88e6d94a8c831b9d5aa481657\"},\"InitCommit\":{\"ID\":\"fec3683\",\"Expected\":\"fec3683\"},\"SecurityOptions\":[\"name=seccomp,profile=default\"],\"Warnings\":[\"WARNING: API is accessible on http://0.0.0.0:2375 without encryption.\\n Access to the remote API is equivalent to root access on the host. Refer\\n to the 'Docker daemon attack surface' section in the documentation for\\n more information: https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface\"]} 8. 构建Chinahrss服务Dokcer镜像 8.1 新建远程Docker服务 点击IDEA -> File -> Settings… -> Build,Execution,Deployment -> Docker： 填写远程Docker地址，如果显示Connection successful说明连接远程Docker服务成功。 8.2 构建chinahrss-auth镜像 准备好后，点击IDEA -> Run -> Edit Configrations…，添加Docker配置： 选择Dockerfile文件地址，并且添加镜像标签为 harbor.isture.com/chinahrss/chinahrss-auth:latest ，然后保存即可： 8.3 点击构建 在Services->Master Docker Service->Images上，我们可以看到远程仓库上已经部署好了 我们可以到远程服务器上核心一下： docker images | grep chinahrss 这证明我们通过远程Docker服务构建镜像是OK的。 9. 镜像推送 这些镜像都构建好后，我们在服务器上将它们都推送到Harbor仓库，执行下面这条命令批量推送： for i in $(docker images | grep harbor.isture.com | awk 'BEGIN{OFS=\":\"}{print $1,$2}'); do docker push $i; done "},"microservice/monitor/SpringBootAdmin/SpringBootActuator监控应用.html":{"url":"microservice/monitor/SpringBootAdmin/SpringBootActuator监控应用.html","title":"Spring Boot Actuator监控应用","keywords":"","body":"Spring Boot Actuator监控应用1. 背景2. Actuator监控 简介3. Actuator 的 REST 接口3.1 监控分类3.2 原生端点3.3 Actuator提供的13个接口4. 快速上手4.1 添加依赖4.2 配置文件5. 命令详解5.1 监控点5.2 定制监控路径5.3 health5.4 info5.5 beans5.6 conditions5.7 heapdump5.8 shutdown5.9 mappings5.10 threaddump参考文章Spring Boot Actuator监控应用 1. 背景 微服务的特定决定了功能模块的部署是分布式的，大部分功能模块都是运行在不同的机器上，彼此通过服务调用进行交互，前后台的业务流会经过很多个微服务的处理和传递，出现了异常如何快速定位是哪个环节出现了问题？ 2. Actuator监控 简介 Spring Boot 使用“习惯有配置的理念”，采用包扫描和自动化配置的机制来加载依赖jar 中的Spring bean，不需要任何Xml配置，就可以实现Spring 的所有配置。虽然这样做能让我们的代码变得非常间接，但是整个应用的实例创建和依赖关系等信息都被离散到各个配置类的注解是哪个，这使得我们分析整个应用中资源和实例的各种关系变得非常的困难。 Actuator 是 Spring Boot 提供的对应用系统的自省和监控的集成功能，可以查看应用配置的详细信息、例如自动化配置信息、创建的Spring beans以及一些环境属性等 3. Actuator 的 REST 接口 3.1 监控分类 原生端点 用户自定义端点 自定义端点主要是指扩展性、用户可以根据自己的实际应用，定义一些比较关心的指标，在运行期进行监控 3.2 原生端点 原生端点是应用程序里提供众多Web接口，通过他们了解应用程序运行时的内部情况。原生端点又可以分成三类： 应用配置类： 可以查看应用在运行期的静态信息，例如自动配置信息，加载的Springbean信息、yml文件的配置信息、环境信息、请求映射信息 度量指标类： 需要是运行期的动态信息，例如堆栈、请求链、一些健康指标、metrics信息等 操作控制类 主要是指shutdown，用户可以发送一个请求将应用的监控功能关系 3.3 Actuator提供的13个接口 HTTP 方法 路径 描述 GET /auditevents 显示应用暴露的审计事件 (比如认证进入、订单失败) GET /beans 描述应用程序上下文里全部的 Bean，以及它们的关系 GET /conditions 就是 1.0 的 /autoconfig ，提供一份自动配置生效的条件情况，记录哪些自动配置条件通过了，哪些没通过 GET /configprops 描述配置属性(包含默认值)如何注入Bean GET /env 获取全部环境属性 GET /env/{name} 根据名称获取特定的环境属性值 GET /flyway 提供一份 Flyway 数据库迁移信息 GET /liquidbase 显示Liquibase 数据库迁移的纤细信息 GET /health 报告应用程序的健康指标，这些值由 HealthIndicator 的实现类提供 GET /heapdump dump 一份应用的 JVM 堆信息 GET /httptrace 显示HTTP足迹，最近100个HTTP request/repsponse GET /info 获取应用程序的定制信息，这些信息由info打头的属性提供 GET /logfile 返回log file中的内容(如果 logging.file 或者 logging.path 被设置) GET /loggers 显示和修改配置的loggers GET /metrics 报告各种应用程序度量信息，比如内存用量和HTTP请求计数 GET /metrics/{name} 报告指定名称的应用程序度量值 GET /scheduledtasks 展示应用中的定时任务信息 GET /sessions 如果我们使用了 Spring Session 展示应用中的 HTTP sessions 信息 POST /shutdown 关闭应用程序，要求endpoints.shutdown.enabled设置为true GET /mappings 描述全部的 URI路径，以及它们和控制器(包含Actuator端点)的映射关系 GET /threaddump 获取线程活动的快照 4. 快速上手 4.1 添加依赖 org.springframework.boot spring-boot-starter-web org.springframework.boot spring-boot-starter-actuator 4.2 配置文件 info.app.name=spring-boot-actuator info.app.version= 1.0.0 info.app.test=test management.endpoints.web.exposure.include=* management.endpoint.health.show-details=always #management.endpoints.web.base-path=/monitor management.endpoint.shutdown.enabled=true management.endpoints.web.base-path=/monitor 代表启用单独的url地址来监控 Spring Boot 应用，为了安全一般都启用独立的端口来访问后端的监控信息 management.endpoint.shutdown.enabled=true 启用接口关闭 Spring Boot 配置完成之后，启动项目就可以继续验证各个监控功能了。 5. 命令详解 在 Spring Boot 2.x 中为了安全起见，Actuator 只开放了两个端点 /actuator/health 和 /actuator/info。可以在配置文件中设置打开。 5.1 监控点 打开所有的监控点 management.endpoints.web.exposure.include=* 也可以选择打开部分 management.endpoints.web.exposure.exclude=beans,trace 5.2 定制监控路径 Actuator 默认所有的监控点路径都在/actuator/*，当然如果有需要这个路径也支持定制。 management.endpoints.web.base-path=/manage 设置完重启后，再次访问地址就会变成/manage/* 5.3 health health 主要用来检查应用的运行状态，这是我们使用最高频的一个监控点。通常使用此接口提醒我们应用实例的运行状态，以及应用不”健康“的原因，比如数据库连接、磁盘空间不够等。 默认情况下 health 的状态是开放的，添加依赖后启动项目，访问：http://localhost:8080/actuator/health即可看到应用的状态。 { \"status\" : \"UP\" } 默认情况下，最终的 Spring Boot 应用的状态是由 HealthAggregator 汇总而成的，汇总的算法是： 1 设置状态码顺序：setStatusOrder(Status.DOWN, Status.OUT_OF_SERVICE, Status.UP, Status.UNKNOWN);。 2 过滤掉不能识别的状态码。 3 如果无任何状态码，整个 Spring Boot 应用的状态是 UNKNOWN。 4 将所有收集到的状态码按照 1 中的顺序排序。 5 返回有序状态码序列中的第一个状态码，作为整个 Spring Boot 应用的状态。 health 通过合并几个健康指数检查应用的健康情况。Spring Boot Actuator 有几个预定义的健康指标比如DataSourceHealthIndicator, DiskSpaceHealthIndicator, MongoHealthIndicator, RedisHealthIndicator等，它使用这些健康指标作为健康检查的一部分。 举个例子，如果你的应用使用 Redis，RedisHealthindicator 将被当作检查的一部分；如果使用 MongoDB，那么MongoHealthIndicator 将被当作检查的一部分。 可以在配置文件中关闭特定的健康检查指标，比如关闭 redis 的健康检查： management.health.redise.enabled=false 默认，所有的这些健康指标被当作健康检查的一部分。 5.4 info info 就是我们自己配置在配置文件中以 info 开头的配置信息，比如我们在示例项目中的配置是： info.app.name=spring-boot-actuator info.app.version= 1.0.0 info.app.test= test 启动示例项目，访问：http://localhost:8080/actuator/info返回部分信息如下： { \"app\": { \"name\": \"spring-boot-actuator\", \"version\": \"1.0.0\", \"test\":\"test\" } } 5.5 beans 根据示例就可以看出，展示了 bean 的别名、类型、是否单例、类的地址、依赖等信息。 启动示例项目，访问：http://localhost:8080/actuator/beans返回部分信息如下： [ { \"context\": \"application:8080:management\", \"parent\": \"application:8080\", \"beans\": [ { \"bean\": \"embeddedServletContainerFactory\", \"aliases\": [ ], \"scope\": \"singleton\", \"type\": \"org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory\", \"resource\": \"null\", \"dependencies\": [ ] }, { \"bean\": \"endpointWebMvcChildContextConfiguration\", \"aliases\": [ ], \"scope\": \"singleton\", \"type\": \"org.springframework.boot.actuate.autoconfigure.EndpointWebMvcChildContextConfiguration$$EnhancerBySpringCGLIB$$a4a10f9d\", \"resource\": \"null\", \"dependencies\": [ ] } } ] 5.6 conditions Spring Boot 的自动配置功能非常便利，但有时候也意味着出问题比较难找出具体的原因。使用 conditions 可以在应用运行时查看代码了某个配置在什么条件下生效，或者某个自动配置为什么没有生效。 启动示例项目，访问：http://localhost:8080/actuator/conditions返回部分信息如下： { \"positiveMatches\": { \"DevToolsDataSourceAutoConfiguration\": { \"notMatched\": [ { \"condition\": \"DevToolsDataSourceAutoConfiguration.DevToolsDataSourceCondition\", \"message\": \"DevTools DataSource Condition did not find a single DataSource bean\" } ], \"matched\": [ ] }, \"RemoteDevToolsAutoConfiguration\": { \"notMatched\": [ { \"condition\": \"OnPropertyCondition\", \"message\": \"@ConditionalOnProperty (spring.devtools.remote.secret) did not find property 'secret'\" } ], \"matched\": [ { \"condition\": \"OnClassCondition\", \"message\": \"@ConditionalOnClass found required classes 'javax.servlet.Filter', 'org.springframework.http.server.ServerHttpRequest'; @ConditionalOnMissingClass did not find unwanted class\" } ] } } } 5.7 heapdump 返回一个 GZip 压缩的 JVM 堆 dump 启动示例项目，访问：http://localhost:8080/actuator/heapdump会自动生成一个 Jvm 的堆文件 heapdump，我们可以使用 JDK 自带的 Jvm 监控工具 VisualVM 打开此文件查看内存快照。类似如下图： 5.8 shutdown 开启接口优雅关闭 Spring Boot 应用，要使用这个功能首先需要在配置文件中开启： management.endpoint.shutdown.enabled=true 配置完成之后，启动示例项目，使用 curl 模拟 post 请求访问 shutdown 接口。 shutdown 接口默认只支持 post 请求。 curl -X POST \"http://localhost:8080/actuator/shutdown\" { \"message\": \"Shutting down, bye...\" } 此时你会发现应用已经被关闭。 5.9 mappings 描述全部的 URI 路径，以及它们和控制器的映射关系 启动示例项目，访问：http://localhost:8080/actuator/mappings返回部分信息如下： { \"/**/favicon.ico\": { \"bean\": \"faviconHandlerMapping\" }, \"{[/hello]}\": { \"bean\": \"requestMappingHandlerMapping\", \"method\": \"public java.lang.String com.neo.controller.HelloController.index()\" }, \"{[/error]}\": { \"bean\": \"requestMappingHandlerMapping\", \"method\": \"public org.springframework.http.ResponseEntity> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)\" } } 5.10 threaddump /threaddump 接口会生成当前线程活动的快照。这个功能非常好，方便我们在日常定位问题的时候查看线程的情况。 主要展示了线程名、线程ID、线程的状态、是否等待锁资源等信息。 启动示例项目，访问：http://localhost:8080/actuator/threaddump返回部分信息如下： [ { \"threadName\": \"http-nio-8088-exec-6\", \"threadId\": 49, \"blockedTime\": -1, \"blockedCount\": 0, \"waitedTime\": -1, \"waitedCount\": 2, \"lockName\": \"java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1630a501\", \"lockOwnerId\": -1, \"lockOwnerName\": null, \"inNative\": false, \"suspended\": false, \"threadState\": \"WAITING\", \"stackTrace\": [ { \"methodName\": \"park\", \"fileName\": \"Unsafe.java\", \"lineNumber\": -2, \"className\": \"sun.misc.Unsafe\", \"nativeMethod\": true }, ... { \"methodName\": \"run\", \"fileName\": \"TaskThread.java\", \"lineNumber\": 61, \"className\": \"org.apache.tomcat.util.threads.TaskThread$WrappingRunnable\", \"nativeMethod\": false } ... ], \"lockInfo\": { \"className\": \"java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject\", \"identityHashCode\": 372286721 } } ... ] 生产出现问题的时候，可以通过应用的线程快照来检测应用正在执行的任务。 参考文章 Spring Boot (十九)：使用 Spring Boot Actuator 监控应用 "},"microservice/monitor/SpringBootAdmin/SpringBootAdmin入门.html":{"url":"microservice/monitor/SpringBootAdmin/SpringBootAdmin入门.html","title":"Spring Boot Admin入门指南","keywords":"","body":"Spring Boot Admin入门指南1. Spring Boot Actuator2. Spring Boot Admin3. Spring Boot Admin 的组成4. Server端集成使用4.1 POM依赖4.2 启动类中加入注解@EnableAdminServer5. Client 集成使用5.1 POM 依赖5.2 指定 Admin Server的url5.3 添加Actuator6. 安全配置6.1 Client安全6.2 Server安全参考文章Spring Boot Admin入门指南 1. Spring Boot Actuator Actuator 是Spring Boot 的模块，他在应用中添加了REST/JMS 端点，方便监控和管理应用。端点提供了监控检测、指标监控、访问日志、线程转储、堆转储和环境信息等等。 2. Spring Boot Admin Actuator功能强大，便于其他应用使用端点（只需要简单的REST调用）。但是开发人员使用时就没那么方便，对于开发人员，有良好的交互界面会更方便浏览监控数据和管理应用。这正是Spirng Boot Admin做的工作，他的actuator端点提供了良好的交互界面，并提供了额外的特性 3. Spring Boot Admin 的组成 Client Client部分是包含被监控应用中，并注册到Admin Server Server Server部分包括 Admin用户界面并独立运行于被监控应用 所以应用挂掉欧哲不能正常运行，监控的Server依然正常运行。假如你有多个应用（比如Spirng boot微服务应用），每个应用运行多个实例。对于传统的Actuator监控，很难单独访问每个应用，因为你要跟踪有多少实例及他们在哪运行 对于Spring Boot Admin，被监控应用的每个实例（client）在启动时注册到Server，每个实例在Admin Server 就有一个单点，就可以检查他们的状态了 4. Server端集成使用 4.1 POM依赖 de.codecentric spring-boot-admin-starter-server 2.1.0 4.2 启动类中加入注解@EnableAdminServer 启动类中加入注解@EnableAdminServer来开启Admin Server。 @EnableAdminServer @SpringBootApplication public class ChinahrssMonitorAdminApplication { public static void main(String[] args) { SpringApplication.run(ChinahrssMonitorAdminApplication.class, args); } } 现在运行程序并在浏览器打开http://localhost:8080/，可以看到如下界面： Server运行正常，但是没有Client 注册 5. Client 集成使用 5.1 POM 依赖 和Server一样，第一步是向新建Client工程添加依赖： de.codecentric spring-boot-admin-starter-client 2.2.0 5.2 指定 Admin Server的url 然后指定运行的Admin Server的url，即在application.properties中添加： spring.boot.admin.client.url=http://localhost:8080 5.3 添加Actuator org.springframework.boot spring-boot-starter-actuator 大部分的端点默认是不对外暴露的，所以需要在application.properties添加配置使它们暴露： management.endpoints.web.exposure.include=* 暴露Actuator端点后就可以在Admin Server上看到更多的信息了。 6. 安全配置 现在所有服务都能正常运行，但是我们要保证Actuator端点和Admin管理界面的安全性 6.1 Client安全 如果你已经使用了Spring Security，上面的内容不会起作用，因为Actuator端点默认是被保护的，Admin Server 不能访问他们。如果没有使用Spring Security，首先需要添加依赖 org.springframework.boot spring-boot-starter-security 为了方便测试，可以配置management.security.enabled=false临时禁用Actuator端点的保护。如果使用基本身份认证，需要在配置文件中提供用户名和密码。Admin Server使用这些凭证来认证Client的Actuator端点。 spring.boot.admin.client.instance.metadata.user.name=client spring.boot.admin.client.instance.metadata.user.password=client 默认情况下，如果没有配置Spring Boot使用默认用户user并在应用启动时自动生成密码。启动时你可以在控制台找到密码。如果你要在应用中明确指定用户名和密码，可以在配置文件中配置： spring.security.user.name=client spring.security.user.password=client 6.2 Server安全 和Client一样，在Admin Server添加依赖： org.springframework.boot spring-boot-starter-security 在application.properties配置登录Admin Server的用户名和密码： spring.security.user.name=server spring.security.user.password=server 然后在Client端也要添加这些凭证，否则不能注册到server。 。 spring.boot.admin.client.username=server spring.boot.admin.client.password=server 回到Server模块，最后一件事是添加Spring Security配置来保证Admin管理界面的安全性。 @Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { SavedRequestAwareAuthenticationSuccessHandler successHandler = new SavedRequestAwareAuthenticationSuccessHandler(); successHandler.setTargetUrlParameter(\"redirectTo\"); successHandler.setDefaultTargetUrl(\"/\"); http.authorizeRequests() .antMatchers(\"/assets/**\").permitAll() .antMatchers(\"/login\").permitAll() .anyRequest().authenticated().and() .formLogin().loginPage(\"/login\") .successHandler(successHandler).and() .logout().logoutUrl(\"/logout\").and() .httpBasic().and() .csrf() .csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse()) .ignoringAntMatchers( \"/instances\", \"/actuator/**\" ); } } 这段代码的作用是：限制只有通过HTTP基本认证和登录用户可以使用Admin管理界面。登录界面和静态资源（javascript, HTML, CSS）是公开的，否则无法登录。它是基于cookie的CSRF保护。你可以看到在CSRF保护中有些路径被忽略了，因为Admin Server缺少适当的支持。 重启Server，可以看到更加美观的登录界面。 参考文章 Spring Boot Admin入门指南 "},"microservice/action/中国人社微服务实践.html":{"url":"microservice/action/中国人社微服务实践.html","title":"chinahrss微服务实践","keywords":"","body":"中国人社微服务实践1. 技术考虑1.11.2 网关1.3 授权认证服务1.4 系统服务1.5 监控服务1.5 基础服务中国人社微服务实践 1. 技术考虑 1.1 1.2 网关 网关主要考虑zuul和Spring Cloud Gateway Q: 为什么选Spring Cloud Gateway? A: 支持限流和非阻塞API,基于Filter 链很容易扩展 Q: 你主要用网关做了什么 A: 通过动态路由转发到对应的服务 负载均衡（以lb:// 开头） 全局过滤器： 认证授权：设置统一的请求头，其他微服务通过请求头来判断是否走了网关（） 请求之前，打印请求日志（自定义GlobalFilter） 过滤掉那些禁止外部调用的地址（自定义GlobalFilter） 对验证码进行限流（使用阿里Sentinel 做的网关限流） 对网关请求异常做统一处理 对服务断路器做降级处理（Hystrix） 跨域设置 1.3 授权认证服务 Q: 你授权认证服务具体都做写什么 实现OAuth2 认证（通过spring security 和 spring oauth） 配置一个认证服务器（AuthorizationServer） 和 资源服务器（ResourceServer） 认证服务器（AuthorizationServer） 客户端详情（ClientDetailsServiceConfigurer）通过JdbcClientDetailsService来读取数据库中的client 信息，并将它们保存在redis中 令牌端点(Token Endpoint)的安全约束(AuthorizationServerSecurityConfigurer)，本应用暂没配置，用Spring Security来配置 配置端点接入 AuthorizationServerEndpointsConfigurer TokenStore保存token之类的管理操作，包含配置了RedisTokenStore和JwtTokenStore 配置userDetailsService管理用户信息，从数据库中查出用户信息，校验用户权限 配置 认证管理器（authenticationManager），Grant Type设置为password，因为设置了密码 自定义异常处理exceptionTranslator 验证码生成与校验 验证码过滤器 将所有HTTP请求拦截，获取请求头，从请求头中获取clientId 判断请求是否为 /oauth/token，或者一些要认证的接口 转发到验证码验证 验证通过再请求对应的请求 注册、登录查找用户信息等 第三方登录 授权客户端管理（客户端包括，app，swagger，前端） Q: 如何确定当前登录用户 通过SecurityContextHolder存储安全上下文，就能知道当前的用户是谁，是否已经被认证，用户拥有哪些权限。 原理：通过ThreadLocal 来存储认证信息 Authentication authentication = SecurityContextHolder.getContext().getAuthentication(); Object principalObj = authentication.getPrincipal(); 注：principalObj 为ChinahrssAuthUser， 它实现了org.springframework.security.core.userdetails.user 接口 Q:在什么时候添加请求token 在刷新token和登录的时候，才添加请求头 认证token。 这个认证的token 是包含了client_id和client_secret,通过Base64加密 // 获取令牌时，请求头信息(Basic Base64.encode(client_id:client_secret)) authorizationValue: 'Basic ZmViczoxMjM0NTY=', 1.4 系统服务 Q: 系统服务服务的主要做什么？ 登录，记录登录日志，更新登录时间 获取用户显示菜单和对应的路由 获取用户首页相关显示信息 部门：增删改查导出 用户：增删改查导出 角色：增删改查导出 菜单：增删改查导出 操作日志：对关键操作增加操作日志，添加自定义注解（@ControllerEndpoint ）进行操作日志的添加 应用登录日志：增删查导出 线程池实例 ThreadPoolTaskExecutor Q: 如何做分页 依赖mybatisplus 继承ServiceImpl 通过page方法查询分页数据 QueryWrapper queryWrapper = new QueryWrapper<>(); queryWrapper.lambda().eq(Log::getUsername, log.getUsername().toLowerCase()); Page page = new Page<>(request.getPageNum(), request.getPageSize()); this.page(page, queryWrapper); 1.5 监控服务 使用Spring Boot Actuator 和 Spring Boot Admin监控微服务的状态 1.5 基础服务 通过拦截器将未经过网关的请求拦截（ implements HandlerInterceptor） 供各个微服务 security 的资源服务的 没有权限访问该资源情况 (implements AccessDeniedHandler) Token 无效的情况（ implements AuthenticationEntryPoint ） 全局的异常处理（@RestControllerAdvice） 1.5.1 链路追踪 RabbitMQ用于收集Sleuth提供的追踪信息， 然后zipkin Server 从RabbitMQ里获取 1.5.2 日志 sql日志打印 P6spy输出最终数据库sql日志 日志收集 使用ELK 做对各微服务做日志收集 "},"microservice/action/chinahrss微服务Docker化.html":{"url":"microservice/action/chinahrss微服务Docker化.html","title":"chinahrss微服务Docker化","keywords":"","body":"chinahrss微服务Docker化1.项目打包2. 服务器创建相关目录3. 对文件夹授权4. 将jar上传到服务器5. 构建Docker镜像chinahrss微服务Docker化 1.项目打包 项目根目录下执行 mvn clean package -Dmaven.test.skip=true 每个微服务（除了聚合模块）target目录下会有个可运行jar包，比如chinahrss-auth模块： 2. 服务器创建相关目录 mkdir -p /chinahrss/chinahrss-auth /chinahrss/chinahrss-gateway \\ /chinahrss/chinahrss-monitor-admin /chinahrss/zipkin-server /chinahrss/chinahrss-register \\ /chinahrss/chinahrss-server-system /chinahrss/chinahrss-server-test 3. 对文件夹授权 chmod 777 -R /chinahrss 4. 将jar上传到服务器 将文件上传到对应目录下，如本地的chinahrss-auth-0.0.1-SNAPSHOT.jar 就上传到 /chinahrss/chinahrss-auth 上传好后，运行一下目录，看是否都上传成功： ls /chinahrss/chinahrss-auth /chinahrss/chinahrss-gateway \\ /chinahrss/chinahrss-monitor-admin /chinahrss/zipkin-server /chinahrss/chinahrss-register \\ /chinahrss/chinahrss-server-system /chinahrss/chinahrss-server-test 5. 构建Docker镜像 在 /chinahrss/chinahrss-auth 模块下创建一个Dockerfile 内容如下所示： FROM openjdk:8u212-jre MAINTAINER zsz 312905679@qq.com COPY chinahrss-auth-0.0.1-SNAPSHOT.jar /chinahrss/chinahrss-auth-0.0.1-SNAPSHOT.jar ENTRYPOINT [\"java\", \"-Xmx256m\", \"-jar\", \"/chinahrss/chinahrss-auth-1.0-SNAPSHOT.jar\"] 上面脚本包含4条命令： FROM openjdk:8u212-jre 表示由openjdk:8u212-jre基础镜像构建。因为我们的项目使用的是JDK 1.8，所以我们要依赖于1.8版本的JDK镜像构建，openjdk官方Docker镜像仓库为https://hub.docker.com/_/openjdk?tab=tags，我挑了个体积相对较小的openjdk:8u212-jre； MAINTAINER zsz 312905679@qq.com 指定镜像的作者及联系方式； COPY chinahrss-auth-1.0-SNAPSHOT.jar /chinahrss/chinahrss-auth-1.0-SNAPSHOT.jar 表示将当前目录（/chinahrss/chinahrss-auth）下的chinahrss-auth-1.0-SNAPSHOT.jar拷贝到openjdk:8u212-jre镜像里的/chinahrss目录下，名称也为chinahrss-auth-0.0.1-SNAPSHOT.jar； ENTRYPOINT [\"java\", \"-Xmx256m\", \"-jar\", \"/febs/febs-auth-1.0-SNAPSHOT.jar\"] 表示运行java -jar运行镜像里的jar包，JVM内存最大分配为256m（因为要运行的微服务较多并且虚拟机内存只有6GB，所以内存分配小了点，实际可以根据宿主服务器的配置做出相应调整）。 DockerFile创建好后，在/chinahrss/chinahrss-auth目录下运行以下命令构建镜像： docker build -t chinahrss-auth:v1 . 第一次构建的时候因为要下载openjdk:8u212-jre镜像，所以速度较慢，耐心等待即可，构建好后终端输出如下所示： 查看当前镜像： docker images "},"microservice/action/chinahrss使用DockerCompose部署.html":{"url":"microservice/action/chinahrss使用DockerCompose部署.html","title":"chinahrss使用Docker Compose部署","keywords":"","body":"chinahrss使用Docker Compose部署2. 部署第三方服务2.1 创建存储第三方服务Docker Compose文件目录：2.2 新建docker-compose.yml文件2.3 创建挂载目录2.4 创建Redis配置文件redis.conf：2.5 运行docker-compose2.6 连接测试2.6.1 Mysql连接测试2.6.2 连接Redis2.6.3 连接RabbitMQ测试3. 部署 nacos3.1 新建docker-compose.yml3.2 新建对应的数据库3.3 启动nacos3.3 浏览nacos3. 部署微服务3.1 新建Docker Compose文件chinahrss使用Docker Compose部署 我们使用Docker Compose 一键部署我们的微服务。主要分为三部分: 部署ELK 部署第三方服务（MySQL、Redis和RabbitMQ） 部署自己编写的微服务 2. 部署第三方服务 2.1 创建存储第三方服务Docker Compose文件目录： mkdir -p /chinahrss/third-part 2.2 新建docker-compose.yml文件 在该目录下新建一个docker-compose.yml文件： vim /chinahrss/third-part/docker-compose.yml 内容如下 version: '3' services: mysql: image: mysql:5.7.24 container_name: mysql environment: MYSQL_ROOT_PASSWORD: 123456 ports: - 13306:3306 volumes: - /chinahrss/mysql/data:/var/lib/mysql #挂载 MySQL数据 redis: image: redis:4.0.14 container_name: redis command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes volumes: - /chinahrss/redis/data:/data #挂载 Redis数据 - /chinahrss/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf #挂载 Redis配置 ports: - 16379:6379 rabbitmq: image: rabbitmq:3.7.15-management container_name: rabbitmq volumes: - /chinahrss/rabbitmq/data:/var/lib/rabbitmq #挂载 RabbitMQ数据 - /chinahrss/rabbitmq/log:/var/log/rabbitmq #挂载 RabbitMQ日志 ports: - 5673:5672 - 15673:15672 因为原有的端口都被占用了，我改了端口。 mysql：13306 Redis：16379 rabbitmq：5673 和 15673 2.3 创建挂载目录 mkdir -p /chinahrss/mysql/data /chinahrss/redis/data /chinahrss/redis/conf \\ /chinahrss/rabbitmq/data /chinahrss/rabbitmq/log 2.4 创建Redis配置文件redis.conf： touch /chinahrss/redis/conf/redis.conf 因为我使用的是Redis默认配置，所以并没有在该配置文件里编写任何内容。 2.5 运行docker-compose 准备完毕后，将目录切换到/chinahrss/third-part，然后运行docker-compose up -d启动： 2.6 连接测试 2.6.1 Mysql连接测试 使用Navicat连接MySQL 连接成功 创建数据库 最后导入chinahrss_cloud_base SQL和zipkin相关SQL，导入后库表如下所示： 2.6.2 连接Redis 使用Redis Desktop Manager看看能否成功连接Redis： 2.6.3 连接RabbitMQ测试 浏览器访问 http://120.79.200.111:15673/ 默认用户名和密码都为guest。在Admin标签页里新建一个用户，用户名为chinahrss，密码为123456，角色为管理员： 然后对chinahrss用户授权，点击chinahrss用户： 点击Set Permission按钮： 至此第三方服务都准备完毕，接下来开始部署我们自己的微服务。 3. 部署 nacos 3.1 新建docker-compose.yml 在/chinahrss/nacos 目录下新建docker-compose.yml文件 vim /chinahrss/nacos/docker-compose.yml 内容如下 version: '3' services: nacos: image: nacos/nacos-server:latest container_name: nacos-standalone-mysql environment: - \"PREFER_HOST_MODE=hostname\" - \"MODE=standalone\" - \"SPRING_DATASOURCE_PLATFORM=mysql\" - \"MYSQL_MASTER_SERVICE_HOST=120.79.200.111\" - \"MYSQL_MASTER_SERVICE_DB_NAME=chinahrss_nacos\" - \"MYSQL_MASTER_SERVICE_PORT=13306\" - \"MYSQL_MASTER_SERVICE_USER=root\" - \"MYSQL_MASTER_SERVICE_PASSWORD=123456\" - \"MYSQL_SLAVE_SERVICE_HOST=120.79.200.111\" - \"MYSQL_SLAVE_SERVICE_PORT=13306\" volumes: - /chinahrss/nacos/standalone-logs/:/home/nacos/logs ports: - 8001:8848 restart: on-failure 注意事项 mysql 的数据库必须要两个主从，两个一样也可以 端口我改成了8001 3.2 新建对应的数据库 3.3 启动nacos docker-compose up -d 3.3 浏览nacos http://120.79.200.111:8001/nacos/ 3. 部署微服务 3.1 新建Docker Compose文件 mkdir -p /chinahrss/chinahrss-cloud vim /chinahrss/chinahrss-cloud/docker-compose.yml 其内容如下所示： version: '3' services: febs-register: image: febs-register:latest # 指定基础镜像，就是上一节中我们自己构建的镜像 container_name: febs-register # 容器名称 volumes: - \"/febs/log:/log\" #日志挂载 command: - \"--febs-register=192.168.33.10\" # 通过command指定地址变量值 - \"--febs-monitor-admin=192.168.33.10\" ports: - 8001:8001 # 端口映射 febs-monitor-admin: image: febs-monitor-admin:latest container_name: febs-monitor-admin volumes: - \"/febs/log:/log\" ports: - 8401:8401 febs-gateway: image: febs-gateway:latest container_name: febs-gateway depends_on: - febs-register volumes: - \"/febs/log:/log\" command: - \"--febs-register=192.168.33.10\" - \"--febs-monitor-admin=192.168.33.10\" ports: - 8301:8301 febs-auth: image: febs-auth:latest container_name: febs-auth depends_on: - febs-register volumes: - \"/febs/log:/log\" command: - \"--mysql.url=192.168.33.10\" - \"--redis.url=192.168.33.10\" - \"--febs-register=192.168.33.10\" - \"--febs-monitor-admin=192.168.33.10\" febs-server-system: image: febs-server-system:latest container_name: febs-server-system depends_on: - febs-register volumes: - \"/febs/log:/log\" command: - \"--mysql.url=192.168.33.10\" - \"--rabbitmq.url=192.168.33.10\" - \"--febs-register=192.168.33.10\" - \"--febs-monitor-admin=192.168.33.10\" - \"--febs-gateway=192.168.33.10\" febs-server-test: image: febs-server-test:latest container_name: febs-server-test depends_on: - febs-register volumes: - \"/febs/log:/log\" command: - \"--rabbitmq.url=192.168.33.10\" - \"--febs-register=192.168.33.10\" - \"--febs-monitor-admin=192.168.33.10\" - \"--febs-gateway=192.168.33.10\" zipkin-server: image: zipkin-server container_name: zipkin-server command: - \"--server.port=8402\" - \"--zipkin.storage.type=mysql\" - \"--zipkin.storage.mysql.db=febs_cloud_base\" - \"--zipkin.storage.mysql.username=root\" - \"--zipkin.storage.mysql.password=123456\" - \"--zipkin.storage.mysql.host=192.168.33.10\" - \"--zipkin.storage.mysql.port=3306\" - \"--zipkin.collector.rabbitmq.addresses=192.168.33.10:5672\" - \"--zipkin.collector.rabbitmq.username=febs\" - \"--zipkin.collector.rabbitmq.password=123456\" ports: - 8402:8402 "},"rbac/":{"url":"rbac/","title":"权限管理系统","keywords":"","body":"权限管理系统权限管理系统 RBAC权限管理 创建数据库 权限认证基础 shiro shiro基础 SpringBoot整合shiro Shiro与JWT整合 jwt JWT详解 "},"rbac/RBAC权限管理.html":{"url":"rbac/RBAC权限管理.html","title":"RBAC权限管理","keywords":"","body":"RBAC权限管理1. 简介1.1 角色是什么1.2 权限是什么2. 用户分组（扩展）3. 权限的划分（扩展）4. RBAC 权限模型扩展模型完整设计RBAC权限管理 1. 简介 RBAC（Role-Based Access Control, 基于角色的访问控制），就是用户通过角色与权限进行关联。简单地说，一个用户拥有若干角色，每一个角色拥有若干权限。 这样就构成“用户-角色-权限”的授权模型，这种模型中，用户与角色，角色与权限之间，是多对多关系 1.1 角色是什么 可以理解为一定数量的权限的集合，权限的载体。 例如：一个论坛讨论系统，“超级管理员”，“版主”都是角色 1.2 权限是什么 版主可管理版内的帖子，可管理版内的用户等，这些就是权限 要给某个用户授予这些权限，不需要直接将权限授予用户，可将“版主”这个角色赋予用户 2. 用户分组（扩展） 用用户的数量非常大时，要给系统每个用户逐一授权（授角色），是件非常繁琐的事情，这时，就需要给用户分组，每个用户组内有多个用户，除了可给用户授权外，还可以给用户组授权。这样一来，用户拥有的所有权限，就是用户个人拥有的权限与该用户组拥有的权限之和 3. 权限的划分（扩展） 在应用系统中，权限表现成什么？对功能模块的操作，对上传文件的删改，菜单的访问，甚至是页面的按钮、某个图片的可见性控制，都可属于权限的范畴。 权限划分 功能操作作为一类， 而把文件，菜单、页面元素作为一类。 这样就构成了“用户-角色-权限-资源的授权模型”。而在做数据建模时，可把功能操作和资源统一管理。也就是都直接与权限进行关联，这样更具有便捷性和易扩展性 小项目并不一定要按此划分，直接权限表就可以了，此方案是为了更好的扩展性 注意：请留意权限表中有一列“权限类型”，我们根据它的取值来区分是哪一类权限，如“MENU”表示菜单的访问权限、“OPERATION”表示功能模块的操作权限、“FILE”表示文件的修改权限、“ELEMENT”表示页面元素的可见性控制等。 好处： 其一，不需要区分哪些是权限操作，哪些是资源，（实际上，有时候也不好区分，如菜单，把它理解为资源呢还是功能模块权限呢？） 其二，方便扩展，当系统要对新的东西进行权限控制时，我只需要建立一个新的关联表“权限XX关联表”，并确定这类权限的权限类型字符串 4. RBAC 权限模型扩展模型完整设计 "},"rbac/db/":{"url":"rbac/db/","title":"创建数据库","keywords":"","body":"创建数据库用户表的创建创建数据库 用户表的创建 create table sys_user ( id bigint not null auto_increment comment '用户id', user_name VARCHAR(50) comment '用户名' , user_password varchar(50) comment '密码', user_email varchar(50) comment '邮箱', user_info text comment '简介', head_img blob comment '头像', create_time datetime comment '创建时间', primary key (id) ); alter table sys_user comment '用户表'; "},"rbac/base/权限认证基础.html":{"url":"rbac/base/权限认证基础.html","title":"权限认证基础","keywords":"","body":"权限认证基础1. 认证（Authentication）和授权（Authorization）的区别？2. 什么是Token？什么是JWT?如何基于Token进行身份验证？2.1 为什么要使用Token/session 的缺点2.2 什么是JWT2.3 JWT 工作流程3. 什么是OAuth 2.0?权限认证基础 1. 认证（Authentication）和授权（Authorization）的区别？ 这是一个很多人都会混淆的问题。 简单点： 认证（Authentication）:你是谁 授权（Authorization）：你有权限干什么 正式点： Authentication（认证）：是验证您的身份的凭据（例如用户名/用户ID和密码），通过这个凭据，系统得以知道你是谁。也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证 /** * 用户认证 * * @param authenticationToken 身份认证 token * @return AuthenticationInfo 身份认证信息 * @throws AuthenticationException 认证相关异常 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { String username = (String) token.getPrincipal(); if (!\"javaboy\".equals(username)) { throw new UnknownAccountException(\"账户不存在!\"); } return new SimpleAuthenticationInfo(username, \"123\", getName()); } Authorization（授权）：发生在Authentication(认证)之后。授权主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问，比如admin，有些对系统资源操作比如删除，添加、更新只能特定人才有 /** * 授权模块，获取用户角色和权限 * * @param token token * @return AuthorizationInfo 权限信息 */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection token) { String username = JWTUtil.getUsername(token.toString()); SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); // 获取用户角色集 Set roleSet = userManager.getUserRoles(username); simpleAuthorizationInfo.setRoles(roleSet); // 获取用户权限集 Set permissionSet = userManager.getUserPermissions(username); simpleAuthorizationInfo.setStringPermissions(permissionSet); return simpleAuthorizationInfo; } 2. 什么是Token？什么是JWT?如何基于Token进行身份验证？ 2.1 为什么要使用Token/session 的缺点 session 保存在内存中，随着使用者增多，开销大 分布式 session 还会面临session 共享问题 CSRF: cookie 如果被拦截，使用者就会很容易受到跨站伪造请求的攻击 我们知道session 可以用来鉴别用户的身份，我们Session 信息需要保存一份在服务端。这种方式会带来一些麻烦，比如需要我们保证保存session 信息的服务器的可用性，不适合移动端（依赖Cookie）等等 移动端有Cookie，但还是需要手动实现cookie持久化。默认是非持久化生命周期跟app保持一致。还需要再写拦截器每次封装进请求header 有没有一种不需要自己存放 Session 信息就能实现身份验证的方式呢？使用 Token 即可！JWT （JSON Web Token） 就是这种方式的实现，通过这种方式服务器端就不需要保存 Session 数据了，只用在客户端保存服务端返回给客户的 Token 就可以了，扩展性得到提升。 2.2 什么是JWT JWT（JSON Web Token） 本质上就一段签名的 JSON 格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性。 JWT 由3部分构成 Header：描述JWT 的元数据。定义了生成签名的算法以及token的类型 Payload（负载）：用来存放实际需要传递的数据 Signature（签名）：服务器通过Payload、Header 和一个密钥（secret）使用Header里面指定的算法签名（默认是HMAC SHA256）生成 在基于 Token 进行身份验证的的应用程序中，服务器通过Payload、Header和一个密钥(secret)创建令牌（Token）并将 Token 发送给客户端，客户端将 Token 保存在 Cookie 或者 localStorage 里面，以后客户端发出的所有请求都会携带这个令牌。你可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP Header 的 Authorization字段中：Authorization: Bearer Token。 2.3 JWT 工作流程 用户向服务器发送用户名和密码用于登陆系统 身份验证服务响应并返回了签名的 JWT，上面包含了用户是谁的内容 用户以后每次想后端发请求都在Header中带上 JWT 服务端检查JWT 并从中获取用户相关信息 3. 什么是OAuth 2.0? OAuth 是行业的标准授权协议，主要用来授权第三方应用获取有限的权限。 而 OAuth 2.0是对 OAuth 1.0 的完全重新设计，OAuth 2.0更快，更容易实现，OAuth 1.0 已经被废弃 实际上他就是一种授权机制，他的最终目的是为第三方应用颁发一个有时效性的令牌token。使得第三方应用能够通过该令牌获取相关资源 使用场景 第三方登录： 当你的网站接入了第三方登录的时候一般就是使用的 OAuth 2.0 协议。 "},"rbac/shiro/shiro基础.html":{"url":"rbac/shiro/shiro基础.html","title":"shiro基础","keywords":"","body":"shiro基础1. 架构2. 实现Realmeshiro基础 1. 架构 黄色部分为shiro 内部 使用用户的登录信息创建令牌 UsernamePasswordToken token = new UsernamePasswordToken(username, password); token 可以理解为用户令牌，登录的过程被抽象为Shiro验证令牌是否具有合法身份以及相关权限 执行登录动作 SecurityUtils.setSecurityManager(securityManager); // 注入SecurityManager Subject subject = SecurityUtils.getSubject(); // 获取Subject单例对象 subject.login(token); // 登陆 SecurityManager: Shiro的核心部分是SecurityManager，它负责安全认证与授权。Shiro本身已经实现了所有的细节，用户可以完全把它当做一个黑盒来使用 SecurityUtils对象 本质上就是一个工厂类似Spring中的ApplicationContext Subject 肤浅一点可以理解为User 官方翻译：项目。它是你目前所设计的需要通过Shiro保护的项目的一个抽象概念 通过令牌（token）与项目（subject）的登陆（login）关系，Shiro保证了项目整体的安全。 判断用户 Shiro本身无法知道所持有令牌的用户是否合法，因为除了项目的设计人员恐怕谁都无法得知。 因此Realm是整个框架中为数不多的必须由设计者自行实现的模块，当然Shiro提供了多种实现的途径 数据库 redis缓存中取 AuthenticationInfo&AuthorizationInfo 用户具有角色和权限两种基本属性 AuthenticationInfo：代表用户角色信息集合 AuthorizationInfo：代表了角色的权限信息集合 2. 实现Realme Realm就是提供AuthorizationInfo和AuthenticationInfo这两个混淆概念的地方 public class UserRealm extends AuthorizingRealm { // 用户对应的角色信息与权限信息都保存在数据库中，通过UserService获取数据 private UserService userService = new UserServiceImpl(); /** * 提供用户信息返回权限信息 */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { String username = (String) principals.getPrimaryPrincipal(); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); // 根据用户名查询当前用户拥有的角色 Set roles = userService.findRoles(username); Set roleNames = new HashSet(); for (Role role : roles) { roleNames.add(role.getRole()); } // 将角色名称提供给info authorizationInfo.setRoles(roleNames); // 根据用户名查询当前用户权限 Set permissions = userService.findPermissions(username); Set permissionNames = new HashSet(); for (Permission permission : permissions) { permissionNames.add(permission.getPermission()); } // 将权限名称提供给info authorizationInfo.setStringPermissions(permissionNames); return authorizationInfo; } /** * 提供账户信息返回认证信息 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { String username = (String) token.getPrincipal(); User user = userService.findByUsername(username); if (user == null) { // 用户名不存在抛出异常 throw new UnknownAccountException(); } if (user.getLocked() == 0) { // 用户被管理员锁定抛出异常 throw new LockedAccountException(); } SimpleAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(user.getUsername(), user.getPassword(), ByteSource.Util.bytes(user.getCredentialsSalt()), getName()); return authenticationInfo; } } "},"rbac/shiro/SpringBoot整合shiro.html":{"url":"rbac/shiro/SpringBoot整合shiro.html","title":"SpringBoot整合shiro","keywords":"","body":"SpringBoot整合shiro1. 集成步骤1.1 创建Spring Boot项目1.2 创建Realm1.3 配置Shiro基本信息1.4 配置 ShiroConfig1.5 配置 controller2. 测试参考文章SpringBoot整合shiro 1. 集成步骤 1.1 创建Spring Boot项目 创建一个SpringBoot 项目，添加Web依赖 同时添加shiro 依赖 org.apache.shiro shiro-spring-boot-web-starter 1.4.0 1.2 创建Realm import org.apache.shiro.authc.*; import org.apache.shiro.authz.AuthorizationInfo; import org.apache.shiro.realm.AuthorizingRealm; import org.apache.shiro.subject.PrincipalCollection; /** * @author zhangshengzhong * @date 2019/10/17 */ public class MyRealm extends AuthorizingRealm { @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { return null; } @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { String username = (String) token.getPrincipal(); if (!\"javaboy\".equals(username)) { throw new UnknownAccountException(\"账户不存在!\"); } return new SimpleAuthenticationInfo(username, \"123\", getName()); } } 1.3 配置Shiro基本信息 在application.properties 中配置shiro 基本信息 # 是否允许将sessionId 放到 cookie 中 shiro.sessionManager.sessionIdCookieEnabled=true # 是否允许将 sessionId 放到 Url 地址拦中 shiro.sessionManager.sessionIdUrlRewritingEnabled=true # 访问未获授权的页面时，默认的跳转路径 shiro.unauthorizedUrl=/unauthorizedurl # 开启 shiro shiro.web.enabled=true # 登录成功的跳转页面 shiro.successUrl=/index # 登录页面 shiro.loginUrl=/login 1.4 配置 ShiroConfig @Configuration public class ShiroConfig { @Bean MyRealm myRealm() { return new MyRealm(); } @Bean DefaultWebSecurityManager securityManager() { DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(myRealm()); return manager; } @Bean ShiroFilterChainDefinition shiroFilterChainDefinition() { DefaultShiroFilterChainDefinition definition = new DefaultShiroFilterChainDefinition(); definition.addPathDefinition(\"/doLogin\", \"anon\"); definition.addPathDefinition(\"/**\", \"authc\"); return definition; } } 1.5 配置 controller @RestController public class LoginController { @PostMapping(\"/doLogin\") public String doLogin(String username, String password) { Subject subject = SecurityUtils.getSubject(); try { subject.login(new UsernamePasswordToken(username, password)); return \"登录成功!\"; } catch (AuthenticationException e) { e.printStackTrace(); return \"登录失败!\"; } } @GetMapping(\"/hello\") public String hello() { return \"hello\"; } @GetMapping(\"/login\") public String login() { return \"please login!\"; } } 2. 测试 请求http://localhost:8080/hello由于未登录，所以会自动跳转到 /login 接口 调用doLogin完成登录 再次调用/hello，就可以成功访问了： 参考文章 Spring Boot 整合 Shiro ，两种方式全总结！ "},"rbac/shiro/Shiro与JWT整合.html":{"url":"rbac/shiro/Shiro与JWT整合.html","title":"Shiro与JWT整合","keywords":"","body":"Shiro与JWT整合1. 添加依赖2. JWTUtil工具类2.1 JWTUtil 执行的时机3. 配置ShiroRealm4. ShiroConfig 配置类5. JWTFilter 配置过滤器5.1 token是怎么获取并设置进来的？6. JWTTokenShiro与JWT整合 1. 添加依赖 org.apache.shiro shiro-spring-boot-web-starter 1.4.0 com.auth0 java-jwt 3.4.1 2. JWTUtil工具类 该工具类主要负责 生成JWT token 验证 token 从token 中获取用户名 public class JWTUtil { /** * 默认超时 一天 */ private static final long EXPIRE_TIME = 86400 * 1000; /** * 从token 中获取用户名 * @param token * @return */ public static String getUsername(String token) { try { DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(\"username\").asString(); } catch (JWTDecodeException e) { e.printStackTrace(); return null; } } /** * 生成JWT token * @param username * @param secret * @return */ public static String sign(String username, String secret) { try { username = username.toLowerCase(); Date date = new Date(System.currentTimeMillis() + EXPIRE_TIME); Algorithm algorithm = Algorithm.HMAC256(secret); return JWT.create() .withClaim(\"username\", username) .withExpiresAt(date) .sign(algorithm); } catch (Exception e) { log.error(\"error：{}\", e); return null; } } /** * 验证token * @param token * @param username * @param secret * @return */ public static boolean verify(String token, String username, String secret) { try { Algorithm algorithm = Algorithm.HMAC256(secret); JWTVerifier verifier = JWT.require(algorithm) .withClaim(\"username\", username) .build(); verifier.verify(token); log.info(\"token is valid\"); return true; } catch (Exception e) { e.printStackTrace(); log.info(\"token is invalid{}\", e.getMessage()); return false; } } } 2.1 JWTUtil 执行的时机 生成token时机（JWTUtil.sign()） 在登录成功后，并将生成的token 再进行一次加密，保存在redis 中。并返回给前端页面 从token 中获取用户名（JWTUtil.getUsername(token)） 在ShiroRealm 中的两个方法中都能获取到token 授权模块 doGetAuthorizationInfo 用户认证 doGetAuthenticationInfo 验证token 时机（JWTUtil.verify()） 在用户认证模块，通过token 获取到用户名，再将用户名获取用户信息。在调用验证接口 3. 配置ShiroRealm @Slf4j public class ShiroRealm extends AuthorizingRealm{ @Autowired private UserManager userManager; @Autowired private RedisService redisService; /** * 授权模块，获取用户角色和权限 * @param token * @return */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection token) { String username = JWTUtil.getUsername(token.toString()); SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); // 获取用户角色集 Set roleSet = userManager.getUserRoles(username); simpleAuthorizationInfo.setRoles(roleSet); // 获取用户权限集 Set permissionSet = userManager.getUserPermissions(username); simpleAuthorizationInfo.setStringPermissions(permissionSet); return simpleAuthorizationInfo; } /** * 用户认证 * @param authenticationToken 身份认证token * @return AuthenticationInfo 身份认证信息 * @throws AuthenticationException */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { // 这里的 token 是从 JWTFilter 的 executeLogin 方法传递过来的，已经经过了解密 String token = (String) authenticationToken.getCredentials(); // 从 redis 里获取这个token String encryptToken = FebsUtil.encryptToken(token); String encryptTokenInRedis = null; try { encryptTokenInRedis = redisService.get(TOKEN_CACHE_PREFIX + encryptToken ); } catch (Exception ignore) { log.error(\"token 保存失败\"); ignore.printStackTrace(); } // 如果找不到，说明已经失效 if (StringUtils.isBlank(encryptTokenInRedis)) throw new AuthenticationException(\"token已经过期\"); String username = JWTUtil.getUsername(token); if (StringUtils.isBlank(username)) throw new AuthenticationException(\"token校验不通过\"); // 通过用户名查询用户信息 User user = userManager.getUser(username); if (user == null) throw new AuthenticationException(\"用户名或密码错误\"); if (!JWTUtil.verify(token, username, user.getPassword())) throw new AuthenticationException(\"token校验不通过\"); return new SimpleAuthenticationInfo(token, token, \"febs_shiro_realm\"); } } 4. ShiroConfig 配置类 @Configuration public class ShiroConfig { @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean(SecurityManager securityManager){ ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 设置 securityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 在 Shiro 过滤器链上加入 JWTFilter LinkedHashMap filters = new LinkedHashMap<>(); filters.put(\"jwt\",new JWTFilter()); shiroFilterFactoryBean.setFilters(filters); LinkedHashMap filterChainDefinitionMap = new LinkedHashMap<>(); // 所有请求都要经过 jwt过滤器 filterChainDefinitionMap.put(\"/**\", \"jwt\"); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return shiroFilterFactoryBean; } @Bean public SecurityManager securityManager(){ DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 配置 SecurityManager，并注入 shiroRealm securityManager.setRealm(shiroRealm()); return securityManager; } @Bean private Realm shiroRealm() { // 配置 Realm return new ShiroRealm(); } } 5. JWTFilter 配置过滤器 所有的页面都走过滤器，然后自己设置哪些页面不需要过滤 @Slf4j public class JWTFilter extends BasicHttpAuthenticationFilter { private static final String TOKEN = \"Authentication\"; private AntPathMatcher pathMatcher = new AntPathMatcher(); @Override protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws UnauthorizedException { HttpServletRequest httpServletRequest = (HttpServletRequest) request; FebsProperties febsProperties = SpringContextUtil.getBean(FebsProperties.class); String[] anonUrl = StringUtils.splitByWholeSeparatorPreserveAllTokens(febsProperties.getShiro().getAnonUrl(), \",\"); boolean match = false; for (String u : anonUrl) { String requestURI = httpServletRequest.getRequestURI(); if (pathMatcher.match(u,requestURI )) match = true; } if (match) return true; if (isLoginAttempt(request, response)) { return executeLogin(request, response); } return false; } @Override protected boolean isLoginAttempt(ServletRequest request, ServletResponse response) { HttpServletRequest req = (HttpServletRequest) request; String token = req.getHeader(TOKEN); return token != null; } @Override protected boolean executeLogin(ServletRequest request, ServletResponse response) { HttpServletRequest httpServletRequest = (HttpServletRequest) request; String token = httpServletRequest.getHeader(TOKEN); JWTToken jwtToken = new JWTToken(FebsUtil.decryptToken(token)); try { getSubject(request, response).login(jwtToken); return true; } catch (Exception e) { log.error(e.getMessage()); return false; } } /** * 对跨域提供支持 */ @Override protected boolean preHandle(ServletRequest request, ServletResponse response) throws Exception { HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; httpServletResponse.setHeader(\"Access-control-Allow-Origin\", httpServletRequest.getHeader(\"Origin\")); httpServletResponse.setHeader(\"Access-Control-Allow-Methods\", \"GET,POST,OPTIONS,PUT,DELETE\"); httpServletResponse.setHeader(\"Access-Control-Allow-Headers\", httpServletRequest.getHeader(\"Access-Control-Request-Headers\")); // 跨域时会首先发送一个 option请求，这里我们给 option请求直接返回正常状态 if (httpServletRequest.getMethod().equals(RequestMethod.OPTIONS.name())) { httpServletResponse.setStatus(HttpStatus.OK.value()); return false; } return super.preHandle(request, response); } } 5.1 token是怎么获取并设置进来的？ 在过滤器中从Header中获取，并封装成JWTToken，并执行 getSubject(request, response).login(jwtToken); @Override protected boolean executeLogin(ServletRequest request, ServletResponse response) { HttpServletRequest httpServletRequest = (HttpServletRequest) request; String token = httpServletRequest.getHeader(TOKEN); JWTToken jwtToken = new JWTToken(FebsUtil.decryptToken(token)); try { getSubject(request, response).login(jwtToken); return true; } catch (Exception e) { log.error(e.getMessage()); return false; } } 6. JWTToken /** * JSON Web Token */ @Data public class JWTToken implements AuthenticationToken { private static final long serialVersionUID = 1282057025599826155L; private String token; private String exipreAt; public JWTToken(String token) { this.token = token; } public JWTToken(String token, String exipreAt) { this.token = token; this.exipreAt = exipreAt; } @Override public Object getPrincipal() { return token; } @Override public Object getCredentials() { return token; } } "},"rbac/SpringSecurity/SpringBoot整合SpringSecurity.html":{"url":"rbac/SpringSecurity/SpringBoot整合SpringSecurity.html","title":"SpringBoot整合Spring Security","keywords":"","body":"SpringBoot整合Spring Security1. 准备工作2. 整合Spring Security3. 新增登录请求与页面参考文章SpringBoot整合Spring Security 1. 准备工作 构建一个简单的Web工程，用于后续添加安全控制 Web层实现请求映射 @Controller public class HelloController { @RequestMapping(\"/\") public String index() { return \"index\"; } @RequestMapping(\"/hello\") public String hello() { return \"hello\"; } } /：映射到index.html /hello：映射到hello.html 实现映射的页面 src/main/resources/templates/index.html Spring Security入门 欢迎使用Spring Security! 点击 这里 打个招呼吧 src/main/resources/templates/hello.html Hello World! Hello world! index.html中提供到/hello的链接，显然在这里没有任何安全控制，所以点击链接后就可以直接跳转到hello.html页面。 2. 整合Spring Security 在这一节，我们将对/hello页面进行权限控制，必须是授权用户才能访问。当没有权限的用户访问后，跳转到登录页面。 添加依赖 在pom.xml中添加如下配置，引入对Spring Security的依赖。 ... org.springframework.boot spring-boot-starter-security ... Spring Security配置 创建Spring Security的配置类WebSecurityConfig，具体如下： @Configuration @EnableWebSecurity public class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .antMatchers(\"/\", \"/home\").permitAll() .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/login\") .permitAll() .and() .logout() .permitAll(); } @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { auth .inMemoryAuthentication() .withUser(\"user\").password(\"password\").roles(\"USER\"); } } 通过@EnableWebSecurity 注解开启Spring Security 的功能 继承WebSecurityConfigurerAdapter，并重写它的方法来设置一些web安全的细节 configure(HttpSecurity http) 方法 通过authorizeRequests()定义哪些URL需要被保护、哪些不需要被保护。例如以上代码指定了/和/home不需要任何认证就可以访问，其他的路径都必须通过身份验证。 通过formLogin()定义当需要用户登录时候，转到的登录页面。 configureGlobal(AuthenticationManagerBuilder auth)方法，在内存中创建了一个用户，该用户的名称为user，密码为password，用户角色为USER。 3. 新增登录请求与页面 在HelloController 中新增/login 请求映射到login.html @Controller public class HelloController { // 省略之前的内容... @RequestMapping(\"/login\") public String login() { return \"login\"; } } 新增登录页面：src/main/resources/templates/login.html Spring Security Example 用户名或密码错 您已注销成功 用户名 : 密 码 : 这里简单的通过用户名和密码提交到/login的登录方式 根据配置，Spring Security 提供了一个过滤器来拦截请求并验证用户身份。如果用户身份认证失败，请求就重定向到/login?erroer,并且页面中会展现相应的错误信息，若用户想要注销登录，可以通过访问/login?logout请求，在完成注销之后，页面展现相应的功能消息 到这里，我们启用应用，并访问http://localhost:8080/，可以正常访问。但是访问http://localhost:8080/hello的时候被重定向到了http://localhost:8080/login页面，因为没有登录，用户没有访问权限，通过输入用户名user和密码password进行登录后，跳转到了Hello World页面，再也通过访问http://localhost:8080/login?logout，就可以完成注销操作。 参考文章 Spring Boot中使用Spring Security进行安全控制 "},"rbac/SpringSecurity/SpringSecurity入门.html":{"url":"rbac/SpringSecurity/SpringSecurity入门.html","title":"Spring Security入门","keywords":"","body":"Spring Security入门参考文章Spring Security入门 参考文章 Spring Security零基础入门之一 Spring Security(一)--Architecture Overview "},"rbac/oauth2/OAuth2基础.html":{"url":"rbac/oauth2/OAuth2基础.html","title":"OAuth2基础","keywords":"","body":"OAuth2基础1. 简介2. 应用场景3. 名词定义4. OAuth的思路5. 运行流程6. 客户端的授权模式6.1 授权码模式6.2 简化模式6.3 密码模式6.4 客户端模式7. 更新令牌参考文章OAuth2基础 1. 简介 OAuth 是一个关于授权（authorization）的开放网络标准，在全世界得到广泛应用，目前的版本是2.0版。 2. 应用场景 我们举个例子 有一个“云冲印”的网站，可以将用户存在Google 的招聘，冲印出来。用户为了使用该服务，必须让“云冲印”读取自己存储在Goodle上的照片 问题是只有得到用户的授权，google 才会同意“云冲印”读取这些招聘，那么，“云冲印”怎样获得用户的授权呢？ 传统方法是，用户将自己的Google用户名和密码，告诉“云冲印”，后者就可以读取用户的照片了。但这样的做法有以下几个严重的缺点 “云冲印”为了后序的服务，会保存用户的密码，这样很不安全 google 不得不部署密码登录，而我们知道，单纯的密码登录并不安全 “云冲印”拥有了获取用户存储在Google所有资料的权利，用户没法限制“云冲印”获得授权的范围和有效期 用户只有修改密码。才能收回赋予“云冲印”的权力，但是这样做，会使得其他所有获得用户授权的第三方应用程序全部失效 只要有一个第三方应用程序被破解，就会导致用户密码泄露，以及所有被密码保护的数据泄露 3. 名词定义 Third-party application: 第三方应用程序，本文又称“客户端”（client），既上一节的“云冲印” Http service：Http服务器提供商，本文中简称“服务提供商”，既上一节例子中的Google Resource owner: 资源所有者，本文中又称“用户”（user） User Agent：用户代理，本文中就是指浏览器 Authorization server: 认证服务器，既服务提供商专门用来处理认证的服务器 Resource server: 资源服务器，既服务提供商存放用户生成资源的服务器。他与认证服务器，可以是同一台服务器，也可以是不同的服务器 4. OAuth的思路 OAuth在“客户端”与“服务提供商”之间，设置了一个授权层（authorization layer）。“客户端”不能直接登录“服务提供商”，只能登录授权层，以此将用户与客户端区分开来。“客户端”登录授权层所用的令牌（token），与用户的密码不同，用户可以在登录的时候，指定授权层令牌的权限范围和有效期 “客户端”登录授权层以后。“服务提供商”根据令牌的权限范围和有效期，向“客户端”开发用户存储的资料 5. 运行流程 OAuth 2.0 的运行流程如下图 用户打开客户端以后，客户端要求用于给予授权 用户同意给予客户端授权 客户端使用上一步获得的授权，向认证服务器申请令牌 认证服务器对客户端进行认证以后，确认无误，同意发放令牌 客户端使用令牌，向资源服务器申请获取资源 资源服务器确认令牌无误，同意向客户端开放资源 通过上面6个步骤，可以看出2是关键。既用户怎样才能给予客户端授权，有了这个授权以后，客户端就可以获取令牌，进而凭令牌获取资源 6. 客户端的授权模式 客户端必须得到用户的授权（authorization grant）,才能获得令牌（access token）。OAuth 2.0 定义了四种授权方式 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credential） 6.1 授权码模式 授权码模式（authorization code）是功能最完整、流程最验密的授权模式。他的特点就是通过客户端的后台服务器，与“服务提供商”的认证服务器进行互动 他的步骤如下： 用户访问客户端，后者将前者导向认证服务器 用户选择是否给予客户端授权 假设用户给予授权，认证服务器将用户导向客户端事先指定的“重定向URL”(redirection URL),同时附上一个授权码 客户端收到授权码，附上早先的“重定向URI”,向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。 认证服务器核对授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token） 6.2 简化模式 简化模式（implicit grant type）不通过第三方应用服务器，直接在浏览器中想认证服务器申请令牌，跳过了“授权码”这个步骤。所有步骤都在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证 客户端将用户导向认证服务器 用户决定是否给予客户端授权 假设用户给予授权，认证服务器将用户导向客户端指定的“重定向URI”,并在URI 的Hash部分包含了访问令牌 浏览器想资源服务器发出请求，其中不包括上一步收到的Hash值 资源服务区返回一个网页，其中包含的代码可以获取Hash值中的令牌 浏览器执行上一步获得的脚本，提取出令牌 浏览器将令牌发给客户端 6.3 密码模式 密码模式（Resource Owner Password Credentials Grant）中，用于向客户端提供自己的用户名和密码。客户端使用这些信息，向“服务商提供商”索要授权 在这种模式中，用户必须要把自己的密码给客户端，但是客户端不得存储密码。这通常用在用户对客户端高度信任的情况下。比如客户端是操作系统的一部分，或者由一个著名公司出品。而认证服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式 用户向客户端提供用户名和密码 客户端将用户名和密码发送给认证服务器，向后者请求令牌 认证服务器确认无误后，向客户端提供访问令牌 6.4 客户端模式 客户端模式（Client Credentials Grant）指客户端以自己的名义，而不是以用户的名义，向“服务提供商”进行认证。严格的说，客户端模式并不属于OAuth框架所有解决的问题。在这种模式中，用户直接向客户端注册，客户端以自己的名义要求“服务提供商”，提供服务，其实不存在授权问题。 客户端向认证服务器进行身份认证，并要求一个访问令牌 认证服务器确认无误后，向客户端提供访问令牌 7. 更新令牌 如果用户访问的时候，客户端的“访问令牌”已经过期，则需要使用“更新令牌”申请一个新的访问令牌 参考文章 理解OAuth 2.0 "},"rbac/jwt/JWT详解.html":{"url":"rbac/jwt/JWT详解.html","title":"JWT详解","keywords":"","body":"JWT详解什么是JWT2. JWT 安全吗？3. JWT Payload 內容可以被偽造嗎？参考文章JWT详解 什么是JWT JWT（JSON Web Token） 本质上就一段签名的 JSON 格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性。 JWT 由3部分构成 例如： xxxxx.yyyyy.zzzzz Header：描述JWT 的元数据。定义了生成签名的算法以及token的类型 { 'typ':'JWT', //宣告型別，這裡是jwt 'alg':'HS256' //宣告加密的演算法，通常直接使用HMAC SHA256或RSA } Header部分的JSON被Base64Url編碼，形成JWT的第一部分。 eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 Payload（负载）：用来存放实际需要传递的数据 定义了三种宣告 Registered claims（註冊宣告）: 這些是一組預先定義的宣告，它們不是強制性的，但推薦使用，以提供一組有用的，可互操作的宣告。 其中一些是：iss（發行者），exp（到期時間），sub（主題），aud（受眾）等。 Public claims（公開宣告）: 這些可以由使用JWT的人員隨意定義。 但為避免衝突，應在IANA JSON Web令牌登錄檔中定義它們，或將其定義為包含防衝突名稱空間的URI。 Private claims（私有宣告）: 這些是為了同意使用它們但是既沒有登記，也沒有公開宣告的各方之間共享資訊，而建立的定製宣告。 示例（這裡存放的即使一個使用者的資訊，沒有到期時間、主題之類的宣告）： 例如这是我实际存储的值 {\"exp\":1571370401,\"username\":\"zsz\"} Playload部分的JSON被Base64Url編碼，形成JWT的第二部分。 eyJleHAiOjE1NzEzNzA0MDEsInVzZXJuYW1lIjoienN6In0 此部分信息尽管受到篡改保护，但是任何人都可以阅读。这部分不要放任何关键信息 Signature（签名）：服务器通过Payload、Header 和一个密钥（secret）使用Header里面指定的算法签名（默认是HMAC SHA256）生成 如果你想使用HMAC SHA256演算法，簽名將按照以下方式建立： HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload) + \".\" + secret) 2. JWT 安全吗？ Base64 编码方式是可逆的，也就是透过编码后开放的token 内容是可以被解析的。一般我们不能放敏感信息，如密码等 3. JWT Payload 內容可以被偽造嗎？ JWT 其中一个组成内容是Signature，可以防止通过Base64可逆回推有效负荷内容，因为signature 是你Header和payload一起base64组成的 参考文章 通俗易懂版講解JWT和OAuth2，以及他倆的區別和聯絡（Token鑑權解決方案） "},"test/":{"url":"test/","title":"测试","keywords":"","body":"单元测试单元测试 单元测试 Junit Junit常见注解和执行顺序 Mock Mockito Mockito基本功能 Mockito使用案例 Mockito原理 Mockito单测service "},"test/单元测试.html":{"url":"test/单元测试.html","title":"单元测试","keywords":"","body":"单元测试什么是单元测试哪些地方需要些单元测试为什么要写单元测试单元测试 什么是单元测试 单元测试又称模块测试，是针对程序模块（软件设计中最小单元）来进行正确性检验的测试工作 ​ -----------维基百科 单元测试是正确性校验的测试工作 哪些地方需要些单元测试 在Dao层，Service层中的每一个方法都确保单元测试通过。 在重要的算法，核心业务逻辑必须单元测试通过 为什么要写单元测试 提高代码质量 提高代码可读性 对自己代码的一种负责 …. "},"test/junit/":{"url":"test/junit/","title":"Junit","keywords":"","body":"JunitJunit "},"test/junit/Junit常见注解和执行顺序.html":{"url":"test/junit/Junit常见注解和执行顺序.html","title":"Junit常见注解和执行顺序","keywords":"","body":"Junit常见注解和执行顺序常见注解执行顺序Junit常见注解和执行顺序 常见注解 @Before： 初始化方法在任何一个测试方法执行之前，必须执行的代码 @After： 释放资源，在如何一个测试方法执行之后，需要进行的收尾工作 @Test 测试方法：表明这是一个测试方法，在Junit中将会自动执行 @Ignore: 忽略的测试方法：标注的含义就是“尚未完成。不参与测试”，这样测试就是忽略而不是失败 @BeforeClass： 针对所有测试，也就是整个测试类中，在所有测试方法执行前，都会先执行他的注解方法，而且只执行一次 修饰符必须是 public static void xxx @AfterClass: 针对所有测试，也就是整个测试类中，在所有测试方法都执行完之后，才会执行的的注解方法，而且只会执行一次啊 修饰符必须是public static void xxx 执行顺序 @BeforeClass -> @Befor-> @Test ->@Ater ->@AfterClass "},"test/mock/":{"url":"test/mock/","title":"Mock","keywords":"","body":"Mock1.mock 是什么1.为什么要使用mock1.1相对于Junit的优势参考Mock 1.mock 是什么 mock一词指效仿，模仿 在单元测试里，mock用来构造一个替身。 这个替身主要用于作为被测试类的依赖关系的替代 1.为什么要使用mock 因为在做单元测试的时候，我们要测试的方法会引用很多外部依赖的对象，比如（发送邮箱，网络通讯，记录Log）。而我们没办法控制这些外部依赖的对象 1.1相对于Junit的优势 Junit是单元测试框架，可以轻松完成关联关系少或者比较简单的类的单元测试。但对于关联到其他比较复杂的类或者对运行环境有要求的类的单元测试。模拟环境或者配置环境会非常耗时，实施单元测试比较困难 注：不是不能做，是模拟环境比较耗时 mock的优势 可以通过mock框架模拟一个对象的行为，从而隔离我们不关心的其他对象，使得测试变得简单 例如 service调用dao，既service依赖dao，我们可以通过mock dao来模拟真实的dao调用，从而能达到测试service 的目的 模拟对象（Mock Object） 可以取代真实对象的位置，用于测试一些与真实对象进行交互或依赖于真实对象的功能，模拟对象的背后目的就是创建一个轻量级的，可控制的对象来代替测试中需要的真实对象，模拟真实对象的行为和功能 参考 从百变怪Mockito到单元测试 "},"test/mock/mockito/":{"url":"test/mock/mockito/","title":"Mockito","keywords":"","body":"MockitoMockito "},"test/mock/mockito/Mockito基本功能.html":{"url":"test/mock/mockito/Mockito基本功能.html","title":"Mockito基本功能","keywords":"","body":"Mockito基本功能1.对象“复制”2.技能复制2.1指定返回值3.验证Mockito基本功能 1.对象“复制” mockito可以轻易复制出各种类型的对象，并与之进行交互 // 列表 List mockList = mock(List.class); mockList.add(1); mockList.clear(); // Socket对象 Socket mockSocket = mock(Socket); mockSocket.connect(new InetSocketAddress(8080)); mockSocket.close(); 2.技能复制 List mockList = mock(List.class); mockList.add(1); // 简单交互 mockList.get(1); // 返回值为null mockList.size(); // 返回值为0 虽然复制出来的对象上所有的方法都能被调用，只会返回默认的返回值 需要返回对象：默认返回null 需要返回int：默认返回0 2.1指定返回值 List mockList = mock(List.class); when(mockList.get(anyInt()).thenReturn(1); when(mockList.size()).thenReturn(1, 2, 3); assertEquals(\"预期返回1\", 1, mockList.get(1)); // pass assertEquals(\"预期返回1\", 1, mockList.get(2)); // pass assertEquals(\"预期返回1\", 1, mockList.get(3)); // pass assertEquals(\"预期返回1\", 1, mockList.size()); // pass assertEquals(\"预期返回2\", 2, mockList.size()); // pass assertEquals(\"预期返回3\", 3, mockList.size()); // pass when（mock执行什么方法）.thenReturn(\"指定mock需要返回的值\")，返回值都是mock指定 3.验证 verify(mockList, never()).clear(); // 从未调用过clear方法 verify(mockList, times(2)).get(1); // get(1)方法调用了2次 verify(mockList, times(3)).get(anyInt()); // get(任意数字)调用了3次 verfiy(mockList, times(4)).size(); // 这里会失败，因为上面我们只调用了size方法3次 mock对他做过什么一清二楚 "},"test/mock/mockito/Mockito使用案例.html":{"url":"test/mock/mockito/Mockito使用案例.html","title":"Mockito使用案例","keywords":"","body":"Mockito使用案例业务场景使用Mockito测试Mockito使用案例 业务场景 需求 有一段业务逻辑，需要对给定的请求（如http请求）做处理， 常规测试: 需要把代码部署到服务器上 还需要构造并发起一个请求（前端配合/postman进行测试） 等服务器接收到请求后，才能交给业务处理 // 业务代码 public boolean handleRequest(HttpServletRequest request) { String module = request.getParameter(\"module\"); if (\"live\".equals(module)) { // handle module live request return true; } else if (\"user\".equals(module)) { // handle module user request return true; } return false; } 常规测试面临的问题 操作繁琐 一次性操作 等待时间漫长 使用Mockito测试 回顾需求： 1. 给定一个特定的输入（这个输入可能不容易创建，如HttpRequest） 2. 验证其输出结果是否正确 也就是我们验证的过程要尽可能的方便，而不是把大部分时间耗费在验证过程上 @Test public void handleRequestTestLive() throws Exception { HttpServletRequest request = mock(HttpServletRequest); when(request.getParameter(\"module\")).thenReturn(\"live\"); boolean ret = handleRequest(request); assertEquals(true, ret) } @Test public void handleRequestTestUser() throws Exception { HttpServletRequest request = mock(HttpServletRequest); when(request.getParameter(\"module\")).thenReturn(\"user\"); boolean ret = handleRequest(request); assertEquals(true, ret) } @Test public void handleRequestTestNone() throws Exception { HttpServletRequest request = mock(HttpServletRequest); when(request.getParameter(\"module\")).thenReturn(null); boolean ret = handleRequest(request); assertEquals(false, ret) } 首先模拟出一个假对象 设置这个假对象行为 这个行为会影响我们业务结果 验证各种输入下，业务逻辑正确性 "},"test/mock/mockito/Mockito原理.html":{"url":"test/mock/mockito/Mockito原理.html","title":"Mockito原理","keywords":"","body":"Mockito原理1.提出问题2.原理2.1 Mock原理2.2 打桩原理2.3 验证原理Mockito原理 带着问题针对性研究，才是学习框架最好的方法 1.提出问题 mock(List.class) 是怎么从List.class这个接口中构建List对象的？ when(mockList.size()).thenReturn(20) 如何干预mock对象的执行，插桩返回20？ verify(mockList,never()).add(10) 这种验证方式是怎么实现的？ 2.原理 2.1 Mock原理 我们先看下看mock一个对象需要做什么 首先需要知道mock的对象类型，这样才能生成这个类型的对象 实例化这个类型的对象 如果是抽象类或者接口，继承后给这些方法返回一个空实现 向上转型成目标类返回 总结：给定要mock类型，生成一个继承这个类型的类，实例化生成的类，得到mock对象 2.1.1 mock源码实现 暴露出Mockito.mock 接口给使用者 得到要mock类型，进行一些设置，然后一路传递到SubclassBytecodeGenertor，有它来生成mock类型的子类 得到这个类型后，SubclassBytecodeGenertor将其实例化 第二步的实现借助了ByteBuddy这个框架，这个框架可以直接生成Java类，然后通过ClassLoader加载进来使用 第三步实现化，使用了objenesis，一个能在不同平台实例化一个类的库 2.2 打桩原理 when这一步要实现的功能是打桩 那么对于when(mockType.someMethod()).thenReturn(value)这样的方法调用，该怎么实现 原理实现 在mock的时候，我们知道Mockito生成了一个派生类，派生类里的所有方法调用，已经 被hook掉，既素有的方法调用，并不会执行到原有的逻辑里，而是会返回一个默认值 所有的方法最终都会交由MockHandlerImpl.handle来执行。 这个MockHandlerImpl.handle 是mockito 核心所在 在进行方法调用的时候，Mockito会假定这个方法调用需要被打桩，生成一个和这个方法调用相对用的OngoingStubbing对象，将这个对象暂时存起来。 当when 方法执行的时候，就会取出这个暂存的OngoingStubing 对象返回，这样我们就能在这个上面打桩（调用thenReturn等方法），返回我们需要的值，打桩完毕会生成一个Answer对象，存在在一个链表里，后调调用对象那个的方法的时候，就会从这个链表内找出对应的Answer对象，从中获取对应的值返回 2.3 验证原理 验证的代码 verify(mockList,times(2)).get(anyInt()) 要达成这样的效果，实现里必须 在verify方法的执行过程里，记录下要验证的对象，一集要验证的参数 在执行方法调用的时候，取出要验证的对象，验证的参数，执行验证 "},"test/mock/Mockito单测service.html":{"url":"test/mock/Mockito单测service.html","title":"Mockito单测service","keywords":"","body":"Mockito单测serviceMockito单测service Mock声明的对象，对函数的调用均执行mock（即虚假函数），不执行真正部分。 Spy声明的对象，对函数的调用均执行真正部分。 public class PushControllerTest { @Spy RestTemplate restTemplate; @InjectMocks private PushServiceImpl pushService; @Before public void setUp() throws Exception { MockitoAnnotations.initMocks(this); } @Test public void test() throws IOException { pushService.sendVersionUpdatePush(); } } "},"test/PressureTest/siege/Mac压测Siege.html":{"url":"test/PressureTest/siege/Mac压测Siege.html","title":"Mac压测Siege","keywords":"","body":"Mac/Linux压测Siege1. 安装2. 压测前准备工作2.1 修改系统的文件描述符限制3. 常用压测命令4. 小试牛刀5. 常用命令总结参考文章Mac/Linux压测Siege 1. 安装 brew install siege 默认安装在/usr/local/bin/，并自动添加到系统环境变量中,在终端输入siege 如果可以显示出相应的命令介绍，如下图所示，则表示我们已经安装成功。 2. 压测前准备工作 2.1 修改系统的文件描述符限制 当我们需要模拟大并发情况下的压测，则需要开启数量可观的线程，以及要占用大量的文件描述符，而系统默认对能够使用的文件描述符数量做了限制。首先使用 ulimit -a 命令查看目前系统开启的文件描述符数量 可以看到目前系统允许使用的文件描述符的数量为 256，这是远远不够的。如果不修改该参数，要是进行大量并发的模拟场景时则会报TOO MANY FILES OPEN 错误。为此我们可以先使用 ulimit -n 10000命令来调大系统可打开的文件描述符数量。 3. 常用压测命令 下面列举出了一些siege的常用命令: -C, --config 在屏幕上打印显示出当前的配置,配置是包括在他的配置文件$HOME/.siegerc中,可以编辑里面的参数,这样每次siege 都会按照它运行. -v, --verbose 运行时能看到详细的运行信息. -c, --concurrent=NUM 模拟有n个用户在同时访问,n不要设得太大,因为越大,siege消耗本地机器的资源越多. -r, --reps=NUM 重复运行测试n次,不能与-t同时存在 -t, --time=NUMm 持续运行siege ‘n’秒(如10S),分钟(10M),小时(10H) -d, --delay=NUM 每个url之间的延迟,在0-n之间. -b, --benchmark 请求无需等待 delay=0. -i, --internet 随机访问urls.txt中的url列表项. -f, --file=FILE 指定用特定的urls文件运行 ,默认为urls.txt,位于siege安装目录下的etc/urls.txt -R, --rc=FILE 指定用特定的siege 配置文件来运行,默认的为$HOME/.siegerc -l, --log[=FILE] 运行结束,将统计数据保存到日志文件中siege .log,一般位于/usr/local/var/siege .log中,也可在.siegerc中自定义 4. 小试牛刀 做好了前面的准备工作，我们便可以使用siege进行压测的实践，就拿我们经常访问的百度来做一个测试，在命令行中执行下面的命令 siege -c 5 -r 5 http://www.baidu.com 测试的结果如下： 这些测试结果中的项目含义分别为： Transactions 总共测试次数 Availability 成功次数百分比 Elapsed time 总共耗时多少秒 Data transferred 总共数据传输 Response time 等到响应耗时 Transaction rate 平均每秒处理请求数 Throughput 吞吐率 Concurrency 最高并发 Successful transactions 成功的请求数 Failed transactions 失败的请求数 Longest transaction 每次传输所花最长时间 Shortest transaction 每次传输所花最短时间 5. 常用命令总结 # 200个并发对百度发送请求100次 siege -c 200 -r 100 http://www.baidu.com # 对urls.txt中列出所有的网址进行压测 siege -c 200 -r 100 -f urls.txt # 随机选取urls.txt中列出的网址,按照100*100的并发度进行测试 siege -c 100 -r 100 -f urls.txt -i # 指定http请求头 文档类型 siege -H \"Content-Type:application/json\" -c 200 -r 100 -f urls.txt -i -b # 发送post请求，在网址后添加POST说明，并且紧跟参数在其后 siege -c 100 -r 100 http://www.baidu.com/ POST k1=v1&k2=v2 需要注意的是，如果地址和参数中含有中文或非ASCII字符时，首先需要对这些字符进行url编码，这样才可正确的进行测试。 参考文章 Mac/Linux压力测试神器Siege详解(附安装过程 "},"test/PressureTest/siege/Siege压测POST实例.html":{"url":"test/PressureTest/siege/Siege压测POST实例.html","title":"Siege压测POST实例","keywords":"","body":"Siege压测POST实例1. 新建POST 的参数文件2. 执行测试Siege压测POST实例 1. 新建POST 的参数文件 新建 param.json参数文件 {\"accessId\":\"1000000000\", \"secretValue\":\"7be4265165a9\"} 2. 执行测试 siege -H \"Content-Type:application/json\" -c 500 -r 500 \"http://app.msyos.com/ele_wallet_service/api/essc/getSign POST "},"test/PressureTest/JMeter/JMeter的安装.html":{"url":"test/PressureTest/JMeter/JMeter的安装.html","title":"mac下JMeter的安装","keywords":"","body":"JMeter的安装1. 简介2. 安装2.1 手动安装2.2 使用 homebrew3. 启动3.1 直接启动JMeter的安装 1. 简介 Apache JMeter 是一個 Apache 專案，目的是用來作 load test 工具，可以提供於分析和測量各種服務的性能，主要目標是 Web application。 JMeter 也可以用來進行 JDBC數據庫連接，FTP，LDAP，WebService，JMS，HTTP，一般 TCP 連線和 OSnative processes 的單元測試工具。 2. 安装 安装方式有两种 2.1 手动安装 下載並解壓 \bApache JMeter 執行 apache-jmeter-5.1.1/bin/jmeter.sh 2.2 使用 homebrew brew install jmeter 會自動加入環境變數，不用指定執行路徑 3. 启动 安装完成后提示的安装路径 /usr/local/Cellar/jmeter/5.2.1 点击bin下的 可以看到启动后的页面为： 3.1 直接启动 直接在终端（任意目录）输入jmeter，即可启动JMeter。 "},"test/PressureTest/JMeter/JMeter的基本使用.html":{"url":"test/PressureTest/JMeter/JMeter的基本使用.html","title":"JMeter的基本使用","keywords":"","body":"JMeter的基本使用1. 基本使用1.1 步骤11.2 步骤21.3 步骤31.4 步骤4：设定要测试的Http Request请求1.5 建立测量图报表1.6 View Results Tree1.7 看结果集概述参考文章JMeter的基本使用 1. 基本使用 1.1 步骤1 在 Test Plan 上點右鍵， Add → Threads (Users) → Thread Group 1.2 步骤2 设定有10个使用者来存取（测试）我们的服务 Ramp-Up Period (in seconds) 指得是「在幾秒內達到所設定的使用者人數」，可以讓受測服務一開始不會接受到太過巨量的 Requests 1.3 步骤3 模拟每个使用者，都会对我们的服务存取一定的次数 在 Thread Group 上點右鍵， Add → Logic Controller → Loop Controller 設定 Loop count (迴圈/重複執行次數)為 100 次 設定完 Thread Group 和 Loop Count 後，也就等於控制了對受測服務所發出的 request 數量，這邊作個簡單的計數的話就是： 10 (Users) * 100 (Loop Count) = 1,000 (Requests) 也就是我們的服務將接受 1,000 次 requests 的測試。 1.4 步骤4：设定要测试的Http Request请求 建立一個 HTTP Request。在 Loop Controller 上點右鍵， Add → Sampler → HTTP Request 输入要测试的内容 如果有需要添加请求header 例如这里添加到content-type 为json 1.5 建立测量图报表 我們一樣在 Loop Controller 上點右鍵， Add → Listener → Graph Results 加入圖形化的測量結果： 图标结果之后的展示页面 1.6 View Results Tree Loop Controller 上點右鍵， Add → Listener → View Results Tree 加入 View Results Tree 來記錄每一筆 Request 的結果 View Results Tree 裡可以看到實際傳回的 request 和 response data. 1.7 看结果集概述 概览图 参考文章 Apache JMeter 測試工具簡單基本教學 "},"test/PressureTest/JMeter/question/JMeter测试连接超时.html":{"url":"test/PressureTest/JMeter/question/JMeter测试连接超时.html","title":"JMeter测试连接超时","keywords":"","body":"JMeter测试连接超时参考文章JMeter测试连接超时 主要原因tomcat 最大线程数150，压测500。超过的就会出现此错误 参考文章 性能测试工具 jmeter 一个接口并发 1000，结果一小部分返回 Connection timed out: connect 随笔记录-SpringBoot项目配置Tomcat和JVM参数 "},"linux/":{"url":"linux/","title":"linux","keywords":"","body":"linuxlinux 操作命令 用户管理 查找文件 根据端口号/进程号查项目位置 sshd ssh一段时间就断掉 清除挖矿程序（sysupdate, networkservice进程） 软件安装 Java安装 MySQL CentosMySQL安装 nginx 安装nginx 问题集锦 nginx设置请求body大小 403 Forbidden 支持websocket FTP 安装FTP服务 unzip kafka "},"linux/operation/":{"url":"linux/operation/","title":"操作命令","keywords":"","body":"操作命令操作命令 "},"linux/operation/用户管理.html":{"url":"linux/operation/用户管理.html","title":"用户管理","keywords":"","body":"修改密码1. 添加用户2. 修改密码修改密码 1. 添加用户 useradd ftpuser 2. 修改密码 # 修改登录账户密码 passwd # root用户下 修改其他账户密码 passwd "},"linux/operation/查找文件.html":{"url":"linux/operation/查找文件.html","title":"查找文件","keywords":"","body":"查找文件查找文件 find后面写上 -name，表明要求系统按照文件名查找，最后写上httpd.conf这个目标文件名即可 find / -name httpd.conf "},"linux/operation/根据端口号/进程号查项目位置.html":{"url":"linux/operation/根据端口号/进程号查项目位置.html","title":"根据端口号/进程号查项目位置","keywords":"","body":"根据端口号/进程号查项目位置1. 根据端口号查pid2. 根据pid查路径根据端口号/进程号查项目位置 1. 根据端口号查pid 仅有端口号时，需要根据端口号先查出进程号 方式一：losf losf -i:{port} 方式二：netstat netstat -tunlp|grep {port} 2. 根据pid查路径 ll /proc/9293 可以查到所有pid相关的信息，其中cwd就是进程所在位置 当然也可以直接加上cwd查询位置 "},"linux/sshd/":{"url":"linux/sshd/","title":"sshd","keywords":"","body":"sshdsshd "},"linux/sshd/ssh一段时间就断掉.html":{"url":"linux/sshd/ssh一段时间就断掉.html","title":"ssh一段时间就断掉","keywords":"","body":"ssh一段时间就断掉1. 编辑sshd_config2.重启sshd服务ssh一段时间就断掉 1. 编辑sshd_config vim /etc/ssh/sshd_config 找到下面两行 #ClientAliveInterval 0 #ClientAliveCountMax 3 去掉注释，改成 ClientAliveInterval 30 ClientAliveCountMax 86400 ClientAliveInterval 客户端每隔多少秒向服务发送一个心跳数据 ClientAliveCountMax 客户端多少秒没有相应，服务器自动断掉连接 2.重启sshd服务 service sshd restart "},"linux/error/清除挖矿程序.html":{"url":"linux/error/清除挖矿程序.html","title":"清除挖矿程序（sysupdate, networkservice进程）","keywords":"","body":"清除挖矿程序（sysupdate, networkservice进程）1. top查看cpu使用状态2.通过进程号查询位置2.干掉挖矿进程3. 删除挖矿程序4. 删除定时任务参考文章清除挖矿程序（sysupdate, networkservice进程） 1. top查看cpu使用状态 我们发现有两个进程cpu占用都超过100%了，而且我们也并不知道他是干嘛的。 2.通过进程号查询位置 ls -l proc/{进程号}/exe 2.干掉挖矿进程 kill -9 6249 kill -9 6292 3. 删除挖矿程序 主要包括networkservice、sysguard、update.sh、config.json,sysupdate 直接删除可能无法删除，需要先执行 chattr -i chattr -i networkservice rm -f networkservice chattr -i sysguard rm -f sysguard chattr -i sysguard rm -f sysguard chattr -i config.json rm -f config.json chattr -i sysupdate rm -f sysupdate 4. 删除定时任务 cd /var/spool/cron/ # 看是否有定时任务，有则删除 rm -rf nobody 参考文章 记录一次清除Linux挖矿病毒的经历(sysupdate, networkservice进程) "},"linux/error/清除挖矿程序kdevtmpfsi.html":{"url":"linux/error/清除挖矿程序kdevtmpfsi.html","title":"清除挖矿程序（kdevtmpfsi进程）","keywords":"","body":"清除挖矿程序（kdevtmpfsi进程）参考文章清除挖矿程序（kdevtmpfsi进程） top查看当前进程 可以看到该进程就占了97%的cpu kill该进程 kill -9 19012 删除tmp/kdevtmpfsi文件 rm -rf /tmp/kdevtmpfsi 清除相关守护进程请看参考文章，我暂未操作 参考文章 处理kdevtmpfsi挖矿病毒 "},"linux/nginx/":{"url":"linux/nginx/","title":"nginx","keywords":"","body":"nginxnginx "},"linux/nginx/安装nginx.html":{"url":"linux/nginx/安装nginx.html","title":"安装nginx","keywords":"","body":"安装nginx1. 安装步骤1.1 安装依赖包1.2 下载并解压安装包1.3 安装nginx1.4 配置nginx.conf1.5 启动nginx1.6 浏览器打开2. 启动可能遇到的问题2.1 logs 文件夹目录没有2.2 nginx.pid 异常3. nginx 命令安装nginx 1. 安装步骤 1.1 安装依赖包 //一键安装上面四个依赖 yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel 1.2 下载并解压安装包 //下载tar包 wget http://nginx.org/download/nginx-1.16.1.tar.gz tar -xvf nginx-1.16.1.tar.gz wget 下载的tar包版本可以通过nginx官网查看 1.3 安装nginx //进入nginx目录 cd nginx-1.16.1 //执行命令 ./configure //执行make命令 make //执行make install命令 make install 安装完成会在/usr/local目录下出现nginx 1.4 配置nginx.conf # 打开配置文件 vi /usr/local/nginx/conf/nginx.conf 按需修改配置 如默认端口为80，可以修改成想要的地址 server { listen 80; server_name localhost; ... } 1.5 启动nginx /usr/local/nginx/sbin/nginx -s reload 查看nginx进程是否启动 ps -ef | grep nginx 1.6 浏览器打开 2. 启动可能遇到的问题 2.1 logs 文件夹目录没有 错误提示如下 [alert] could not open error log file: open() \"/usr/local/nginx/logs/error.log\" failed (2: No such file or directory) 解决方案：在提示的指定目录创建logs文件夹 mkdir logs 2.2 nginx.pid 异常 错误提示如下 [error] 21478#0: open() \"/usr/local/nginx/logs/nginx.pid\" failed (2: No such file or directory) 解决方案：先设置配置文件 /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 3. nginx 命令 ./nginx 启动 ./nginx -s stop 关闭 ./nginx -s reload 重启 "},"linux/nginx/nginx设置开机自启动.html":{"url":"linux/nginx/nginx设置开机自启动.html","title":"nginx设置开机自启动","keywords":"","body":"nginx设置开机自启动1. 在/etc/init.d 下创建文件nginx2. 将nginx服务加入chkconfig2.1 如出现“服务 nginx 不支持 chkconfig”nginx设置开机自启动 1. 在/etc/init.d 下创建文件nginx Vim 创建nginx文件 vim /etc/init.d/nginx 编辑内容 官网配置 需要修改以下两处 nginx=”/usr/local/nginx/sbin/nginx” //修改成nginx执行程序的路径。 NGINX_CONF_FILE=”/usr/local/nginx/conf/nginx.conf” //修改成nginx.conf文件的路径。 保存后设置文件的执行权限 chmod a+x /etc/init.d/nginx 至此就可以通过下面指令控制启动停止 /etc/init.d/nginx start /etc/init.d/nginx stop 上面的方法完成了用脚本管理nginx服务的功能，但是还是不太方便。 2. 将nginx服务加入chkconfig 先将nginx服务加入chkconfig管理列表： chkconfig --add /etc/init.d/nginx 加完这个之后，就可以使用service对nginx进行启动，重启等操作了。 service nginx start service nginx stop service nginx restart 最后设置开机自动启动 chkconfig nginx on 2.1 如出现“服务 nginx 不支持 chkconfig” 在/etc/init.d/nginx加上 #!/bin/sh #chkconfig: 2345 80 90 #description:auto_run "},"linux/nginx/problem/nginx设置请求body大小.html":{"url":"linux/nginx/problem/nginx设置请求body大小.html","title":"nginx设置请求body大小","keywords":"","body":"nginx设置请求body大小nginx设置请求body大小 nginx默认是1M，需要增大的话。 在nginx.conf中http{}增加一句 client_max_body_size 100M; 重启nginx即可 "},"linux/nginx/problem/403Forbidden.html":{"url":"linux/nginx/problem/403Forbidden.html","title":"403 Forbidden","keywords":"","body":"403 Forbidden1. 情况一2. 情况二403 Forbidden 1. 情况一 nginx 访问静态资源文件提示 是因为权限引起，将nginx 改成root启动 #user nobody; user root; 2. 情况二 如果把它当做文件目录，要列出文件目录 添加: autoindex on; # 文件服务器 server { listen 80; server_name file.isture.com; location / { root /home/ftpuser/file; autoindex on; } } "},"linux/nginx/problem/支持websocket.html":{"url":"linux/nginx/problem/支持websocket.html","title":"支持websocket","keywords":"","body":"支持websocket支持websocket 需要加上 proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; 例如： server { listen 80; server_name gd.isture.com; location / { proxy_pass http://120.79.200.111:9705/; proxy_read_timeout 300; proxy_connect_timeout 300; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } "},"linux/nginx/nginx的root和alias的区别.html":{"url":"linux/nginx/nginx的root和alias的区别.html","title":"nginx的root和alias的区别","keywords":"","body":"nginx的root和alias的区别1. root 用法2.alias用法参考文章nginx的root和alias的区别 1. root 用法 location /request_path/image/ { root /local_path/image/; } 这样配置的结果就是当客户端请求/request_path/image/cat.png 的时候， Nginx把请求映射为/local_path/image/request_path/image/cat.png 注意这时候除了root 的路径，还会带上请求路径（例如这里：/request_path/image/） 2.alias用法 location /request_path/image/ { alias /local_path/image/; } 这时候，当客户端请求 /request_path/image/cat.png 的时候， Nginx把请求映射为/local_path/image/cat.png 注意：alias中的路径最后必须跟上/ root的路径最后可跟可不跟，alias支持正则表达式路径root不支持** 参考文章 nginx 静态文件袋里配置,将本地文件/夹反向代理以及root与alias的区别 "},"linux/insatll/java.html":{"url":"linux/insatll/java.html","title":"Java安装","keywords":"","body":"Java安装1. 具体步骤1.1 下载jdk1.2 安装1.3 设置环境变量Java安装 1. 具体步骤 1.1 下载jdk Oracle官网找到合适的版本下载 centos：下载Linux x64 的tar.gz 版本 1.2 安装 创建安装目录 mkdir /usr/local/java 解压至安装目录 tar -zxvf jdk-8u221-linux-x64.tar.gz -C /usr/local/java/ 1.3 设置环境变量 开打文件 vim /etc/profile 在末尾添加 export JAVA_HOME=/usr/local/java/jdk1.8.0_221 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH 使环境变量生效 source /etc/profile 添加软连接 ln -s /usr/local/java/jdk1.8.0_221/bin/java /usr/bin/java 检查 java -version "},"linux/ftp/":{"url":"linux/ftp/","title":"FTP","keywords":"","body":"FTPFTP "},"linux/ftp/安装FTP服务.html":{"url":"linux/ftp/安装FTP服务.html","title":"安装FTP服务","keywords":"","body":"安装FTP服务1.安装vsftpd2. 修改配置文件3. 启动ftp服务4. 用户管理5. 主动模式和被动模式参考文章安装FTP服务 1.安装vsftpd 安装前先查看ftp 是否已经安装，使用yum 安装 vsftpd -v yum -y install vsftpd 2. 修改配置文件 根据自己的需求，修改ftp配置文件/etc/vsftpd/vsftpd.conf anonymous_enable=NO # 是否允许匿名访问 local_enable=YES # 是否允许使用本地帐户进行 FTP 用户登录验证 local_umask=022 # 设置本地用户默认文件掩码022 chroot_local_user=YES # 是否限定用户在其主目录下（NO 表示允许切换到上级目录） #chroot_list_enable=YES # 是否启用限制用户的名单（注释掉为禁用） chroot_list_file=/etc/vsftpd/chroot_list # 用户列表文件（一行一个用户） allow_writeable_chroot=YES # 如果启用了限定用户在其主目录下需要添加这个配置，解决报错 500 OOPS: vsftpd: refusing to run with writable root inside chroot() xferlog_enable=YES # 启用上传和下载的日志功能，默认开启。 use_localtime=YES # 是否使用本地时(自行添加) userlist_enable=YES chroot_local_user 与 chroot_list_enable 的组合效果如下： chroot_local_user=YES chroot_local_user=NO chroot_list_enable=YES 1. 所有用户都被限制在其主目录下 2. 使用 chroot_list_file 指定的用户列表，这些用户作为“例外”，不受限制 1. 所有用户都不被限制其主目录下 2. 使用 chroot_list_file 指定的用户列表，这些用户作为“例外”，受到限制 chroot_list_enable=NO 1. 所有用户都被限制在其主目录下 2. 不使用 chroot_list_file 指定的用户列表，没有任何“例外”用户 1. 所有用户都不被限制其主目录下 2. 不使用 chroot_list_file 指定的用户列表，没有任何“例外”用户 注意：如果设置了 local_enable=YES ，自带 3. 启动ftp服务 systemctl start vsftpd 4. 用户管理 # 使用useradd 命令添加一个用户 useradd ftpuser # 设置用户密码 passwd ftpuser 5. 主动模式和被动模式 ftp 的主动模式（Port 模式）与被动模式（PASV 模式）的区别：https://www.cnblogs.com/xiaohh/p/4789813.html 本文推荐使用被动模式，vsftp 默认即为被动模式 开启被动模式（PASV） 在 /etc/vsftpd/vsftpd.conf 配置文件添加如下配置 pasv_enable=YES # 是否允许数据传输时使用PASV模式（默认值为 YES） pasv_min_port=port port_number # PASV 模式下，数据传输使用的端口下界（0 表示任意。默认值为 0）把端口范围设在比较高的一段范围内，比如 50000-60000，将有助于安全性的提高. pasv_max_port=port_number # PASV 模式下，数据传输使用的端口上界（0 表示任意。默认值为 0） pasv_promiscuous=NO # 是否屏蔽对 PASV 进行安全检查，默认值为 NO（当有安全隧道时可禁用） pasv_address # PASV 模式中服务器传回的 ip 地址。默认值为 none，即地址是从呼入的连接套接字中获取。 开启主动模式（PORT）的配置 port_enable=YES # 是否开启 Port 模式 connect_from_port_20=YES # 当 Port 模式开启的时候，是否启用默认的 20 端口监听 ftp_data_port=port_number # Port 模式下 FTP 数据传输所使用的端口，默认值为20 参考文章 CentOS 7 安装 FTP 服务 "},"linux/unzip/":{"url":"linux/unzip/","title":"unzip","keywords":"","body":"unzip1. 安装命令unzip 1. 安装命令 yum install zip unzip "},"linux/kafka/":{"url":"linux/kafka/","title":"kafka","keywords":"","body":"kafka1.下载安装步骤1.1 下载1.2 解压安装1.3 修改配置文件2. 启动服务参考文章kafka 1.下载安装步骤 1.1 下载 官网上下载最新版 1.2 解压安装 tar -zvxf kafka-2.3.0-src.tgz -C /usr/local/ 1.3 修改配置文件 vim config/server.properties 修改其中的 broker.id=1 log.dirs=data/kafka-logs 2. 启动服务 bin/kafka-server-start.sh config/server.properties ……. 还要和zookeeper 合在一起用，放弃 参考文章 CentOS7下Kafka的安装介绍 "},"aliyun/ecs/":{"url":"aliyun/ecs/","title":"云服务器ECS","keywords":"","body":"云服务器ECS云服务器ECS "},"aliyun/ecs/安装系统.html":{"url":"aliyun/ecs/安装系统.html","title":"安装/重装系统","keywords":"","body":"安装/重装系统1. 选择重装系统2. 安全组设置2. ssh 连接安装/重装系统 1. 选择重装系统 如果是重装，先确保设备已停止 选择Centos7.6 作为系统盘 确定后输入验证码就完成了 2. 安全组设置 如果未设置安全组外网是连接不上服务器的 2. ssh 连接 刚创建完成的系统直接连接后会提示 此时可以删除knows_hosts解决 rm -rf /root/.ssh/known_hosts // 或者删除该提示中的文件 可能需要重启电脑才能生效 通过ssh -p 22 root@xxx.xx.xxx.xxx 就可以连接上服务器了 localhost:~ zhangshengzhong$ ssh -p 22 root@120.79.200.111 root@120.79.200.111's password: Last failed login: Sat Sep 7 18:32:37 CST 2019 from 120.36.175.115 on ssh:notty There was 1 failed login attempt since the last successful login. Last login: Sat Sep 7 18:26:31 2019 from 120.36.175.115 Welcome to Alibaba Cloud Elastic Compute Service ! [root@iZwz97t3ru69kye3l7uj70Z ~]# "},"cs/":{"url":"cs/","title":"计算机基础","keywords":"","body":"计算机基础计算机基础 计算机网络常见知识点 UML类图 磁盘存取 DNS域名解析 CDN 相关概念 DNS域名解析 "},"net/计算机网络常见知识点.html":{"url":"net/计算机网络常见知识点.html","title":"计算机网络常见知识点","keywords":"","body":"计算机网络常见知识点1. OSI 与 TCP/IP 各层的结构与功能，都有哪些协议1.1 应用层1.2 运输层1.3 网络层1.4 数据链路层1.5 物理层2. TCP 三次握手和四次挥手（常问）2.1 TCP 三次握手漫画图解2.2 为什么要三次握手2.3 第二次握手为什么要回传SYN和ACK?3. TCP 四次挥手（常问）3.1 为什么要四次挥手4. TCP,UDP 协议的区别5. TCP 协议如何保证可靠传输6. 在浏览器中输入url 地址>>显示主页的过程（常问）7. 状态码8. 各种协议与HTTP协议之间的关系9. HTTP 长连接、短连接10. HTTP 是不保存状态协议，如何保存用户状态10.1 服务端如何保存session10.2 如何实现session跟踪10.3 如果cookie 被禁用如何跟踪11. Cookie 的作用是什么？和Session 有什么区别？12. HTTP 1.0 和HTTP 1.1 的主要区别13. URI 和URL 的区别14. HTTP 和 HTTPS 的区别？计算机网络常见知识点 1. OSI 与 TCP/IP 各层的结构与功能，都有哪些协议 我们一般采用比较折中的办法，就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构 我们自上而下的介绍一下各层的作用 1.1 应用层 应用层（application-layer）的任务是通过应用进程间的交互来完成特定的网络应用 主要协议包括 域名系统DNS HTTP协议 电子邮件的SMTP协议 1.2 运输层 运输层（transport layer）的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务 传输层的主要协议： 传输控制协议TCP（Transmission Control Protocol） 提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol） 提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 1.3 网络层 在计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网，网络层的任务就是选择合适的网间路由和交换结点，确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。 1.4 数据链路层 数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 1.5 物理层 在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 2. TCP 三次握手和四次挥手（常问） 为了准确无误的把数据送达目标处，TCP协议采用了三次握手策略 2.1 TCP 三次握手漫画图解 如下图所示，下面的两个机器人通过3次握手确定了对方能正确接收和发送消息 简单示意图： 客户端：发送带有SYN 标志的数据包 一次握手-服务端 服务端- 发送带有SYN/ACK 标志的数据包— 二次握手— 客户端 客户端— 发送带有ACK 标志的数据包 三次握手 服务端 2.2 为什么要三次握手 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据发送与接收，而三次握手最主要的目的就是双方确认与对方的发送和接收是正常的 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 2.3 第二次握手为什么要回传SYN和ACK? 回传SYN 发送端确认自己发送能力没有问题 回传ACK 接收端还不知道自己发送能力有没有问题，需要通过ACK来确认 3. TCP 四次挥手（常问） 断开一个TCP 连接则需要”四次挥手“ 客户端-发送一个FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN 一样，一个FIN 将占用一个序号 服务器-关闭与客户端连接，发送送一个FIN 给客户端 客户端- 发送ACK 报文确认，并将确认序号设置为收到序号加1 3.1 为什么要四次挥手 任何一方都可以在数据传送结束后发出发出释放通知，待对方确认后进入半关闭状态，当另一方也没有数据在发送的时候，则发送链接释放通知。对方确认后就完全关闭了TCP 链接 举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 4. TCP,UDP 协议的区别 UDP 在传送数据之前不需要建立连接，远地主机在收到UDP 报文后，不需要给出任何确认 QQ语言 QQ视频 直播等 TCP提供面向连接的服务。在传送数据之前必须建立连接，数据传送结束后释放连接。 文件传输 发送和接收邮件 远程登录 5. TCP 协议如何保证可靠传输 确认和重传：接收方收到报文就会确认，发送方发送一段时间后没有收到确认就重传。 数据校验：TCP报文头有校验和，用于校验报文是否损坏 数据合理分片和排序： tcp会按MTU合理分片，接收方会缓存未按序到达的数据，重新排序后再交给应用层 UDP：IP数据报大于1500字节,大于MTU.这个时候发送方IP层就需要分片(fragmentation).把数据报分成若干片,使每一片都小于MTU.而接收方IP层则需要进行数据报的重组.这样就会多做许多事情,而更严重的是,由于UDP的特性,当某一片数据传送中丢失时,接收方便无法重组数据报.将导致丢弃整个UDP数据报. 流量控制：当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。（TCP 利用滑动窗口实现流量控制） 拥塞控制：当网络拥塞时，减少数据的发送。 6. 在浏览器中输入url 地址>>显示主页的过程（常问） 总体来说分为以下几个过程 DNS 解析 TCP 连接 发送HTTP 请求 服务器处理请求并返回HTTP报文 浏览器解析渲染页面 连接结束 7. 状态码 8. 各种协议与HTTP协议之间的关系 9. HTTP 长连接、短连接 在HTTP1.0中默认使用短连接 也就是客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML 或其他类型的Web页中包含有其他web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 从HTTP/1.1起默认使用长连接 http1.1 默认使用长连接，用以保持连接特性，使用长连接的HTTP协议，会在响应体加入这行代码 Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 10. HTTP 是不保存状态协议，如何保存用户状态 HTTP 是一种不保存状态，即无状态（stateless）协议。也就是说 HTTP 协议自身不对请求和响应之间的通信状态进行保存。那么我们保存用户状态呢？ Session机制 session 的主要作用就是通过服务端记录用户的状态 典型的应用场景就是购物车，当你要添加商品到购物车时，系统不知道是哪个用户操作的，因为HTTP协议是无状态的。服务器给特定的用户创建特定的session之后就可以标识这个用户并且跟踪这个用户 一般情况下，服务器会在一定时间内保存这个 Session，过了时间限制，就会销毁这个Session 10.1 服务端如何保存session 保存的方式有很多，最常用的就是内存和数据库（比如内存数据库redis） 10.2 如何实现session跟踪 session保存在服务端，那么如何实现session跟踪呢？ 大部分情况下，我们都是通过cookie中附加一个 Session ID来跟踪 10.3 如果cookie 被禁用如何跟踪 最常用的就是利用URL 重写吧Session ID 直接附加到URL 路径后面 11. Cookie 的作用是什么？和Session 有什么区别？ Cookie 和 Session 都是用来跟踪浏览器用户身份的会话方式（会话跟踪的方式） 因为Cookie数据保存在客户端（浏览器端），Session数据保存在服务端。所以造成他们的使用场景不同，安全性不同 Cookie的应用场景 记住用户名 历史记录 Session 的应用场景 购物车 12. HTTP 1.0 和HTTP 1.1 的主要区别 长连接 在HTTP/1.0中，默认使用的是短连接‘ HTTP 1.1起，默认使用长连接 错误状态响应码 在HTTP1.1中新增了24个错误状态响应码 缓存处理 在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准 HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用（断点续传） HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能 HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 13. URI 和URL 的区别 URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。 URL(Uniform Resource Location) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。 14. HTTP 和 HTTPS 的区别？ 端口：HTTP的URL由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。 安全性（传输内容明文） HTTP 安全性没有 HTTPS高 HTTP协议运行在TCP之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。 HTTPS是运行在SSL/TLS之上的HTTP协议，SSL/TLS 运行在TCP之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密 资源消耗上 HTTPS 比HTTP耗费更多服务器资源 "},"net/计算机网络面试问题.html":{"url":"net/计算机网络面试问题.html","title":"计算机网络面试问题","keywords":"","body":"计算机网络面试问题计算机网络面试问题 说说计算机网络五层协议？ 说下 TCP 的三次握手流程？ 为什么要三次握手？ 第二次握手为什么要回传SYN和ACK? TCP的四次挥手？ TCP,UDP协议区别？ 为什么要四次挥手？ TCP是如何保证可靠传输的？ 在浏览器中输入url地址到显示主页的过程 "},"cs/uml/":{"url":"cs/uml/","title":"UML类图","keywords":"","body":"UML类图(Class Diagrams)1.基本概念2.常见的几种关系：2.1 泛化2.2 实现2.3.关联2.4 共享聚合2.5.组合集合2.6.依赖　　2.7 各种类图关系UML类图(Class Diagrams) 1.基本概念 类图是根据用例图抽象成类，描述类的内部结构和类和类之间的关系，是一种静态结构图 2.常见的几种关系： 泛化（Generalization） 实现（Realization） 关联（Association) 聚合（Aggregation） 组合(Composition) 依赖(Dependency) 各种关系的强弱顺序： 泛化 = 实现 > 组合 > 聚合 > 关联 > 依赖 2.1 泛化 【泛化关系】：是一种继承关系，表示一般与特殊的关系，它指定了子类如何继承父类的所有特征和行为。例如：老虎是动物的一种，即有老虎的特性也有动物的共性**即有老虎的特性也有动物的共性。** 描述方式：用三角形箭头加实线表示，箭头指向父类 2.2 实现 【实现关系】：是一种类与接口的关系，表示类是接口所有特征和行为的实现。 描述方式：用三角形箭头加虚线表示，箭头指向实现的接口 2.3.关联 【关联关系】：是一种拥有的关系，它使一个类知道另一个类的属性和方法；如：老师与学生，丈夫与妻子关联可以是双向的，也可以是单向的。双向的关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。 　 【代码体现】：成员变量 2.4 共享聚合 【聚合关系】：是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系，轮胎离开车仍然可以存在。 ​ 　聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。 ​ 【代码体现】：成员变量 2.5.组合集合 【组合关系】：是整体与部分的关系，但部分不能离开整体而单独存在。如公司和部门是整体和部分的关系，没有公司就不存在部门。 ​ 组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期。 　　【代码体现】：成员变量 　　【箭头及指向】：带实心菱形的实线，菱形指向整体 2.6.依赖　　 　【依赖关系】：是一种使用的关系，即一个类的实现需要另一个类的协助，所以要尽量不使用双向的互相依赖. 【代码表现】：局部变量、方法的参数或者对静态方法的调用 【箭头及指向】：带箭头的虚线，指向被使用者 2.7 各种类图关系 "},"cs/disk/":{"url":"cs/disk/","title":"磁盘存取","keywords":"","body":"磁盘存取1 磁盘存取原理2. 磁盘局部性原理与磁盘预读磁盘存取 1 磁盘存取原理 索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O 操作，与主存不同，磁盘I/O存在机械运动耗费。因此磁盘I/O的时间消耗时巨大的 1.1 磁盘的组成 一个磁盘由大小相同且同轴的圆形盘片组成 磁盘可以转动（各个磁盘必须同步转动）。 在磁盘的一则有磁头支架 磁头支架固定了一组磁头 每个磁头负责存储存取一个磁盘的内容 磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动） 每个磁头同一时刻必须是同轴的 1.2 磁盘组成和工作原理 磁道 每个同心环叫做一个 扇区 磁盘的最小存取单元 1.2.1 确定数据位置 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址。既确定要读的数据在哪个磁道，哪个扇区 1.2.2 磁头寻道 为了读取这个扇区的数据，需要将磁头放在这个扇区上方，为了实现这一点，磁头需要移动对准响应的磁道，这个过程叫做寻道，所耗费的时间叫寻道时间， 1.2.3 磁盘旋转到对应扇区 然后磁盘旋转将目标扇区旋转到磁头下,这个过程耗费的时间叫做旋转时间 2. 磁盘局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存储就比主存慢很多，在加上机械运动耗费，磁盘的存取速度往往是主存的几百分之一，因此为了提高效率，要尽量减少磁盘I/O，为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读可以提高I/O效率。预读的长度一般为页（page:计算机管理存储器的逻辑块-通常为4k）的整数倍。主存和磁盘以页为单位交换数据。当程序要去读的数据不再主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中 "},"cs/dns/":{"url":"cs/dns/","title":"DNS域名解析","keywords":"","body":"DNS域名解析1. DNS是什么？2. 查询过程2.1 简化+short3.DNS 的记录类型3.1 CNAME4.其他DNS 工具4.1 host命令4.2 nslookup命令4.3 whois 命令关于上文出现的218.85.152.99是我本机设置的dns地址参考博客DNS域名解析 1. DNS是什么？ DNS（Domain Namo System，域名解析系统）的作用非常简单，就是根据域名查出ip地址（可以想象成一本巨大的电话本） 例如：你要访问www.baidu.com 那么首先要通过DNS查出他的ip地址是14.215.177.38 2. 查询过程 虽然只需要返回一个IP地址，但是DNS的查询过程非常复杂，分成多个步骤 dig命令可以显示整个查询过程,在终端输入 dig www.baidu.com 会输出以下内容 第一段是查询参数和统计 第二段是查询内容 上面的结果表示，查询域名www.baidu.com 的A记录，A是address的缩写 第三段是DNS服务器的答复 www.baidu.com被cname到了www.a.shifen.com 上面的结果显示，www.baidu.com 有三个A记录，既三个ip地址。600是TTL值（Time to live的缩写），表示缓存时间，既600秒之内不用重新查询 第四段显示www.a.shifen.com 的NS记录（Name Service），既哪些服务器负责管路www.a.shifen.com 的DNS记录 上面的结果显示a.shifen.com共有五条NS记录，既五域名服务器，向其中任一台查询都能知道a.shifen.com 的ip地址是什么 第五段是上面5个域名服务器的ip地址，这是随着前一段一起返回的 第六段是dns服务器的一些传输信息 上面的结果显示，本机的DNS服务器是218.168.1.253，查询端口是53（dns服务器的默认端口），以及回应长度是260字节 2.1 简化+short 上面命令值返回www.baidu.com 对应的3个ip地址 3.DNS 的记录类型 域名和IP之间的对应关系，称为“记录”（record），根据使用场景，记录可以分成不同类型（type） 常见的DNS记录类型如下 A：地址记录（Address），返回的域名指向ip地址 NS:域名服务器记录（Name Service），返回保存下一级域名信息的服务器地址。该记录只能设置为域名，不能设置为IP地址 MX:邮件记录（Mail eXchange），返回接收电子邮件的服务器地址 CNAME:规范名称记录（Canonical Name）,返回另一个域名，既当前查询的域名是另一个域名的调整。（例如百度） PTR:逆向查询记录（Pointer Record），只用于从IP地址查询域名 3.1 CNAME CNAME 记录主要用于域名的内部跳转，为服务器配置提供灵活性，用户感知不到。 dig www.baidu.com ; > DiG 9.8.3-P1 > www.baidu.com ... ;; ANSWER SECTION: www.baidu.com. 600 IN CNAME www.a.shifen.com. www.a.shifen.com. 600 IN A 14.215.177.39 www.a.shifen.com. 600 IN A 14.215.177.38 上面的结果显示，www.baidu.com 的cname 记录指向的是www.a.shifen.com.也就是说用户查询www.baidu.com 的时候，实际上返回的是www.a.shifen.com 的ip地址。 好处 变更服务器ip地址的时候，只要修改www.a.shifen.com 这个域名就可以了，用户的www.baidu.com域名不用修改 4.其他DNS 工具 4.1 host命令 host命令可以看作dig命令的简化版本，返回当前请求域名的各种记录 4.2 nslookup命令 nslookup命令用于互动式的查询域名记录 也可以直接过滤出cname 4.3 whois 命令 whois 命令用来查看域名的注册情况 whois github.com 关于上文出现的218.85.152.99是我本机设置的dns地址 参考博客 DNS 原理入门 "},"cdn/":{"url":"cdn/","title":"CDN","keywords":"","body":"CDN1. 是什么2.解决了什么问题3.适用场景4.基本工作过程CDN 1. 是什么 CDN是将源站内容分发至最接近用户的节点，使用户可就近取得所需内容，提高用户访问的响应速度和成功率。 2.解决了什么问题 加速网站的访问 一个网站每慢一秒钟，就会丢失许多访客 视音频点播/大文件下载分发加速 视频直播加速 移动应用加速 移动app更新文件（apk文件）分发，图片，短视频等 3.适用场景 网站站点/应用加速 静态资源如各类型的图片、html、css、js文件等 前后端分离之后，前端就可以使用cdn加速 4.基本工作过程 最简单的cdn网络由一个DNS服务器和几台缓存服务器组成 当用户点击网站页面上的url，经过本地DNS系统解析，DNS系统会最终将域名解析权交给CNAME指向的CND专用DNS服务器 CDN的DNS服务器将CDN的全局负载均衡设备ip地址返回用户 用户向CDN的全局负载均衡设备发起内容URL访问请求 CDN全局负载均衡设备根据用户的ip地址，以及用户请求的内容url，选择一台用户所属区域的负载均衡设备，告诉用户想这台设备发起请求 区域负载均衡设备会为用户选择一台合适的缓存服务器提供服务器，选择的依据包括： 根据用户的ip地址，判断拿一台服务器距用户最近 根据用户所请求的url携带的内容名称，判断哪一台服务器上有用户所需内容 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务器能力 基于以上这些条件的综合分析之后，区域负载均衡设备会向全局负载均衡设备返回一台缓存服务器的ip 地址 全局负载均衡设备吧服务器的ip地址返回给用户 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端，如果这台缓存服务器上没有用户想要的内容，而区域均衡设备依然将它分配给了用户。那么这台服务器就要向他的上一级缓存服务器请求内容，直至追溯大网站的源服务将内容拉到本地 "},"cdn/阿里云CDN使用.html":{"url":"cdn/阿里云CDN使用.html","title":"阿里云CDN使用","keywords":"","body":"阿里云CDN使用1. 背景2. 云存储还是CDN?2.1 云存储2.2 CDN2.3 选择3. CDN使用3.1 购买CDN3.2 配置要加速的CDN域名3.3 配置域名解析下的CNAME3.4 nginx 配置4. 查看是否使用到了CDN实际下载速度阿里云CDN使用 1. 背景 App升级是项目必不可少的环节。在项目初期公司将升级文件都放在公司的文件服务器中，下载的人少，下载速度还有几百K的速度。 但随着公司业务的扩张，用户量的增加。公司内部的文件服务器带宽完全撑不住,下载速度只剩下10k左右，下载一个30M的apk需要1个小时 带宽是单位时间内某一点到另一点的“最高数据率”，带宽属于共享资源，例如我们app下载，随着同时下载的人数增加，带宽会被瓜分 所以我们考虑使用云来解决 2. 云存储还是CDN? 云存储和CDN 都能解决我们的问题。 2.1 云存储 将我们的apk等要下载的文件上传到云存储中，我们从云存储中下载。 存储apk 不需要多大的存储空间，反而是下载apk需要非常大的下行流量。 价格： 以下为阿里云存储的 1TB一个月的 下行流量价格505 2.2 CDN 我们使用CDN的方式，对我们的文件服务器进行CDN 加速。 CDN 第一次加载的时候，会将文件保存在CDN 的缓存服务器中供就近的用户下载。因为有缓存可能会遇到缓存导致异常等风险 价格 以下为阿里CDN的 1TB一个月的CDN加速包,价格162。与云存储的下行流量价格差了3倍 2.3 选择 阿里云存储下行流量和CDN加速包价格相差了3倍 缓存问题，经过处理也是能解决的。可以逐步测试推进 所以最终还是选CND加速。cdn未来还能做全站的加速 3. CDN使用 3.1 购买CDN 3.2 配置要加速的CDN域名 我选择ip解析的比较多。直接映射到你对应的服务器，交由nginx 解析成对应的服务中 如果选择域名解析，切记不能选择同名的域名，否则会造成循环解析，无法回源。 3.3 配置域名解析下的CNAME 我们点开详情可以看到cname的配置，将陪cname的值配置到域名解析中 配置域名解析 3.4 nginx 配置 因为源站信息配置域名，所以在nginx 上要配置域名解析 server { listen 80; server_name filecdn.xxx.com; location / { root /home/ftpuser/file; autoindex on; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods 'GET, POST, OPTIONS'; add_header Access-Control-Allow-Headers 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization'; if ($request_method = 'OPTIONS') { return 204; } } } 4. 查看是否使用到了CDN CDN配置之后大概几分钟就能生效 我们可以使用host 命令来查看是否使用到了CDN 使用dig命令来查看是否使用到了CDN 实际下载速度 同一个文件下载速度。 使用CDN的为1M 每秒 未使用CDN的为100k每秒 "},"cdn/阿里云带宽价格.html":{"url":"cdn/阿里云带宽价格.html","title":"阿里云带宽价格","keywords":"","body":"阿里云带宽价格1. 背景2. 操作流程2.1 准备工作2.2 按固定带宽计费2.3 按使用流量计费阿里云带宽价格 1. 背景 阿里云 CDN（1T 162） 和云存储(1T 505)还是太贵了，我们主要就是使用到了他们的带宽，如果直接购买阿里云的带宽会不会便宜一些 2. 操作流程 2.1 准备工作 选购了一台2核4G，外网带宽1M 的阿里云服务器，每个月的价格为197.82元 2.2 按固定带宽计费 相同变量下我们选购100M 的带宽，7819.82元 100M带宽 下载速度总和=100M/8=12.5M APP版本更新时：假如100个人同时下载，理论的下载速度就是125kb/s，随着下载人数的增加，速度持续下降 2.3 按使用流量计费 我们以100M 峰值计费，每G 是0.8元，下的越多价格就越贵。 以实际情况为例：我们公司发布一次版本大概需要花费10T 的下载流量，每次就需要花费8000元的流量费。且每个月只能更新一次。 "},"library/":{"url":"library/","title":"第三方库","keywords":"","body":"第三方库第三方库 "},"library/Sentinel/Sentinel限流.html":{"url":"library/Sentinel/Sentinel限流.html","title":"Sentinel限流","keywords":"","body":"Sentinel限流实战1.1 引入依赖1.2 新建过滤器1.3 自定义限流异常Sentinel限流 Sentinel限流官方文档) 实战 例如我们图片验证码是免认证的，所以只要知道了地址就可以频繁的去获取验证码，这无形之中给服务器增加了很大的压力。甚至可能导致服务器宕机。 1.1 引入依赖 com.alibaba.csp sentinel-zuul-adapter 1.6.3 1.2 新建过滤器 @Slf4j @Configuration public class GatewaySentinelFilter { @Bean public ZuulFilter sentinelZuulPreFilter() { return new SentinelZuulPreFilter(); } @Bean public ZuulFilter sentinelZuulPostFilter() { return new SentinelZuulPostFilter(); } @Bean public ZuulFilter sentinelZuulErrorFilter() { return new SentinelZuulErrorFilter(); } @PostConstruct public void doInit() { initGatewayRules(); } /** * 定义验证码请求限流，限流规则： * 60秒内同一个IP，同一个 key最多访问 10次 */ private void initGatewayRules() { Set definitions = new HashSet<>(); Set predicateItems = new HashSet<>(); predicateItems.add(new ApiPathPredicateItem().setPattern(\"/auth/captcha\")); ApiDefinition definition = new ApiDefinition(\"captcha\") .setPredicateItems(predicateItems); definitions.add(definition); GatewayApiDefinitionManager.loadApiDefinitions(definitions); Set rules = new HashSet<>(); rules.add(new GatewayFlowRule(\"captcha\") .setResourceMode(SentinelGatewayConstants.RESOURCE_MODE_CUSTOM_API_NAME) .setParamItem( new GatewayParamFlowItem() .setParseStrategy(SentinelGatewayConstants.PARAM_PARSE_STRATEGY_URL_PARAM) .setFieldName(\"key\") .setMatchStrategy(SentinelGatewayConstants.PARAM_MATCH_STRATEGY_EXACT) .setParseStrategy(SentinelGatewayConstants.PARAM_PARSE_STRATEGY_CLIENT_IP) ) .setCount(10) .setIntervalSec(60) ); GatewayRuleManager.loadRules(rules); } } 在initGatewayRules方法中，我们定义了具体的限流逻辑。 1.3 自定义限流异常 默认情况下，当接口超出流量限制后，Sentinel返回如下格式的JSON报文： {\"code\":429, \"message\":\"Sentinel block exception\", \"route\":\"captcha\"} 我们也可以自定义异常响应。 public class sGatewayBlockFallbackProvider implements ZuulBlockFallbackProvider { @Override public String getRoute() { return \"*\"; } @Override public BlockResponse fallbackResponse(String route, Throwable throwable) { if (throwable instanceof BlockException) { return new BlockResponse(HttpStatus.TOO_MANY_REQUESTS.value(), \"访问频率超限\", route); } else { return new BlockResponse(HttpStatus.INTERNAL_SERVER_ERROR.value(), \"系统异常\", route); } } } 要让其生效，还需在上面定义的FebsGatewaySentinelFilter过滤器的doInit方法里通过ZuulBlockFallbackManager.registerProvider设置它： @Slf4j @Configuration public class GatewaySentinelFilter { ...... @PostConstruct public void doInit() { ZuulBlockFallbackManager.registerProvider(new FebsGatewayBlockFallbackProvider()); initGatewayRules(); } ...... } "},"library/Sentinel/Sentinel限流实例.html":{"url":"library/Sentinel/Sentinel限流实例.html","title":"Sentinel与Spring Cloud Gateway限流实例","keywords":"","body":"Sentinel与Spring Cloud Gateway限流实例1. 适配维度2. POM依赖3. 注入对应SentinelSentinel与Spring Cloud Gateway限流实例 1. 适配维度 route 维度：即在 Spring 配置文件中配置的路由条目，资源名为对应的 routeId 自定义 API 维度：用户可以利用 Sentinel 提供的 API 来自定义一些 API 分组 2. POM依赖 com.alibaba.csp sentinel-spring-cloud-gateway-adapter 3. 注入对应Sentinel 使用时只需注入对应的 SentinelGatewayFilter 实例以及 SentinelGatewayBlockExceptionHandler 实例即可。比如： @Configuration public class GatewayConfiguration { private final List viewResolvers; private final ServerCodecConfigurer serverCodecConfigurer; public GatewayConfiguration(ObjectProvider> viewResolversProvider, ServerCodecConfigurer serverCodecConfigurer) { this.viewResolvers = viewResolversProvider.getIfAvailable(Collections::emptyList); this.serverCodecConfigurer = serverCodecConfigurer; } @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public SentinelGatewayBlockExceptionHandler sentinelGatewayBlockExceptionHandler() { // Register the block exception handler for Spring Cloud Gateway. return new SentinelGatewayBlockExceptionHandler(viewResolvers, serverCodecConfigurer); } @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public GlobalFilter sentinelGatewayFilter() { return new SentinelGatewayFilter(); } } 完整实例： @Configuration public class GatewaySentinelConfigure { private final List viewResolvers; private final ServerCodecConfigurer serverCodecConfigurer; public GatewaySentinelConfigure(ObjectProvider> viewResolversProvider, ServerCodecConfigurer serverCodecConfigurer) { this.viewResolvers = viewResolversProvider.getIfAvailable(Collections::emptyList); this.serverCodecConfigurer = serverCodecConfigurer; } @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public SentinelGatewayBlockExceptionHandler sentinelGatewayBlockExceptionHandler() { return new SentinelGatewayBlockExceptionHandler(viewResolvers, serverCodecConfigurer); } @Bean @Order(-1) public GlobalFilter sentinelGatewayFilter() { return new SentinelGatewayFilter(); } @PostConstruct public void doInit() { initGatewayRules(); } /** * 验证码限流 */ private void initGatewayRules() { Set definitions = new HashSet<>(); Set predicateItems = new HashSet<>(); predicateItems.add(new ApiPathPredicateItem().setPattern(\"/auth/captcha\")); ApiDefinition definition = new ApiDefinition(\"captcha\") .setPredicateItems(predicateItems); definitions.add(definition); GatewayApiDefinitionManager.loadApiDefinitions(definitions); Set rules = new HashSet<>(); rules.add(new GatewayFlowRule(\"captcha\") .setResourceMode(SentinelGatewayConstants.RESOURCE_MODE_CUSTOM_API_NAME) .setParamItem( new GatewayParamFlowItem() .setParseStrategy(SentinelGatewayConstants.PARAM_PARSE_STRATEGY_URL_PARAM) .setFieldName(\"key\") .setMatchStrategy(SentinelGatewayConstants.PARAM_MATCH_STRATEGY_EXACT) .setParseStrategy(SentinelGatewayConstants.PARAM_PARSE_STRATEGY_CLIENT_IP) ) .setCount(10)// 限流阈值 .setIntervalSec(60)// 统计时间窗口，单位是秒，默认是 1 秒。 ); GatewayRuleManager.loadRules(rules); } } 所以上面的验证码限流的含义就是，验证码60s最多请求10次 "},"library/SpringBootAdmin/整合SpringBootAdmin.html":{"url":"library/SpringBootAdmin/整合SpringBootAdmin.html","title":"整合Spring Boot Admin","keywords":"","body":"整合Spring Boot Admin1. 简称2. SBA服务端2.1 添加pom依赖2.2 添加@EnableAdminServer注解2.3 编写配置文件application.yml：2.4 浏览器访问3. SBA客户端3.1 添加POM依赖3.2 添加配置信息3.3 浏览器访问整合Spring Boot Admin 1. 简称 Spring Boot Admin 通过 Spring-boot-starter-actuator 提供的REST接口实现了图形化的监控界面，包括应用的配置信息，Beans信息、环境属性、线程信息、JVM 状态等。 Spring Boot Admin 分为服务端和客户端。客户端通过HTTP向服务端提供自身信息，服务端收集这些信息并以图形化界面的方式呈现。（Spring Boot Admin客户端简称为SBA客户端，Spring Boot Admin 服务端简称为SBA服务端） 2. SBA服务端 2.1 添加pom依赖 de.codecentric spring-boot-admin-server 2.1.6 de.codecentric spring-boot-admin-server-ui 2.1.6 spring-boot-admin-server-ui依赖，该依赖用于图形化展示监控数据。 2.2 添加@EnableAdminServer注解 @EnableAdminServer @SpringBootApplication public class FebsMonitorAdminApplication { public static void main(String[] args) { SpringApplication.run(FebsMonitorAdminApplication.class, args); } } 2.3 编写配置文件application.yml： spring: application: name: Monitor-Admin boot: admin: ui: title: ${spring.application.name} 应用名称为FEBS-Monitor-Admin，spring.boot.admin.ui.title配置了Web页面的title为Monitor-Admin。 2.4 浏览器访问 使用浏览器访问 [http://localhost:8080： 因为没有登录，页面直接被重定向到登录页，输入用户名和密码后： 因为还没有搭建SBA客户端，所以监控信息都是空的 3. SBA客户端 被SBA服务端监控的微服务就是SBA 客户端 3.1 添加POM依赖 de.codecentric spring-boot-admin-starter-client 2.1.6 3.2 添加配置信息 spring: boot: admin: client: url: http://localhost:8401 username: zsz password: 123456 info: app: name: ${spring.application.name} description: \"@project.description@\" version: \"@project.version@\" management: endpoints: web: exposure: include: '*' endpoint: health: show-details: ALWAYS 这些配置的含义如下： spring.boot.admin.client.url指定了SBA服务端地址； spring.boot.admin.client.username对应SBA服务端的用户名； spring.boot.admin.client.password对应SBA服务端的密码； info.**配置了SBA客户端的名称，描述和版本信息； management.endpoints.web.exposure.include='*'表示将SBA客户端的所有监控端点都暴露给SBA服务端； management.endpoint.health.show-details表示总是展示详细的健康信息。 3.3 浏览器访问 "},"tools/":{"url":"tools/","title":"工具","keywords":"","body":"工具工具 jrebel热加载 p3c插件检测代码规范 抓包 fiddler(windows推荐) gitlab gitlab搭建 Gitlab-Runner的安装与使用 gitlab配置custom hook 问题集锦 数据库连接池Druid Druid多数据源配置 快捷键 IDEA上阅读源码快捷键 版本控制 git gitignore文件屏蔽规则 android gitignore 模板 "},"tools/maven/Maven核心知识.html":{"url":"tools/maven/Maven核心知识.html","title":"Maven核心知识","keywords":"","body":"Maven核心知识1. Maven 简介2. 核心功能3. 仓库4. Maven 常用命令5. Maven命令package、install、deploy 的联系和区别6. Maven 生命周期7. Maven 依赖范围8. 依赖冲突参考文章Maven核心知识 1. Maven 简介 Maven 是一个项目管理工具，他包含了 一个项目对象模型（Project Object Model） 一组标准集合 一个项目生命周期（Project Lifecycle） 一个依赖管理系统（Dependency Management System） 和用来运行定义在生命周期阶段（phase）中的插件（pulgin）目标（goal）的逻辑 2. 核心功能 依赖管理：Maven工程对jar包的管理过程 项目构建 3. 仓库 本地仓库 远程仓库（私服） 中央仓库 4. Maven 常用命令 clean：删除项目中已经编译好的信息，删除target目录 compile：Maven工程的编译命令，用于编译项目的源代码，将src/main/java下的文件编译成class文件输出到target目录下 test：使用合适的单元测试框架运行测试 package：将编译好的代码打包成可分发的格式，如jar，war install：安装包至本地仓库，已备本地的其它项目作为依赖使用 deploy: 复制最终的包至远程仓库，共享给其他开发人员和项目（通常和一次正式发布相关） 每一个构建项目的命令都对应了maven底层一个插件 5. Maven命令package、install、deploy 的联系和区别 mvn clean package： 依次执行了clean、resources、compile、testResources、testCompile、test、jar（打包）等七个阶段 mvn clean install： 依次执行了 clean、resources、compile、testResuources、testComplie、jar（打包）、install 等8个阶段 mvn clean deploy: 依次执行了clean、resources、compile、testResuources、testComplie、jar（打包）、install、deploy等9个阶段 主要区别： package命令完成了项目编译、单元测试、打包功能，但没有把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库。 install命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库，但没有布署到远程maven私服仓库。 deploy命令完成了项目编译、单元测试、打包功能，同时把打好的可执行jar包（war包或其它形式的包）布署到本地maven仓库和远程maven私服仓库。 6. Maven 生命周期 清理生命周期：运行 mvn clean 将调用清理生命周期 默认生命周期：是一个软件应用程序构建过程的总体模型 compile、test、package、install、deploy 站点生命周期：为一个或者一组项目生成项目文档和报告，使用较少。 7. Maven 依赖范围 complie：默认范围、编译测试运行都有效 provided：编译和运行有效，最后在运行的时候不会加入 官方举了一个例子。比如在JavaEE web项目中我们需要使用servlet的API，但是Tomcat中已经提供这个jar，我们在编译和测试的时候需要使用这个api，但是部署到tomcat的时候，如果还加入servlet构建就会产生冲突，这个时候就可以使用provided。 runtime：测试和运行有效 test：测试有效 system：与本机系统关联，编译和测试时有效 -　import：导入的范围，它只在使用dependencyManagement中，表示从其他pom中导入dependecy的配置。 8. 依赖冲突 每个显示申明的类包都会依赖于一些其它的隐式类，这些隐式的类包会被maven间接依赖进来，因而可能造成一个我们不想要的类包的载入，严重的甚至会引起类包之间的冲突 要解决这个问题，首先就是要查看pom.xml 显式和隐式的依赖类包，然后通过这个类包树找出我们不想要的依赖类包，手工将其排除在外就可以了 unitils-database org.unitils 参考文章 Maven核心知识点梳理 "},"tools/Jrebel/":{"url":"tools/Jrebel/","title":"jrebel热加载","keywords":"","body":"jrebel热加载激活Jrebel离线模式jrebel热加载 激活 激活地址 http://jrebel.pyjuan.com/c95f8c2b-9e97-4bd4-b9bf-48ba24fc3a10 https://jrebel.qekang.com/bb25c9bf-7695-48d6-b1a0-baf893ca7631 Jrebel离线模式 "},"tools/p3c/":{"url":"tools/p3c/","title":"p3c插件检测代码规范","keywords":"","body":"p3c插件检测代码规范p3c插件检测代码规范 阿里巴巴Java开发规约插件p3c详细教程及使用感受 "},"tools/swagger/Swagger的使用.html":{"url":"tools/swagger/Swagger的使用.html","title":"Swagger的使用","keywords":"","body":"Swagger的使用1.集成1.1 引入依赖包1.2. 配置Swagger1.3 打开swagger地址2. 使用2.1 @API注解2.2 @ApiImplicitParam：2.3 @ApiImplicitParams：2.4 @ApiModel：2.5 @ApiModelProperty：2.6 @ApiOperation：2.7 @ApiResponse：2.8 @ApiResponses：2.9 @ApiParam：2.10 @Authorization：2.11 @AuthorizationScope：2.12 @ResponseHeader：参考文章Swagger的使用 1.集成 1.1 引入依赖包 ... 2.7.0 io.springfox springfox-swagger2 ${swagger.version} io.springfox springfox-swagger-ui ${swagger.version} 1.2. 配置Swagger @Configuration @EnableSwagger2 public class SwaggerConfig { @Value(\"${swagger.show}\") private boolean swaggerShow; @Bean public Docket createRestApi() { return new Docket(DocumentationType.SWAGGER_2) .enable(swaggerShow) .apiInfo(apiInfo()) .select() .apis(RequestHandlerSelectors.basePackage(\"com.ylzinfo.appfactory\")) .paths(PathSelectors.any()) .build() .securitySchemes(securitySchemes()) .securityContexts(securityContexts()); } /** * 配置认证模式 */ private List securitySchemes() { return newArrayList(new ApiKey(\"Authorization\", \"Authorization\", \"header\")); } /** * 配置认证上下文 */ private List securityContexts() { return newArrayList(SecurityContext.builder() .securityReferences(defaultAuth()) .forPaths(PathSelectors.any()) .build()); } private List defaultAuth() { AuthorizationScope authorizationScope = new AuthorizationScope(\"global\", \"accessEverything\"); AuthorizationScope[] authorizationScopes = new AuthorizationScope[1]; authorizationScopes[0] = authorizationScope; return newArrayList(new SecurityReference(\"Authorization\", authorizationScopes)); } /** * 项目信息 */ private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(\"App管理平台 RESTful APIs\") .version(\"1.0\") .build(); } } 在application.yml 设置是否启动swagger swagger: show: true 1.3 打开swagger地址 http://localhost:9710/swagger-ui.html 2. 使用 2.1 @API注解 tags：可以使用tags()允许您为操作设置多个标签的属性，而不是使用该属性。 description：可描述描述该类作用。 例如： @RequestMapping(\"appfactory/index\") @Api(value = \"首页工厂\", tags = {\"首页工厂标签\"}, description = \"app首页工厂描述\") public class IndexFactoryController { } 2.2 @ApiImplicitParam： 作用在方法上，表示单独的请求参数 参数： name ：参数名。 value ： 参数的具体意义，作用。 required ： 参数是否必填。 dataType ：参数的数据类型。 paramType ：查询参数类型，这里有几种形式： 类型 作用 path 以地址的形式提交数据 query 直接跟参数完成自动映射赋值 body 以流的形式提交 仅支持POST header 参数在request headers 里边提交 form 以form表单的形式提交 仅支持POST 2.3 @ApiImplicitParams： 用于方法，包含多个 @ApiImplicitParam： 例： @ApiImplicitParams({ @ApiImplicitParam(name = \"id\", value = \"book's name\", required = true, dataType = \"Long\", paramType = \"query\"), @ApiImplicitParam(name = \"date\", value = \"book's date\", required = false, dataType = \"string\", paramType = \"query\")}) 2.4 @ApiModel： 用于类，表示对类进行说明，用于参数用实体类接收； 2.5 @ApiModelProperty： 用于方法，字段 ，表示对model属性的说明或者数据操作更改 例： @ApiModel(value = \"User\", description = \"用户\") public class User implements Serializable{ private static final long serialVersionUID = 1546481732633762837L; /** * 用户ID */ @ApiModelProperty(value = \"用户ID\", required = true) @NotEmpty(message = \"{id.empty}\", groups = {Default.class,New.class,Update.class}) protected String id; /** * code/登录帐号 */ @ApiModelProperty(value = \"code/登录帐号\") @NotEmpty(message = \"{itcode.empty}\", groups = {Default.class,New.class,Update.class}) protected String itcode; /** * 用户姓名 */ @ApiModelProperty(value = \"用户姓名\") @NotEmpty(message = \"{name.empty}\", groups = {Default.class,New.class,Update.class}) protected String name; } 2.6 @ApiOperation： 用于方法，表示一个http请求的操作 。 @ApiOperation(value = \"获取图书信息\", notes = \"获取图书信息\", response = Book.class, responseContainer = \"Item\", produces = \"application/json\") @ApiImplicitParams({ @ApiImplicitParam(name = \"id\", value = \"book's name\", required = true, dataType = \"Long\", paramType = \"query\"), @ApiImplicitParam(name = \"date\", value = \"book's date\", required = false, dataType = \"string\", paramType = \"query\")}) @RequestMapping(value = \"/{id}\", method = RequestMethod.GET) @ResponseBody public Book getBook(@PathVariable Long id, String date) { return books.get(id); } 2.7 @ApiResponse： 用于方法，描述操作的可能响应。 2.8 @ApiResponses： 用于方法，一个允许多个ApiResponse对象列表的包装器。 例： @ApiResponses(value = { @ApiResponse(code = 500, message = \"2001:因输入数据问题导致的报错\"), @ApiResponse(code = 500, message = \"403:没有权限\"), @ApiResponse(code = 500, message = \"2500:通用报错（包括数据、逻辑、外键关联等，不区分错误类型）\")}) 2.9 @ApiParam： 用于方法，参数，字段说明，表示对参数的添加元数据（说明或是否必填等） 2.10 @Authorization： 声明要在资源或操作上使用的授权方案。 2.11 @AuthorizationScope： 介绍一个OAuth2授权范围。 2.12 @ResponseHeader： 响应头设置，使用方法。 参考文章 SwaggerAPI注解详解,以及注解常用参数配置 "},"tools/grabbag/":{"url":"tools/grabbag/","title":"抓包","keywords":"","body":"抓包抓包 "},"tools/grabbag/fiddler.html":{"url":"tools/grabbag/fiddler.html","title":"fiddler(windows推荐)","keywords":"","body":"fiddler(windows推荐)1. 安装与使用1.1 下载1.2 安装1.3 浏览器抓包2. 抓取 iPhone / Android 设备的数据包2.1 确保手机和电脑在同一个局域网内2.2 开启远程连接代理模式2.3 手机设置代理fiddler(windows推荐) 参考文档 1. 安装与使用 1.1 下载 下载地址 1.2 安装 一步步next完成安装 1.3 浏览器抓包 我们随便请求一个地址，如www.baidu.com 在fiddler 就可以看到请求情况 2. 抓取 iPhone / Android 设备的数据包 2.1 确保手机和电脑在同一个局域网内 2.2 开启远程连接代理模式 在tools->options->connections 开启allow remote computers to connect 2.3 手机设置代理 选择对应的wifi，设置代理 此时就可以看到手机的请求了 "},"tools/gitlab/":{"url":"tools/gitlab/","title":"gitlab","keywords":"","body":"gitlabgitlab "},"tools/gitlab/gitlab搭建.html":{"url":"tools/gitlab/gitlab搭建.html","title":"gitlab搭建","keywords":"","body":"gitlab搭建1. 搭建方式2. 安装2.1 安装并配置必要的依赖关系2.2 添加 GitLab 镜像源并安装2.3 配置2.4 启用配置并启动GitLab2.5 浏览器打开配置的地址3. 常用的几个Gitlab命令4. gitlab相关目录参考文章gitlab搭建 1. 搭建方式 gitlab 搭建有两种方式 第一种方法 说白了其实gitlab就是一个web端，打散后其实也是由（nginx，gitaly，redis，gitlab-workhorse...）等这些东西构成；所以你可以一个个组装一个个编译安装；这样你也可以深入去了解gitlab，同时也可以达到最简化（将不必要的东西去掉） 第二种方法 rpm 安装。。。官方和社区都有提供 第一种太折腾人了，本文演示第二种方式 2. 安装 2.1 安装并配置必要的依赖关系 在 CentOS 系统上，下面的命令将会打开系统防火墙 HTTP 和 SSH 的访问。 sudo yum install curl policycoreutils openssh-server openssh-clients sudo systemctl enable sshd sudo systemctl start sshd sudo yum install postfix sudo systemctl enable postfix sudo systemctl start postfix sudo firewall-cmd --permanent --add-service=http sudo systemctl reload firewalld 2.2 添加 GitLab 镜像源并安装 方式1：命令管道的方式安装镜像仓库 curl -sS http://packages.gitlab.com.cn/install/gitlab-ce/script.rpm.sh | sudo bash sudo yum install gitlab-ce 我使用不成功 方式2：使用命名手动安装 curl -LJO https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-XXX.rpm rpm -i gitlab-ce-XXX.rpm gitlab-ce-xxx.rpm 具体是哪个版本可以进gitlab镜像中选择 2.2.1 安装时异常 在使用命令rpm -i gitlab-ce-XXX.rpm 时提示 rpm -i gitlab-ce-12.2.4-ce.0.el7.x86_64.rpm warning: gitlab-ce-12.2.4-ce.0.el7.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID f27eab47: NOKEY error: Failed dependencies: policycoreutils-python is needed by gitlab-ce-12.2.4-ce.0.el7.x86_64 需要安装 yum install policycoreutils-python 2.2.2 安装完成提示 [root@iZwz97t3ru69kye3l7uj70Z ~]# rpm -i gitlab-ce-12.2.4-ce.0.el7.x86_64.rpm warning: gitlab-ce-12.2.4-ce.0.el7.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID f27eab47: NOKEY It looks like GitLab has not been configured yet; skipping the upgrade script. *. *. *** *** ***** ***** .****** ******* ******** ******** ,,,,,,,,,***********,,,,,,,,, ,,,,,,,,,,,*********,,,,,,,,,,, .,,,,,,,,,,,*******,,,,,,,,,,,, ,,,,,,,,,*****,,,,,,,,,. ,,,,,,,****,,,,,, .,,,***,,,, ,*,. _______ __ __ __ / ____(_) /_/ / ____ _/ /_ / / __/ / __/ / / __ `/ __ \\ / /_/ / / /_/ /___/ /_/ / /_/ / \\____/_/\\__/_____/\\__,_/_.___/ Thank you for installing GitLab! GitLab was unable to detect a valid hostname for your instance. Please configure a URL for your GitLab instance by setting `external_url` configuration in /etc/gitlab/gitlab.rb file. Then, you can start your GitLab instance by running the following command: sudo gitlab-ctl reconfigure For a comprehensive list of configuration options please see the Omnibus GitLab readme https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/README.md 2.3 配置 2.3.1 配置外网访问的地址(必须改) 修改/etc/gitlab/gitlab.rb，将默认external_url改成，你的ip或者域名 external_url 'http://gitlab.isture.com' #external_url 'http://120.79.200.xxx' 2.3.2 nginx端口冲突 gitlab 其实是个web，他自带了nginx。但是我们本身服务器也有一个nginx这样就冲突了 2.3.2.1 解决方案 第一种（不建议） 禁用gitlab自带的nginx，使用我们自己安装的nginx 第二种 更改gitlab 自带的nginx的默认端口，域名加端口访问。 如果有强迫症，可以在自己的服务器nginx映射到gitlab 的nginx就好了 2.3.2.2 修改配置 修改/etc/gitlab/gitlab.rb的端口配置 nginx['listen_port'] = 9999 2.3.3 修改Gitlab数据存储路径（非必选） 默认的Gitlab数据存储路径在/var/opt/gitlab/git-data 修改/etc/gitlab/gitlab.rb ###! path that doesn't contain symlinks.** # git_data_dirs({ # \"default\" => { # \"path\" => \"你需要放置的路径\" # } # }) 2.4 启用配置并启动GitLab sudo gitlab-ctl reconfigure 其他gitlab 命令 sudo gitlab-ctl reconfigure sudo gitlab-ctl restart sudo gitlab-ctl status 2.5 浏览器打开配置的地址 在第一次访问时，将被重定向到密码重置页面 默认账户是root，密码在此页面设置 3. 常用的几个Gitlab命令 # 重新应用gitlab的配置,每次修改/etc/gitlab/gitlab.rb文件之后执行 sudo gitlab-ctl reconfigure # 启动gitlab服务 sudo gitlab-ctl start # 重启gitlab服务 sudo gitlab-ctl restart # 查看gitlab运行状态 sudo gitlab-ctl status #停止gitlab服务 sudo gitlab-ctl stop # 查看gitlab运行所有日志 sudo gitlab-ctl tail #查看 nginx 访问日志 sudo gitlab-ctl tail nginx/gitlab_acces.log #查看 postgresql 日志 sudo gitlab-ctl tail postgresql # 停止相关数据连接服务 gitlab-ctl stop unicorn gitlab-ctl stop sidekiq # 系统信息监测 gitlab-rake gitlab:env:info 4. gitlab相关目录 /var/opt/gitlab/git-data/repositories/root：库默认存储目录 /opt/gitlab：是gitlab的应用代码和相应的依赖程序 /var/opt/gitlab：此目录下是运行gitlab-ctl reconfigure命令编译后的应用数据和配置文件，不需要人为修改配置 /etc/gitlab：此目录下存放了以omnibus-gitlab包安装方式时的配置文件，这里的配置文件才需要管理员手动编译配置 /var/log/gitlab：此目录下存放了gitlab各个组件产生的日志 /opt/gitlab/backups/：默认备份文件生成的目录 参考文章 搭建gitlab服务 在CenterOS系统上安装GitLab并自定义域名访问GitLab管理页面 "},"tools/gitlab/ci/Gitlab-Runner的安装与使用.html":{"url":"tools/gitlab/ci/Gitlab-Runner的安装与使用.html","title":"Gitlab-Runner的安装与使用","keywords":"","body":"Gitlab-Runner的安装与使用1. 安装步骤1.1 安装gitlab-ci-multi-runner1.2 使用gitlab-ci-multi-runner注册Runner注意事项1.3 让注册好的Runner运行起来参考文章Gitlab-Runner的安装与使用 1. 安装步骤 1.1 安装gitlab-ci-multi-runner 添加yum源 curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.rpm.sh | sudo bash 安装 yum install gitlab-ci-multi-runner 这里是官网的安装教程，其它操作系统的请参考 https://gitlab.com/gitlab-org/gitlab-ci-multi-runner 1.2 使用gitlab-ci-multi-runner注册Runner 安装好gitlab-ci-multi-runner这个软件之后，我们可以用它想gitlab-ci注册runner 向gitlab-CI注册runner需要两样东西:GitLab-CI的url和注册token。 其中，token是为了确定你这个Runner是所有工程都能够使用的Shared Runner还是具体某一个工程才能使用的Specific Runner。 1.2.1 所有工程都能够使用的Shared Runner 如果要注册Shared Runner，你需要到管理界面的Runners页面里面去找注册token。如下图所示： 1.2.2 某一个工程才能使用的Specific Runner 如果要注册Specific Runner，你需要到项目的设置的Runner页面里面去找注册token。如下图所示： 1.2.3 注册runner 找到token之后，运行下面这条命令注册Runner（当然，除了url和token之外，还需要其他的信息，比如执行器executor、构建目录builds_dir等）。 gitlab-ci-multi-runner register 依据提示输入 注意事项 Whether to run untagged builds [true/false]: 是否运行在没有 tag 的 build 上面。在配置 gitlab-ci 时，会有很多 job，每个 job 可以通过 tags 属性来选择 Runner。这里为 true 表示如果 job 没有配置 tags，也执行。 Whether to lock the Runner to current project [true/false]: 是否锁定 Runner 到当前项目 选择 executor，这里列出了很多 executor shell 注册完成之后，GitLab-CI就会多出一条Runner记录，如下图所示： 1.3 让注册好的Runner运行起来 Runner注册完成之后还不行，还必须让它运行起来，否则它无法接收到GitLab-CI的通知并且执行软件集成脚本。怎么让Runner运行起来呢？gitlab-ci-multi-runner提供了这样一条命令gitlab-ci-multi-runner run-single，详情如下： gitlab-ci-multi-runner start gitbook-runner 查看runner运行状态 ps -aux | grep gitlab-runner gitlab运行成功了 参考文章 GitLab-CI与GitLab-Runner "},"tools/gitlab/ci/Gitlab-Runner实践.html":{"url":"tools/gitlab/ci/Gitlab-Runner实践.html","title":"Gitlab-Runner实践","keywords":"","body":"Gitlab-Runner实践1. 安装Gitlab Runner2. 注册gitlab-runner参考文章Gitlab-Runner实践 根据gitlab-runner 的介绍，我们可以看到 1. 安装Gitlab Runner 2. 注册gitlab-runner 安装了 GitLab Runner 之后,就可以为 GitLab 中的仓库注册一个 Runner，注册的交互式命令如下： sudo gitlab-runner register 命令交互过程 参考文章 GitLab CI/CD 介绍和使用 "},"tools/gitlab/custom-hook.html":{"url":"tools/gitlab/custom-hook.html","title":"gitlab配置custom hook","keywords":"","body":"gitlab配置custom hook1. 具体步骤1.1 设置custom_hooks路径1.2 启用配置1.3 创建hook文件1.4 编写 hook 脚本参考文章gitlab配置custom hook 1. 具体步骤 1.1 设置custom_hooks路径 修改 gitlab 中的vi /etc/gitlab/gitlab.rb 增加 custom_hooks_dir 路径： gitlab_shell['custom_hooks_dir'] = \"/opt/gitlab/embedded/service/gitlab-shell/hooks\" 注：这里直接去掉注释使用自带的 1.2 启用配置 sudo gitlab-ctl reconfigure 1.3 创建hook文件 自定义脚本目录要符合//* 的规范。具体就是 在自定的custom_hooks_dir 目录下可创建三个文件夹对应三类 server hook name ： pre-receive.d update.d post-receive.d 在每个文件夹下可以创建任意文件，在对应的hook时期，gitlab就会主动调用 文件名以~结尾的文件会被忽略 如果想看这部分的实现细节可以看 /lib/gitlab_custom_hook.rb 文件 目录结构示意 [root@localhost custom_hooks]# tree . ├── post-receive.d │ ├── 01.sh │ └── 02.sh~ ├── pre-receive.d │ ├── 01.sh │ ├── 02.py │ └── 03.rb └── update.d ├── 01.sh └── 02.sh 1.4 编写 hook 脚本 hook 脚本就是git 自身的规范，写shell，python、ruby 都可以 留意脚本最后的退出值：0 正常退出，用户可以 push；非 0 异常退出，中断提交（pre-receive 和 update） 。 细节参见： 5.4 Git钩子：自定义你的工作流 · geeeeeeeeek/git-recipes Wiki · GitHub 如果想让用户 push 时看到相应的日志直接在脚本中 echo 即可。 这里举两个例子： &#x1F330;：Say hi. #!/bin/sh echo \"Say hi from gitlab servertes ok &#x1F604;\" exit 0 &#x1F330;：检查提交修改的文件，发现有特定内容禁止提交 #!/bin/sh FIND_KEY='.test.51offer.com' OLD_VALUE=$2 NEW_VALUE=$3 FILES=$(git rev-list --objects $OLD_VALUE...$NEW_VALUE | egrep '\\.(jsp|vm|java)$' | awk '{print $2}' | sort | uniq ) FLAG=0 for FILE in $FILES do git show $NEW_VALUE:$FILE | grep -q \"$FIND_KEY\" if [ $? -eq 0 ] then FLAG=1 echo \"&#x1F4C3; 包含非法字段 '$FIND_KEY' : $FILE\" fi done if [ $FLAG -eq 0 ] then echo \"✅ 代码检查通过.\" else echo \"❌ 代码检查不通过!\" exit 1 fi exit 0 例子结果 上面第二个例子中，尝试 git push，就能看到如下的日志： Pushing to git@gitlab.51offer.inner:mall/paycenter.git remote: &#x1F4C3; 包含非法字段 '.test.51offer.com' : service/src/main/java/com/horizon/module/paycenter/service/PayService.java remote: ❌ 代码检查不通过! remote: error: hook declined to update refs/heads/test To git@gitlab.51offer.inner:mall/paycenter.git = [up to date] release/old -> release/old = [up to date] v1.0.0.2016.9.8 -> v1.0.0.2016.9.8 ! [remote rejected] test -> test (hook declined) error: failed to push some refs to 'git@gitlab.51offer.inner:mall/paycenter.git' Completed with errors, see above 参考文章 Gitlab 服务器端 custom hook 配置 官方文档 "},"tools/gitlab/gitlab内存占用过大.html":{"url":"tools/gitlab/gitlab内存占用过大.html","title":"gitlab内存占用过大","keywords":"","body":"gitlab内存占用过大1. 解决方案1.1 减少 unicorn worker进程数1.2 减少数据库缓存1.3 减少数据库并发数1.4 减少sidekiq并发数gitlab内存占用过大 我的服务器配置是2核4G内存，启动gitlab 就占用了很大内存空间(停止gitlab 会释放1.5G内存) 1. 解决方案 1.1 减少 unicorn worker进程数 我们可以使用 top -ac 先看一下开启了多少unicorn worker进程，gitlab默认开启进程数与CPU内核数相同 vim /etc/gitlab/gitlab.rb unicorn['worker_processes'] = 8 默认是被注释掉的，官方建议该值是CPU核心数加一，可以提高服务器的响应速度，如果内存只有4G，或者服务器上有其它业务，就不要改了，以免内存不足。另外，这个参数最小值是2，设为1，服务器可能会卡死。 1.2 减少数据库缓存 postgresql['shared_buffers'] = \"128MB\" 默认为256MB，可适当改小 1.3 减少数据库并发数 postgresql['max_worker_processes'] = 4 默认为8，可适当减少 1.4 减少sidekiq并发数 sidekiq['concurrency'] = 10 默认是25，可适当改小 "},"tools/gitlab/问题集锦.html":{"url":"tools/gitlab/问题集锦.html","title":"问题集锦","keywords":"","body":"问题集锦1. push 提交异常1.1 解决方案2. clone路径不对2.1 方式一2.2 方式二问题集锦 1. push 提交异常 push 提交时出现fatal: The remote end hung up unexpectedly异常 localhost:android zhangshengzhong$ git push --set-upstream http://gitlab.isture.com/zsz/android-gitbook.git master Counting objects: 245, done. Delta compression using up to 8 threads. Compressing objects: 100% (214/214), done. error: RPC failed; result=22, HTTP code = 41383 MiB/s fatal: The remote end hung up unexpectedly Writing objects: 100% (245/245), 15.83 MiB | 6.85 MiB/s, done. Total 245 (delta 7), reused 0 (delta 0) fatal: The remote end hung up unexpectedly Everything up-to-date 1.1 解决方案 原因：因为上传文件超过了nginx 的文件限制最大值 注意：自己是走哪个nginx，是gitlab自带的还是服务器的nginx 给nginx 添加上 http { ... client_max_body_size 100M; 2. clone路径不对 2.1 方式一 直接更改/etc/gitlab/gitlab.rb不能生效，更改/opt/gitlab/embedded/service/gitlab-rails/config/gitlab.yml文件 vi /opt/gitlab/embedded/service/gitlab-rails/config/gitlab.yml 更改host和port即可 ## GitLab settings gitlab: ## Web server settings (note: host is the FQDN, do not include http://) host: xxx.xxx.xxx.xxx port: 8181 https: fals 2.2 方式二 external_url 'http://gitlab.isture.com' external_url 'http://gitlab.isture.com' "},"druid/":{"url":"druid/","title":"数据库连接池Druid","keywords":"","body":"数据库连接池Druid数据库连接池Druid "},"druid/Druid多数据源配置.html":{"url":"druid/Druid多数据源配置.html","title":"Druid多数据源配置","keywords":"","body":"Druid多数据源配置集成步骤目录具体集成步骤1.引入jar包2.配置参数3.编写配置文件4.修改启动文件参考博客Druid多数据源配置 本篇介绍在 SpringBoot 下如何配置Druid 多数据源 集成步骤目录 引入jar包 设置配置参数 编写配置文件与 编写数据源常量/枚举 创建动态数据源 动态数据源配置 定义动态数据源注解 设置数据源 AOP 代理 修改启动文件 具体集成步骤 1.引入jar包 以我们公司项目为例，数据库主要使用oracle 和 国产数据库 gbase com.oracle ojdbc6 11.2.0.4.0 com.informix ifxjdbc 1.0.1 com.alibaba druid-spring-boot-starter 1.1.10 2.配置参数 spring: profiles: dev application: name: app-platform datasource: druid: orac: #数据源1 oracle # 数据库访问配置, 使用druid数据源 type: com.alibaba.druid.pool.DruidDataSource driver-class-name: oracle.jdbc.driver.OracleDriver url: jdbc:oracle:thin:@192.168.0.xx:1521:orcl username: username password: password gbase:#数据源2 gbase driver-class-name: com.informix.jdbc.IfxDriver type: com.alibaba.druid.pool.DruidDataSource name: test url: jdbc:informix-sqli://192.168.0.xx:9088/app_lzf:INFORMIXSERVER=gbaseserver; username: myusername password: mypassword 3.编写配置文件 3.1.定义数据源名称常量 public interface DataSourceNames { String ORAC = \"orac\"; String GBASE = \"gbase\"; } 3.2 创建动态数据源 /** * 动态数据源 */ public class DynamicDataSource extends AbstractRoutingDataSource { private static final ThreadLocal contextHolder = new ThreadLocal<>(); /** * 配置DataSource, defaultTargetDataSource为主数据库 */ public DynamicDataSource(DataSource defaultTargetDataSource, Map targetDataSources) { super.setDefaultTargetDataSource(defaultTargetDataSource); super.setTargetDataSources(targetDataSources); super.afterPropertiesSet(); } @Override protected Object determineCurrentLookupKey() { return getDataSource(); } public static void setDataSource(String dataSource) { contextHolder.set(dataSource); } public static String getDataSource() { return contextHolder.get(); } public static void clearDataSource() { contextHolder.remove(); } } 3.3 动态数据源配置 /** * 配置多数据源 */ @Configuration public class DynamicDataSourceConfig { /** * 创建 DataSource Bean * */ @Bean @ConfigurationProperties(\"spring.datasource.druid.orac\") public DataSource oneDataSource(){ DataSource dataSource = DruidDataSourceBuilder.create().build(); return dataSource; } @Bean @ConfigurationProperties(\"spring.datasource.druid.gbase\") public DataSource twoDataSource(){ DataSource dataSource = DruidDataSourceBuilder.create().build(); return dataSource; } /** * 如果还有数据源,在这继续添加 DataSource Bean * */ @Bean @Primary public DynamicDataSource dataSource(DataSource oneDataSource, DataSource twoDataSource) { Map targetDataSources = new HashMap<>(2); targetDataSources.put(DataSourceNames.ORAC, oneDataSource); targetDataSources.put(DataSourceNames.GBASE, twoDataSource); // 还有数据源,在targetDataSources中继续添加 System.out.println(\"DataSources:\" + targetDataSources); return new DynamicDataSource(oneDataSource, targetDataSources); } } 3.4.定义动态数据源注解: /** * 多数据源注解 */ @Documented @Target({ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) public @interface DataSource { String value() default DataSourceNames.ORAC; } 3.4 设置数据源 AOP 代理: /** * 数据源AOP切面处理 */ @Aspect @Component public class DataSourceAspect implements Ordered { protected Logger logger = LoggerFactory.getLogger(getClass()); /** * 切点: 所有配置 DataSource 注解的方法 */ @Pointcut(\"@annotation(com.ylzinfo.common.druid.DataSource)\") public void dataSourcePointCut() {} @Around(\"dataSourcePointCut()\") public Object around(ProceedingJoinPoint point) throws Throwable { MethodSignature signature = (MethodSignature) point.getSignature(); Method method = signature.getMethod(); DataSource ds = method.getAnnotation(DataSource.class); // 通过判断 DataSource 中的值来判断当前方法应用哪个数据源 DynamicDataSource.setDataSource(ds.value()); System.out.println(\"当前数据源: \" + ds.value()); logger.debug(\"set datasource is \" + ds.value()); try { return point.proceed(); } finally { DynamicDataSource.clearDataSource(); logger.debug(\"clean datasource\"); } } @Override public int getOrder() { return 1; } } 4.修改启动文件 如果设置了动态数据源，那么需要将自有的配置依赖去除(DataSourceAutoConfiguration) @SpringBootApplication(exclude={DataSourceAutoConfiguration.class}) public class AppPlatformApplication { public static void main(String[] args) { SpringApplication.run(AppPlatformApplication.class, args); } } 参考博客 SpringBoot--Druid多数据源配置 Druid配置 "},"keymap/":{"url":"keymap/","title":"快捷键","keywords":"","body":"快捷键快捷键 > > > "},"keymap/idea/IDEA上阅读源码快捷键.html":{"url":"keymap/idea/IDEA上阅读源码快捷键.html","title":"IDEA上阅读源码快捷键","keywords":"","body":"IDEA上阅读源码快捷键IDEA上阅读源码快捷键 1.跳转方法后跳回 快捷键 option + command + left(方向键中的左箭头) 2.查看某个方法的调用链 快捷键 control + option + h (需要把光标定位到某个方法上) 3.查看某个接口的所有实现类或子类 快捷键 option + command + b (需要把光标定位到某个接口上) 4.换行(无论是否处于行尾) command + shift + enter 5.定位行数 command + g 6.查找类 command + n 7.查看类或接口的继承关系： ctrl + h 8.显示包里类的层次结构 UML 关系图 把光标定位在需要分析的类上,右键->Diagrams-->Show Diagram(或Show Diagram Popup) "},"tools/vcs/":{"url":"tools/vcs/","title":"版本控制","keywords":"","body":"版本控制版本控制 "},"tools/vcs/git/":{"url":"tools/vcs/git/","title":"git","keywords":"","body":"gitgit "},"tools/vcs/git/gitignore文件屏蔽规则.html":{"url":"tools/vcs/git/gitignore文件屏蔽规则.html","title":"gitignore文件屏蔽规则","keywords":"","body":"gitignore文件屏蔽规则2. gitignore 文件格式规范2.1 glob 模式3. 案例gitignore文件屏蔽规则 在使用git仓库时，我们并不希望将所有的文件都提交到仓库中，需要对一些文件进行屏蔽，有些则要保留 此时我们就需要使用到.gitignore文件 2. gitignore 文件格式规范 所有空行或#开通的行都会被忽略 可以使用标准的glob 模式匹配 文件或目录前加/表示仓库根目录的对应文件 匹配模式最后跟反斜杠/说明要忽略目录 要特殊不许了某个文件或目录，可以在模式钱加上取反! 2.1 glob 模式 其中glob模式是指shell 所使用的简化了的正则表达式 星号*匹配零个或多个任意字符 [abc]匹配任何一个列在方括号中的字符（这个例子要么匹配一个a，要么匹配一个b，要么匹配一个c）， ？匹配一个任意字符 如果在方括号中使用短划线分割两个字符，表示所有在这两个字符范围内的都可以匹配 例如：[0-9] 表示匹配所有0-9的数字 3. 案例 *.a # 所有以 '.a' 为后缀的文件都屏蔽掉 # Office 缓存文件 ~'$'*.docx ~'$'*.ppt ~'$'*.pptx ~'$'*.xls tags # 仓库中所有名为 tags 的文件都屏蔽 core.* # 仓库中所有以 'core.' 开头的文件都屏蔽 tools/ # 屏蔽目录 tools log/* # 屏蔽目录 log 下的所有文件，但不屏蔽 log 目录本身 /log.log # 只屏蔽仓库根目录下的 log.log 文件，其他目录中的不屏蔽 readme.md # 屏蔽仓库中所有名为 readme.md 的文件 !/readme.md # 在上一条屏蔽规则的条件下，不屏蔽仓库根目录下的 readme.md 文 注意： 例子中的最后两条的顺序很重要，必须要先屏蔽所有的，然后才建立特殊不屏蔽的规则！ "},"tools/vcs/git/androidGitignore模板.html":{"url":"tools/vcs/git/androidGitignore模板.html","title":"android gitignore 模板","keywords":"","body":"android gitignore 模板android gitignore 模板 # 所有的以.apk 结尾的文件不提交 *.apk *.ap_ *.aab # Files for the ART/Dalvik VM *.dex # 以.class 文件结尾不提交 *.class # 屏蔽自己生成的目录 bin/ gen/ out/ release/ # gradle和build/ 相关文件不提交 .gradle/ build/ # Local configuration file (sdk path, etc) local.properties # Proguard folder generated by Eclipse proguard/ # Log Files *.log # Android Studio Navigation editor temp files .navigation/ # Android Studio captures folder captures/ # IntelliJ 自动生成的文件不提交 *.iml .idea # Keystore 相关关键 # Uncomment the following lines if you do not want to check your keystore files in. #*.jks #*.keystore # 版本控制 vcs.xml # svn 目录 .svn "},"tools/script/运行脚本.html":{"url":"tools/script/运行脚本.html","title":"运行脚本","keywords":"","body":"运行脚本运行脚本 #!/bin/sh # # ### 配置路径 APP_DIR=. #//配置名称 APP_NAME=api-gateway-1.0.0.RELEASE PORT=9091 usage() { echo \"Usage: sh run.sh [start|stop|deploy]\" exit 1 } kills(){ tpid=`ps -ef|grep $PORT|grep -v grep|grep -v kill|awk '{print $2}'` if [[ $tpid ]]; then echo 'Kill Process!' kill -9 $tpid fi } start(){ rm -f $APP_DIR/tpid nohup java -Xms200m -Xmx200m -XX:PermSize=356m -XX:MaxPermSize=500m -XX:MaxNewSize=500m -jar -Dspring.config.location=./application.yml -Dserver.port=$PORT $APP_DIR/\"$APP_NAME\".jar > logs.log 2>&1 & echo $! > $APP_DIR/tpid echo Start Success! } stop(){ tpid1=`ps -ef|grep $APP_NAME|grep -v grep|grep -v kill|awk '{print $2}'` echo tpid1-$tpid1 if [[ $tpid1 ]]; then echo 'Stop Process...' kill -15 $tpid1 fi sleep 5 tpid2=`ps -ef|grep $APP_NAME|grep -v grep|grep -v kill|awk '{print $2}'` echo tpid2-$tpid2 if [[ $tpid2 ]]; then echo 'Kill Process!' kill -9 $tpid2 else echo 'Stop Success!' fi } check(){ tpid=`ps -ef|grep $APP_NAME|grep -v grep|grep -v kill|awk '{print $2}'` if [[ tpid ]]; then echo 'App is running.' else echo 'App is NOT running.' fi } deploy() { kills rm -rf $APP_DIR/\"$APPNAME\".jar cp \"$APP_NAME\".jar $APP_DIR } case \"$1\" in \"start\") start ;; \"stop\") stop ;; \"kill\") kills ;; \"check\") check ;; *) usage ;; esac "},"tools/ide/idea/IDEAmaven下载jar包太慢.html":{"url":"tools/ide/idea/IDEAmaven下载jar包太慢.html","title":"IDEA maven下载jar包太慢","keywords":"","body":"IDEA maven下载jar包太慢参考文章IDEA maven下载jar包太慢 我们在IDEA 的maven下载jar包是可能会超级慢，这时候我们可以使用一些maven镜像来解决。 右键项目选中maven选项，然后选择“open settings.xml”或者 “create settings.xml”，然后把如下代码粘贴进去就可以了。重启IDE alimaven aliyun maven http://maven.aliyun.com/nexus/content/groups/public/ central 参考文章 IntelliJ IDEA maven库下载依赖包速度慢的问题 "},"utils/跨域支持/CorsConfig.html":{"url":"utils/跨域支持/CorsConfig.html","title":"CorsConfig","keywords":"","body":"CorsConfig支持跨域CorsConfig 支持跨域 配置CoreFilter @Configuration public class CorsConfig { @Bean public CorsFilter corsFilter() { final UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); final CorsConfiguration config = new CorsConfiguration(); config.setAllowCredentials(true); config.setAllowedOrigins(Arrays.asList(\"*\")); //http:www.a.com config.setAllowedHeaders(Arrays.asList(\"*\")); config.setAllowedMethods(Arrays.asList(\"*\")); config.setMaxAge(300l); source.registerCorsConfiguration(\"/**\", config); return new CorsFilter(source); } } 该配置类里注册了CorsFilter: setAllowCredentials(true)表示允许cookie跨域； addAllowedHeader(Arrays.asList(\"*\"))表示请求头部允许携带任何内容； addAllowedOrigin(Arrays.asList(\"*\"))表示允许任何来源； addAllowedMethod(Arrays.asList(\"*\"))表示允许任何HTTP方法。 "},"manager/关于亲力亲为的思考.html":{"url":"manager/关于亲力亲为的思考.html","title":"关于亲力亲为的思考","keywords":"","body":"关于亲力亲为的思考关于亲力亲为的思考 因为自己是技术出身，在刚接触管理工作的时候，总是亲力亲为，感觉看谁干活都不放心，恨不得自己把所有事情都给做了 之前在项目中我参与了大部分设计和讨论，所有的核心代码都是自己写，组里其他人的代码我也会过目，参与代码评审，希望对每一个改动都心中有数。随着业务的扩大，项目组越来越大。事事亲为也变得越来越吃力，导致自己周末和晚上都用来加班，但总有忙不完的事 你要学会带人和授权，把事情分出轻重缓急，把有些事分出去让别人做 但是我总觉得每一件事都很重要，什么都不愿放手。我在分配工作上一直做得不好，好几次分配出去都出了点小问题，最后还要自己介入，另一方面自己深陷每个细节，没办法做得退一步海阔天空，只能紧紧盯住眼前的事，无法看得更高更远。其实这种状态对个人而言，我忙的疲惫不堪，对团队而言我一旦停下来我就成为了所有人的瓶颈 从亲力亲为模式到授权模式，这个过程花了好长时间，为什么这么困难呢？原因有很多，其中最重要的一点，就是跟自己的工作性质有关，如果自己没参与就会担心事情没做好。这里面有两个误区。 事情做没做好，跟有没有按你的方式做好是两码事，别人有别人的工作方式也能把事情做好 介入和不介入并不是非黑即白，用什么方式介入，在什么地方介入才是关键 出发点就是把事情做好，我们把任务分配出去一样能把事情做好，我们的关注点要放在如何让别人把事情做好上，比如需要怎么样的支持和帮助才能把事情做好，需要在什么时间点去提供这样的帮助 授权和分配任务注意点 让对方明确目标，知道最终想要达成的结果是什么，对这个任务完成的期望值是怎么样的 制定一个计划并保持跟进，跟进不是指导，不是让你频繁介入别人任务。在对方有疑问的时候提供帮助，这种可能是解决方案，或技术方向。在对方对下一步有疑问的时候，给出建议和探讨 关键在于授权和任务分配 我们要把任务有效的分配出去 保证分配的任务能被圆满的完成 很多优秀的工程师都是独行侠，单枪匹马完成很很多丰功伟绩，但是当他们发现团队协作能做出更大的成就时，就会从亲力亲为转为授权模式，帮助他人成功 "},"manager/如何进行工作分配.html":{"url":"manager/如何进行工作分配.html","title":"如何进行工作分配","keywords":"","body":"如何进行工作分配如何进行工作分配 管理者的主要职责就是将工作进行分解和授权，那么工作分配的时候需要注意些什么？ 我们一般把重要的任务分配给自己信任的人，但在没有建立起充分信任的情况下，如何分配工作 建立参考基线 没有直接接触的时候，可以通过第三方评价，个人履历以及该员工做过的项目或产品衡量他的履历 问对问题比正确答案更正在 把任务分配到员工手里的时候，要进行充分的沟通。告诉他任务的详细情形，看他会问出什么样的问题，提出哪些想法。 沟通的时候，要看他的问题和想法是否尽可能考虑所有的情况，问问题和提想法之前是会去调研，还是直接去做想当然的假定，在你给出反馈意见或指导性建议之后，他的反馈是什么，又会问出什么样的问题，提出哪些想法。这些都能帮助我们进一步评估他是不是真的了解任务状况，有没有综合考虑任务中的问题 工期估算 工期估算是必不可少的环节，你可以让员工试着估算:需要多久完成，大概什么时候完成，需要什么样的资源等 如果他是个思维缜密的人，就会考虑完成这个任务的所有相关问题，开发工作量是多大，会不会依赖其他人的工作，有多少沟通成本，技术难点是什么，有没有现成的方案，系统框架是什么，后期集成和测试的时间成本有多少，综合考虑后，再给出一个相对全面的时间评估 很多时候，我们希望任务能够更快完成，所以员工给出一个短平快的工期是符合我们预期，但如果他很多关键因素没有考虑到，这就是一个过于乐观和不切实际的估算 执行力 如果执行能力不强，重要任务也不能交代给他 后期维护 完成一个项目并不意味着项目的结束，很多时候，项目上线还需要一段时间的维护工作，这包括了bug修复，排查用户反馈的问题，完成后续的迭代开发等等，你需要观察：一个人是不是可以自觉维护产品，有没有责任感，会不会推卸责任 "},"manager/如何做职场规划.html":{"url":"manager/如何做职场规划.html","title":"如何做职场规划","keywords":"","body":"如何做职场规划如何做职场规划 做职场规划时，先思考以下问题 你的个人价值观是什么，最在意什么？换句话说，什么样情境或状态会让你有幸福感或者自信心。比如，在你的成长路上哪些是你在意的，是独立解决问题的能力，还是挑战别人做不到的东西，是受欢迎程度，还是更在意自由，健康的家庭生活。 最在意自身的成长，无论是技术上的硬实力，还是个人的软实力。在技术上希望身边能有共同奋进的人，能一起探讨技术问题。 ​ 2019.12.2 你的长期愿景是什么，五年甚至十年后，你希望自己成为什么样的人？ 在行业技术上有一定的知名度 ​ 2019.12.2 为了达到目标，你还需要哪些技能或者经验？你可以在短期内发展什么技能让你走得更远？想达成你的职业梦想职业，或在工作中取得成功，你还需要做些什么，需要哪些必备技能？哪些技能不是必须的，但是会有很大好处 技术的深度和广度还需要加强，现阶段先把java学精，并记录学习过程，不断沉淀自身 ​ 2019.12.2 你的优势和长处是什么？是合作性，独立思考、行动快速，还有良好的产品思维，你现在的日常工作能否让你展示你的长处，又是如何展示的，你觉得你比别人在哪些方面做得好，能不能举出些具体例子。 执行力强，想到的事第一时间就把他实现，不拖拉 责任感，敢于担当，从不推卸责任 毅力，有明确的目标，不轻言放弃 ​ 2019.12.2 "},"work/APP更新方案选择.html":{"url":"work/APP更新方案选择.html","title":"APP更新方案选择","keywords":"","body":"APP更新方案选择1. 背景2. 可选方案2.1 推荐方案3. 方案对比4. 总结APP更新方案选择 1. 背景 app升级项目发展必不可少的环节，因掌上12333用户量大，用户同时更新会造成服务器带宽压力过大。 版本更新时，每天的下载量为400G，那么1个月就需要10T 左右的下行流量。 100M带宽 下载速度总和=100M/8=12.5M APP版本更新时：假如100个人同时下载，理论的下载速度就是125kb/s，随着下载人数的增加，速度持续下降 2. 可选方案 阿里云存储 阿里CND 云服务器购买带宽 人社部/易联众公司等文件服务器 跳转应用市场 用应用宝apk下载链接 2.1 推荐方案 用应用宝apk下载链接+阿里CDN 3. 方案对比 方案 优势 缺点 价格 阿里云存储) 使用方便 1.价格高2.超出部分按流量计费 10T每月4874 阿里CND 1.价格是云存储的1/3 1.可能存在缓存问题2. 为了不影响其他服务，需要有独立的域名 10T每年1620 云服务器购买带宽 1.价格高2.峰值时带宽不够（最高200M带宽） 100M带宽7899200M的带宽15819 人社部/易联众公司等文件服务器 免费 带宽不够，下载速度慢 跳转应用市场 免费 1.需要各个平台上架，时间成本，且登录各平台不便2.用户跳转过去可能就不会下载了3.小众手机无法覆盖 用应用宝apk下载链接 免费，下载速度快 1.应用市场下载地址(抓包获得)可能会变2.需要应用宝上架，时间成本，且登录不便3. 可能会被应用宝风控，链接失效等风险 4. 总结 采用应用宝apk下载链接+阿里CDN "}}